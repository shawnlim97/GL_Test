{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489ae600-9a37-456b-b654-ff1cd64624db",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998d4f73-2cd0-45e4-9007-342735625c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import pymap3d as pm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import (find_peaks, windows, resample_poly, iirnotch,\n",
    "                            butter, sosfilt, sosfiltfilt, get_window, firwin, \n",
    "                            filtfilt, lfilter, peak_widths)\n",
    "from spkit import frft\n",
    "import pandas as pd\n",
    "from scipy.ndimage import (\n",
    "        percentile_filter, minimum_filter1d, maximum_filter1d, median_filter,\n",
    "        uniform_filter1d, binary_opening, binary_closing, label\n",
    "    )\n",
    "from numpy.fft import fft, fftfreq\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "from geopy.distance import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f8fe42-02f2-417a-922b-a622ebd21c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5c012-ee42-4158-a399-5869a4422bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.fft import fft, ifft, fftfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a292b-ac7d-49ac-a177-52660bbabde2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pre-Deinterleaver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0e62e-5959-476b-92ef-3f2b69d6e7da",
   "metadata": {},
   "source": [
    "## Generate Emitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b5eecb-2ca8-4af3-bf21-5d3ad9d238af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emitters_in_if(n, seed, fmin, fmax):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    kinds = [\"single frequency pulse\", \"cw\", \"lfm\", \"lfm\", \n",
    "             \"lfmcw\", \"single frequency pulse\", \"lfmcw\", \"cw\"][:n]\n",
    "    rows = []\n",
    "    for i, kind in enumerate(kinds):\n",
    "        f_rf = rng.uniform(fmin, fmax)\n",
    "        eirp_dbm = rng.normal(90.0, 5.0)\n",
    "\n",
    "        if kind == \"single frequency pulse\":\n",
    "            PW  = rng.uniform(5e-6, 15e-6)\n",
    "            PRI = rng.uniform(0.8e-3, 5e-3)\n",
    "            f0 = f1 = f_rf\n",
    "            k  = 0.0\n",
    "            BW = 0.0\n",
    "            f_center = f_rf\n",
    "\n",
    "        elif kind == \"cw\":\n",
    "            PW = np.nan; PRI = np.nan\n",
    "            f0 = f1 = f_rf\n",
    "            k = 0.0\n",
    "            BW = 0.0\n",
    "            f_center = f_rf\n",
    "\n",
    "        elif kind == \"lfm\":\n",
    "            PW = rng.uniform(20e-6, 80e-6)\n",
    "            BW = rng.uniform(8e6, 20e6)        \n",
    "            sgn = rng.choice([-1.0, +1.0])     \n",
    "            k   = sgn * BW / PW                \n",
    "\n",
    "            margin = BW/2\n",
    "            f_center = rng.uniform(fmin + margin, fmax - margin)\n",
    "\n",
    "            if sgn > 0:\n",
    "                f0 = f_center - BW/2\n",
    "                f1 = f_center + BW/2\n",
    "            else:\n",
    "                f0 = f_center + BW/2\n",
    "                f1 = f_center - BW/2\n",
    "\n",
    "            PRI = rng.uniform(0.8e-3, 3e-3)\n",
    "\n",
    "        elif kind == \"lfmcw\":\n",
    "            PW = PRI = rng.uniform(20e-6, 80e-6)\n",
    "            BW = rng.uniform(10e6, 30e6)\n",
    "            margin = BW/2\n",
    "            f_center = rng.uniform(fmin + margin, fmax - margin)\n",
    "        \n",
    "            # Count how many LFMCWs are already in rows\n",
    "            lfmcw_count = sum(r[\"type\"] == \"lfmcw\" for r in rows)\n",
    "        \n",
    "            if lfmcw_count == 0:\n",
    "                sgn = +1.0  # first LFMCW → up-saw\n",
    "            else:\n",
    "                sgn = -1.0  # second LFMCW → down-saw\n",
    "        \n",
    "            k = sgn * BW / PW\n",
    "            if sgn > 0:\n",
    "                f0 = f_center - BW/2\n",
    "                f1 = f_center + BW/2\n",
    "            else:\n",
    "                f0 = f_center + BW/2\n",
    "                f1 = f_center - BW/2\n",
    "\n",
    "        rows.append(dict(\n",
    "            emitter_id=i, type=kind, EIRP_dBm=eirp_dbm,\n",
    "            f_tx_center_hz=f_center,\n",
    "            f0_tx_hz=f0, f1_tx_hz=f1,\n",
    "            k_tx_hz_per_s=k,\n",
    "            BW_sig_hz=abs(f1 - f0),\n",
    "            PW_s=PW, PRI_s=PRI\n",
    "        ))\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f2ebf-e4af-4198-9b5b-6b1320531ffd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Geometry and Earth Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ba621-b2c4-4691-9bcc-87f6abc1bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical constants\n",
    "c = 299792458.0  # [m/s]\n",
    "\n",
    "# WGS-84\n",
    "WGS84_A  = 6378137.0\n",
    "WGS84_E2 = 6.69437999014e-3\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Coordinate Transforms (WGS-84)\n",
    "# =============================================================================\n",
    "\n",
    "def lla_to_ecef(lat_deg, lon_deg, alt_m):\n",
    "    \"\"\"Convert (lat, lon, alt) [deg,deg,m] → ECEF [m].\"\"\"\n",
    "    lat = np.deg2rad(lat_deg)\n",
    "    lon = np.deg2rad(lon_deg)\n",
    "    a   = WGS84_A\n",
    "    e2  = WGS84_E2\n",
    "\n",
    "    sin_lat = np.sin(lat)\n",
    "    cos_lat = np.cos(lat)\n",
    "    cos_lon = np.cos(lon)\n",
    "    sin_lon = np.sin(lon)\n",
    "\n",
    "    N = a / np.sqrt(1.0 - e2 * sin_lat**2)\n",
    "\n",
    "    x = (N + alt_m) * cos_lat * cos_lon\n",
    "    y = (N + alt_m) * cos_lat * sin_lon\n",
    "    z = (N * (1.0 - e2) + alt_m) * sin_lat\n",
    "    return np.array([x, y, z], dtype=float)\n",
    "\n",
    "\n",
    "def ecef_to_enu(xyz, ref_lat_deg, ref_lon_deg, ref_alt_m=0.0):\n",
    "    \"\"\"\n",
    "    Convert ECEF point(s) to ENU w.r.t. a reference LLA.\n",
    "    - xyz: (3,) or (N,3)\n",
    "    \"\"\"\n",
    "    lat = np.deg2rad(ref_lat_deg)\n",
    "    lon = np.deg2rad(ref_lon_deg)\n",
    "\n",
    "    R = np.array([\n",
    "        [-np.sin(lon),              np.cos(lon),             0.0],\n",
    "        [-np.sin(lat)*np.cos(lon), -np.sin(lat)*np.sin(lon), np.cos(lat)],\n",
    "        [ np.cos(lat)*np.cos(lon),  np.cos(lat)*np.sin(lon), np.sin(lat)]\n",
    "    ])\n",
    "\n",
    "    ref_ecef = lla_to_ecef(ref_lat_deg, ref_lon_deg, ref_alt_m)\n",
    "    xyz = np.asarray(xyz)\n",
    "    d = xyz - ref_ecef\n",
    "\n",
    "    if d.ndim == 1:\n",
    "        return R @ d\n",
    "    else:\n",
    "        return (R @ d.T).T\n",
    "\n",
    "\n",
    "def ecef_to_lla(x, y, z):\n",
    "    \"\"\"Convert ECEF [m] → (lat, lon, h) [deg,deg,m].\"\"\"\n",
    "    a  = WGS84_A\n",
    "    e2 = WGS84_E2\n",
    "\n",
    "    b   = a * np.sqrt(1.0 - e2)\n",
    "    ep2 = (a*a - b*b) / (b*b)\n",
    "\n",
    "    lon = np.arctan2(y, x)\n",
    "    p   = np.sqrt(x*x + y*y)\n",
    "    th  = np.arctan2(a * z, b * p)\n",
    "\n",
    "    sin_th = np.sin(th)\n",
    "    cos_th = np.cos(th)\n",
    "\n",
    "    lat = np.arctan2(z + ep2*b*sin_th**3,\n",
    "                     p - e2*a*cos_th**3)\n",
    "\n",
    "    sin_lat = np.sin(lat)\n",
    "    N = a / np.sqrt(1.0 - e2*sin_lat**2)\n",
    "    h = p/np.cos(lat) - N\n",
    "\n",
    "    return np.rad2deg(lat), np.rad2deg(lon), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e8358-c669-4438-a43c-093a6d227461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tau_fd_one_emitter_lla(e_lla, r_sats, v_sats, f_c):\n",
    "    \"\"\"\n",
    "    Compute TDOA/FDOA for one emitter in LLA.\n",
    "    Returns (tau21, tau31, fD21, fD31) where FF1 is the reference.\n",
    "    \"\"\"\n",
    "    e_ecef = lla_to_ecef(e_lla[\"lat\"], e_lla[\"lon\"], e_lla[\"alt\"])\n",
    "\n",
    "    r1 = r_sats['FF1']; v1 = v_sats['FF1']\n",
    "    r2 = r_sats['FF2']; v2 = v_sats['FF2']\n",
    "    r3 = r_sats['FF3']; v3 = v_sats['FF3']\n",
    "\n",
    "    R1 = np.linalg.norm(e_ecef - r1)\n",
    "    R2 = np.linalg.norm(e_ecef - r2)\n",
    "    R3 = np.linalg.norm(e_ecef - r3)\n",
    "\n",
    "    tau21 = (R2 - R1) / c    # time at FF2 minus FF1\n",
    "    tau31 = (R3 - R1) / c\n",
    "\n",
    "    u1 = (e_ecef - r1) / R1\n",
    "    u2 = (e_ecef - r2) / R2\n",
    "    u3 = (e_ecef - r3) / R3\n",
    "\n",
    "    vr1 = np.dot(v1, u1)\n",
    "    vr2 = np.dot(v2, u2)\n",
    "    vr3 = np.dot(v3, u3)\n",
    "\n",
    "    fD1 = (f_c / c) * vr1\n",
    "    fD2 = (f_c / c) * vr2\n",
    "    fD3 = (f_c / c) * vr3\n",
    "\n",
    "    fD21 = fD2 - fD1\n",
    "    fD31 = fD3 - fD1\n",
    "\n",
    "    return tau21, tau31, fD21, fD31\n",
    "\n",
    "\n",
    "def build_tau_fd_maps(emitters_df, r_sats, v_sats, f_c):\n",
    "    \"\"\"\n",
    "    For each emitter_id, build per-satellite (tau_rel, fD_rel) w.r.t. FF1.\n",
    "\n",
    "    tau_fd_by_sat['FF1'][eid] = (0, 0)\n",
    "    tau_fd_by_sat['FF2'][eid] = (tau21, fD21)\n",
    "    tau_fd_by_sat['FF3'][eid] = (tau31, fD31)\n",
    "    \"\"\"\n",
    "    tau_fd_by_sat = {sat: {} for sat in ['FF1', 'FF2', 'FF3']}\n",
    "\n",
    "    for _, row in emitters_df.iterrows():\n",
    "        eid = int(row[\"emitter_id\"])\n",
    "        e_lla = {\n",
    "            \"lat\": float(row[\"tar_lat_deg\"]),\n",
    "            \"lon\": float(row[\"tar_lon_deg\"]),\n",
    "            \"alt\": 0.0,\n",
    "        }\n",
    "\n",
    "        tau21, tau31, fD21, fD31 = compute_tau_fd_one_emitter_lla(\n",
    "            e_lla, r_sats, v_sats, f_c\n",
    "        )\n",
    "\n",
    "        tau_fd_by_sat['FF1'][eid] = (0.0,   0.0)\n",
    "        tau_fd_by_sat['FF2'][eid] = (tau21, fD21)\n",
    "        tau_fd_by_sat['FF3'][eid] = (tau31, fD31)\n",
    "\n",
    "    return tau_fd_by_sat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04f8bd-3754-4c85-afb4-0d4fac2a926f",
   "metadata": {},
   "source": [
    "## Link Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01646682-12ca-48ce-b987-f5cdbe3bbeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_vector_ecef(lat_deg, lon_deg):\n",
    "    lat = np.deg2rad(lat_deg)\n",
    "    lon = np.deg2rad(lon_deg)\n",
    "    return np.array([\n",
    "        np.cos(lat) * np.cos(lon),\n",
    "        np.cos(lat) * np.sin(lon),\n",
    "        np.sin(lat)\n",
    "    ])\n",
    "\n",
    "def elevation_and_range(r_sat_ecef, r_emit_ecef, up_ecef):\n",
    "    los = r_sat_ecef - r_emit_ecef\n",
    "    R = np.linalg.norm(los)\n",
    "    los_hat = los / R\n",
    "    elev_deg = np.degrees(np.arcsin(los_hat @ up_ecef))\n",
    "    return elev_deg, R\n",
    "\n",
    "def fspl_dB(R_m, f_hz):\n",
    "    \"\"\"Free-space path loss in dB.\"\"\"\n",
    "    R_km  = R_m / 1e3\n",
    "    f_GHz = f_hz / 1e9\n",
    "    return 92.45 + 20*np.log10(R_km) + 20*np.log10(f_GHz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce26b18-02f2-44c5-b75f-a89b16df7763",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## IQ Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "364e8b6b-ca35-4008-9aac-dd2a2d38b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_iq(\n",
    "    emitters_df,\n",
    "    T=1.0,             # capture length (s)\n",
    "    fs=125e6,          # sample rate (Hz)\n",
    "    F_LO=9.50e9,       # baseband LO (Hz)\n",
    "    noise_V=0.1,       # noise voltage std (per real or imag component)\n",
    "    sat_name=\"FF1\",    # which satellite to generate IQ for\n",
    "    tau_fd_map=None,   # dict[eid] -> (tau_rel_s, fD_rel_hz) w.r.t FF1\n",
    "    seed=0,\n",
    "    use_equal_power=False,  # if False: scale by SNR_dB_{sat_name}\n",
    "):\n",
    "    \"\"\"\n",
    "    Build one composite complex baseband IQ stream from emitters_df\n",
    "    for a *specific* satellite (sat_name = 'FF1'/'FF2'/'FF3').\n",
    "\n",
    "    If tau_fd_map is provided, each emitter_id i uses:\n",
    "        tau_rel, fD_rel = tau_fd_map[i]\n",
    "\n",
    "      - tau_rel shifts the *arrival time* at this satellite\n",
    "        (pulses start at t_start + tau_rel).\n",
    "\n",
    "      - fD_rel shifts the *baseband frequency* by the relative Doppler\n",
    "        (f0_bb_eff = (f0 - F_LO) + fD_rel).\n",
    "\n",
    "    Amplitude logic:\n",
    "      - If use_equal_power=True: all emitters have amplitude 1.0\n",
    "      - Else: amplitudes derived from link-budget SNR_dB_{sat_name} using:\n",
    "            amp = noise_V * sqrt(2 * 10^(SNR_dB/10))\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    N  = int(round(T * fs))\n",
    "    t  = np.arange(N, dtype=np.float64) / fs\n",
    "    y  = np.zeros(N, dtype=np.complex64)\n",
    "\n",
    "    snr_col = f\"SNR_dB_{sat_name}\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Amplitude scaling\n",
    "    # ------------------------------------------------------------\n",
    "    if use_equal_power:\n",
    "        amps = {int(r.emitter_id): 1.0 for _, r in emitters_df.iterrows()}\n",
    "    else:\n",
    "        amps = {}\n",
    "        for _, r in emitters_df.iterrows():\n",
    "            eid = int(r.emitter_id)\n",
    "            if snr_col not in r or np.isnan(r[snr_col]):\n",
    "                raise ValueError(\n",
    "                    f\"emitters_df must include {snr_col} when use_equal_power=False\"\n",
    "                )\n",
    "            snr_db  = float(r[snr_col])\n",
    "            snr_lin = 10.0 ** (snr_db / 10.0)\n",
    "            amp     = noise_V * np.sqrt(2.0 * snr_lin)\n",
    "            amps[eid] = amp\n",
    "\n",
    "        print(f\"\\n[{sat_name}] Amplitude per emitter (based on {snr_col} and noise_V):\")\n",
    "        for eid, a in amps.items():\n",
    "            print(f\"  emitter {eid}: amp = {a:.3f}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Signal synthesis loop\n",
    "    # ------------------------------------------------------------\n",
    "    truth = []\n",
    "\n",
    "    for _, e in emitters_df.iterrows():\n",
    "        i   = int(e[\"emitter_id\"])\n",
    "        typ = str(e[\"type\"]).lower()\n",
    "\n",
    "        f0  = float(e[\"f0_tx_hz\"])\n",
    "        f1  = float(e[\"f1_tx_hz\"])\n",
    "        k   = float(e[\"k_tx_hz_per_s\"])\n",
    "        PW  = float(e[\"PW_s\"]) if not pd.isna(e[\"PW_s\"]) else None\n",
    "        PRI = float(e[\"PRI_s\"]) if not pd.isna(e[\"PRI_s\"]) else None\n",
    "\n",
    "        # base PDW start time (global)\n",
    "        has_tstart = (\"t_start_s\" in e) and (e[\"t_start_s\"] is not None) and (not np.isnan(e[\"t_start_s\"]))\n",
    "        t_start = float(e[\"t_start_s\"]) if has_tstart else 0.0\n",
    "\n",
    "        # relative delay & Doppler for this sat (w.r.t FF1)\n",
    "        tau_rel = 0.0\n",
    "        fD_rel  = 0.0\n",
    "        if tau_fd_map is not None and i in tau_fd_map:\n",
    "            tau_rel, fD_rel = tau_fd_map[i]\n",
    "\n",
    "        # total start time for this satellite\n",
    "        t_start_total = t_start + tau_rel\n",
    "\n",
    "        # Baseband frequency INCLUDING Doppler for this satellite\n",
    "        # Treat FF1 as having fD_rel ≈ 0, others get fD21/fD31\n",
    "        f0_bb = (f0 - F_LO) + fD_rel\n",
    "\n",
    "        # For CW / LFMCW we don't care about t_start gating:\n",
    "        if typ == \"cw\":\n",
    "            phase = 2 * np.pi * (f0_bb * t)\n",
    "            sig = np.exp(1j * phase)\n",
    "\n",
    "        elif typ == \"lfmcw\":\n",
    "            if PRI is None:\n",
    "                raise ValueError(\"lfmcw requires PRI for LFMCW sweeps\")\n",
    "        \n",
    "            # Apply absolute timing (emitter start + propagation delay)\n",
    "            t_rel = t - t_start_total\n",
    "        \n",
    "            # Wrap within each sweep period\n",
    "            tau = np.mod(t_rel, PRI)\n",
    "        \n",
    "            # Continuous-time LFMCW ramp with Doppler shift included\n",
    "            phase = 2 * np.pi * (f0_bb * tau + 0.5 * k * tau**2)\n",
    "        \n",
    "            sig = np.exp(1j * phase)\n",
    "\n",
    "\n",
    "        elif typ == \"single frequency pulse\":\n",
    "            if PRI is None or PW is None:\n",
    "                raise ValueError(\"single frequency pulse requires non-NaN PW and PRI\")\n",
    "\n",
    "            tau  = t - t_start_total\n",
    "            gate = (tau >= 0.0) & (np.mod(tau, PRI) < PW)\n",
    "            gate = gate.astype(np.float64)\n",
    "\n",
    "            phase = 2 * np.pi * (f0_bb * t)\n",
    "            sig = np.exp(1j * phase) * gate\n",
    "\n",
    "        elif typ == \"lfm\":\n",
    "            if PRI is None or PW is None:\n",
    "                raise ValueError(\"lfm requires non-NaN PW and PRI\")\n",
    "\n",
    "            tau     = t - t_start_total\n",
    "            tau_mod = np.mod(tau, PRI)\n",
    "            gate    = (tau >= 0.0) & (tau_mod < PW)\n",
    "            gate    = gate.astype(np.float64)\n",
    "\n",
    "            tau_p = np.where(gate > 0.0, tau_mod, 0.0)\n",
    "            phase_rel = 2 * np.pi * (f0_bb * tau_p + 0.5 * k * tau_p**2)\n",
    "            sig = np.exp(1j * phase_rel) * gate\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown emitter type: {typ}\")\n",
    "\n",
    "        # Apply amplitude\n",
    "        y += (amps[i] * sig).astype(np.complex64)\n",
    "\n",
    "        truth.append({\n",
    "            \"emitter_id\": i,\n",
    "            \"type\": e[\"type\"],\n",
    "            \"t_start_s\": t_start_total,\n",
    "            \"f0_bb_hz\": f0_bb,\n",
    "            \"f1_bb_hz\": (f1 - F_LO) + fD_rel,\n",
    "            \"k_hz_per_s\": k,\n",
    "            \"PW_s\": PW,\n",
    "            \"PRI_s\": PRI,\n",
    "            \"amp\": amps[i],\n",
    "            snr_col: e.get(snr_col, np.nan),\n",
    "            \"tau_rel_s\": tau_rel,\n",
    "            \"fD_rel_hz\": fD_rel,\n",
    "        })\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Add noise (complex Gaussian)\n",
    "    # ------------------------------------------------------------\n",
    "    noise = noise_V * (\n",
    "        rng.standard_normal(N, dtype=np.float32)\n",
    "        + 1j * rng.standard_normal(N, dtype=np.float32)\n",
    "    )\n",
    "    y += noise\n",
    "\n",
    "    return y, pd.DataFrame(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b17f55-e2c4-457f-af3f-e5248d07910b",
   "metadata": {},
   "source": [
    "# Deinterleaver Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a40552-e585-4554-8e51-36a584e3e45c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## QOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0515725-192c-4662-ad10-6bee1f468bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_stage_times = {}\n",
    "\n",
    "def mark_stage(name, start=False):\n",
    "    \"\"\"Call with start=True to start timing, else logs elapsed.\"\"\"\n",
    "    global _stage_times\n",
    "    if start:\n",
    "        _stage_times[name] = time.time()\n",
    "    else:\n",
    "        elapsed = time.time() - _stage_times[name]\n",
    "        print(f\"{name}: {elapsed:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ce44a0-cf09-4213-934b-4756e548af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_window(w, signal_noisy, fs, stride, chunk_len,\n",
    "                   k_vals, nfft, nfft_mode,\n",
    "                   nfft_safety_margin, max_df_hz,\n",
    "                   precomp_refs):\n",
    "    start = w * stride\n",
    "    end = start + chunk_len\n",
    "    if end > len(signal_noisy):\n",
    "        return None, None\n",
    "\n",
    "    chunk = signal_noisy[start:end].astype(np.complex64)\n",
    "    row_db = k_scan_energy_window(chunk, fs, k_vals,\n",
    "                                  nfft=nfft,\n",
    "                                  nfft_mode=nfft_mode,\n",
    "                                  nfft_safety_margin=nfft_safety_margin,\n",
    "                                  max_df_hz=max_df_hz,\n",
    "                                  precomp_refs=precomp_refs)\n",
    "    t0_global_s = start / fs\n",
    "    return row_db, t0_global_s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432eb5f-e7e6-484e-b073-088af609a45d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PDW Extraction (Use this for Pulse as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be5bff9-da69-4438-8953-cdd439b08395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseband_PDW_extractor(\n",
    "    signal, fs, *,\n",
    "    k_hz_per_s,\n",
    "    t0_global_s,           # window start time (global)\n",
    "    window_us=5,           # smoothing / local-threshold window\n",
    "    min_sustain_us=5,      # morphology size (opening/closing)\n",
    "    threshold_percentile=50,\n",
    "    amp_floor_pct=90,      # percentile of smoothed amp to floor low-level clutter\n",
    "    downsample_factor=1,   # NEW\n",
    "    plot=False, plot_raw=False, plot_smoothed=False, plot_mask=False, plot_thres=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans the envelope of a BASEBANDED signal, extracts pulse bounds, and computes PDWs.\n",
    "    Assumes 'signal' is the re-chirped, isolated IQ from dechirp+heterodyne filter.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(signal)\n",
    "    t_local = np.arange(N) / fs\n",
    "    t_global = t0_global_s + t_local\n",
    "\n",
    "    # Envelope\n",
    "    amp_env = np.abs(signal)\n",
    "\n",
    "    # ---------- Step 1: optional downsampling ----------\n",
    "    ds = max(1, int(downsample_factor))\n",
    "    if ds > 1:\n",
    "        amp_env_proc = amp_env[::ds]\n",
    "        fs_proc = fs / ds\n",
    "    else:\n",
    "        amp_env_proc = amp_env\n",
    "        fs_proc = fs\n",
    "\n",
    "    # ---------- Step 2: robust smoothing ----------\n",
    "    win_samples = max(1, int(round(window_us * 1e-6 * fs_proc)))\n",
    "    smoothed_amp_proc = uniform_filter1d(\n",
    "        amp_env_proc, size=win_samples, mode='reflect'\n",
    "    )\n",
    "\n",
    "    # ---------- Step 3: adaptive local threshold ----------\n",
    "    local_max_proc = maximum_filter1d(smoothed_amp_proc, size=win_samples, mode='reflect')\n",
    "    threshold_proc = (threshold_percentile / 100.0) * local_max_proc\n",
    "    sustained_proc = smoothed_amp_proc > threshold_proc\n",
    "\n",
    "    # ---------- Step 4: morphology ----------\n",
    "    min_sustain_samples = max(1, int(round(min_sustain_us * 1e-6 * fs_proc)))\n",
    "    mask_proc = binary_opening(sustained_proc, structure=np.ones(min_sustain_samples))\n",
    "    mask_proc = binary_closing(mask_proc, structure=np.ones(min_sustain_samples))\n",
    "\n",
    "    # ---------- Step 5: recover edge-trimmed valid pulses ----------\n",
    "    labeled_sus, num_sus = label(sustained_proc)\n",
    "    for i in range(1, num_sus + 1):\n",
    "        idxs = np.where(labeled_sus == i)[0]\n",
    "        if len(idxs) >= min_sustain_samples:\n",
    "            mask_proc[idxs[0]:idxs[-1] + 1] = True\n",
    "\n",
    "    # ---------- Step 6: amplitude floor rejection ----------\n",
    "    amp_cut_proc = np.percentile(smoothed_amp_proc, amp_floor_pct)\n",
    "    mask_proc[smoothed_amp_proc < 0.5 * amp_cut_proc] = 0\n",
    "\n",
    "    # ---------- Step 7: upsample mask + threshold back to full rate ----------\n",
    "    if ds > 1:\n",
    "        mask = np.repeat(mask_proc, ds)[:len(amp_env)]\n",
    "        smoothed_amp = np.interp(\n",
    "            np.arange(len(amp_env)),\n",
    "            np.arange(len(smoothed_amp_proc)) * ds,\n",
    "            smoothed_amp_proc\n",
    "        )\n",
    "        threshold = np.interp(\n",
    "            np.arange(len(amp_env)),\n",
    "            np.arange(len(threshold_proc)) * ds,\n",
    "            threshold_proc\n",
    "        )\n",
    "    else:\n",
    "        mask = mask_proc\n",
    "        smoothed_amp = smoothed_amp_proc\n",
    "        threshold = threshold_proc\n",
    "\n",
    "    # ---------- Step 8: extract pulses + PDWs ----------\n",
    "    labeled_mask, num_regions = label(mask)\n",
    "    pulses, pulse_bounds, pdws = [], [], []\n",
    "\n",
    "    for rid in range(1, num_regions + 1):\n",
    "        idxs = np.where(labeled_mask == rid)[0]\n",
    "        if len(idxs) < 2:\n",
    "            continue\n",
    "        s, e = idxs[0], idxs[-1] + 1\n",
    "\n",
    "        # Global timing\n",
    "        t_start = t_global[s]\n",
    "        t_end   = t_global[e-1] + 1/fs\n",
    "        pw_s    = t_end - t_start\n",
    "\n",
    "        # Amplitude stats\n",
    "        env_seg = amp_env[s:e]\n",
    "        amp_med = float(np.median(env_seg))\n",
    "\n",
    "        # Frequency estimate via phase slope\n",
    "        ph = np.unwrap(np.angle(signal[s:e]))\n",
    "        fi = (fs/(2*np.pi)) * np.diff(ph)\n",
    "        if len(fi) > 20:\n",
    "            i_lo, i_hi = int(0.4*len(fi)), int(0.6*len(fi))\n",
    "            f_center_hz = float(np.median(fi[i_lo:i_hi]))\n",
    "        else:\n",
    "            f_center_hz = float(np.median(fi)) if len(fi) else np.nan\n",
    "\n",
    "        f_start_hz = f_center_hz - 0.5 * k_hz_per_s * pw_s\n",
    "        f_end_hz   = f_center_hz + 0.5 * k_hz_per_s * pw_s\n",
    "\n",
    "        pulses.append(signal[s:e])\n",
    "        pulse_bounds.append((s, e))\n",
    "        pdws.append({\n",
    "            \"TOA_global_us\": t_start*1e6,\n",
    "            \"PW_us\": pw_s*1e6,\n",
    "            \"Amp_med\": amp_med,\n",
    "            \"f_start_Hz\": f_start_hz,\n",
    "            \"f_end_Hz\": f_end_hz,\n",
    "            \"f_center_Hz\": f_center_hz,\n",
    "        })\n",
    "\n",
    "    # ---------- Optional diagnostics ----------\n",
    "    if plot:\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        if plot_raw: plt.plot(t_local*1e6, amp_env, label='Raw Amp', alpha=0.35)\n",
    "        if plot_smoothed: plt.plot(t_local*1e6, smoothed_amp, label='Smoothed', linewidth=1.2)\n",
    "        if plot_mask: plt.plot(t_local*1e6, smoothed_amp*mask, label='Masked', alpha=0.9)\n",
    "        if plot_thres: plt.plot(t_local*1e6, threshold, 'r--', label='Local Threshold')\n",
    "        for (s, e) in pulse_bounds:\n",
    "            plt.axvspan(s/fs*1e6, e/fs*1e6, color='lime', alpha=0.08)\n",
    "        plt.xlabel(\"Time [µs]\"); plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "    return pulses, pulse_bounds, mask, np.zeros_like(signal), pdws\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd798d14-82df-4018-8440-96f0f1791dd0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Signal Chirp Scanning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae7e61-d808-4554-ab4f-3024611e9565",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfce2e-f6cd-4690-8665-267c1d92c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_energy_map_dechirp_gpu_batched(\n",
    "    signal_noisy, fs, total_duration_s,\n",
    "    T_chunk=1e-3, stride_s=None,\n",
    "    k_range=(-1.0e12, 1.0e12), k_step=0.1e12,\n",
    "    batch_size=32,\n",
    "    nfft=None, nfft_mode=\"downpow2\",\n",
    "    nfft_safety_margin: float = 1.05,\n",
    "    max_df_hz: float | None = None,\n",
    "    show_progress=True,\n",
    "    return_candidates=True,\n",
    "    row_prom_db=4.5,\n",
    "    row_skip_margin_db=6.0,\n",
    "    k0_eps_MHz_per_us=0.05,\n",
    "    max_peaks_per_row=None,\n",
    "    k_vals_override=None,\n",
    "    # NEW\n",
    "    use_parallel: bool = True,\n",
    "    n_jobs: int = -1,\n",
    "):\n",
    "    \"\"\"\n",
    "    GPU build of dechirp-FFT energy map, batched across windows.\n",
    "    Candidate picking can be done serially or in parallel (joblib).\n",
    "    \"\"\"\n",
    "\n",
    "    t_total = time.perf_counter()\n",
    "\n",
    "    if stride_s is None:\n",
    "        stride_s = T_chunk\n",
    "\n",
    "    chunk_len = int(round(T_chunk * fs))\n",
    "    stride = int(round(stride_s * fs))\n",
    "    n_total = int(round(total_duration_s * fs))\n",
    "    n_win = 1 + max(0, int(np.floor((n_total - chunk_len) / stride)))\n",
    "\n",
    "    # --- k grid ---\n",
    "    if k_vals_override is not None:\n",
    "        k_vals = np.array(k_vals_override, dtype=float)\n",
    "    else:\n",
    "        k_vals = np.arange(k_range[0], k_range[1] + 0.5*k_step, k_step, dtype=float)\n",
    "\n",
    "    # GPU refs\n",
    "    k_vals_gpu = cp.asarray(k_vals, dtype=cp.float32)\n",
    "    t_rel = cp.arange(chunk_len, dtype=cp.float32) / fs\n",
    "    t2_rel = t_rel * t_rel\n",
    "    refs = cp.exp(-1j * cp.pi * k_vals_gpu[:, None] * t2_rel[None, :])  # (K, N)\n",
    "\n",
    "    # Upload full signal once\n",
    "    y_gpu = cp.asarray(signal_noisy)\n",
    "\n",
    "    energy_rows = []\n",
    "    time_axis = []\n",
    "\n",
    "    # --- GPU FFT stage ---\n",
    "    t_fft_start = time.perf_counter()\n",
    "    for w0 in range(0, n_win, batch_size):\n",
    "        w_batch = list(range(w0, min(w0+batch_size, n_win)))\n",
    "        if not w_batch:\n",
    "            break\n",
    "\n",
    "        # Stack batch windows: (B, N)\n",
    "        chunks = []\n",
    "        for w in w_batch:\n",
    "            start = w * stride\n",
    "            end = start + chunk_len\n",
    "            if end > y_gpu.size:\n",
    "                continue\n",
    "            chunks.append(y_gpu[start:end])\n",
    "            time_axis.append(w * stride / fs)\n",
    "\n",
    "        if not chunks:\n",
    "            break\n",
    "\n",
    "        batch_gpu = cp.stack(chunks, axis=0)             # (B, N)\n",
    "        scratch = refs[None, :, :] * batch_gpu[:, None, :]   # (B, K, N)\n",
    "        Y = cp.fft.fft(scratch, n=nfft or chunk_len, axis=-1)\n",
    "        row_db_batch = 10 * cp.log10(cp.max(cp.abs(Y)**2, axis=-1) + 1e-30)\n",
    "\n",
    "        # Back to CPU\n",
    "        energy_rows.append(cp.asnumpy(row_db_batch))\n",
    "\n",
    "        if show_progress and (w0 % (100*batch_size) == 0):\n",
    "            print(f\"Processed {w0}/{n_win} windows\")\n",
    "\n",
    "    energy_map_db = np.vstack(energy_rows)\n",
    "    time_axis_s = np.array(time_axis)\n",
    "    t_fft_end = time.perf_counter()\n",
    "\n",
    "    if not return_candidates:\n",
    "        print(f\"[Timing] GPU FFT stage: {t_fft_end - t_fft_start:.2f} s \"\n",
    "              f\"(total {time.perf_counter() - t_total:.2f} s)\")\n",
    "        return energy_map_db, time_axis_s, k_vals\n",
    "\n",
    "    # --- Candidate picking ---\n",
    "    t_pick_start = time.perf_counter()\n",
    "\n",
    "    def process_row(row_db):\n",
    "        if row_skip_margin_db is not None:\n",
    "            if np.max(row_db) < (np.median(row_db) + row_skip_margin_db):\n",
    "                return []\n",
    "        pk, _ = find_peaks(row_db, prominence=row_prom_db)\n",
    "        if (max_peaks_per_row is not None) and (len(pk) > max_peaks_per_row):\n",
    "            top_idx = np.argsort(row_db[pk])[::-1][:max_peaks_per_row]\n",
    "            pk = pk[top_idx]\n",
    "        return pk.tolist()\n",
    "\n",
    "    if use_parallel:\n",
    "        row_k_peaks = Parallel(n_jobs=n_jobs, prefer=\"processes\")(\n",
    "            delayed(process_row)(row_db) for row_db in energy_map_db\n",
    "        )\n",
    "    else:\n",
    "        row_k_peaks = [process_row(row_db) for row_db in energy_map_db]\n",
    "\n",
    "    t_pick_end = time.perf_counter()\n",
    "\n",
    "    print(f\"[Timing] GPU FFT stage: {t_fft_end - t_fft_start:.2f} s\")\n",
    "    print(f\"[Timing] CPU candidate picking ({'parallel' if use_parallel else 'serial'} find_peaks): \"\n",
    "          f\"{t_pick_end - t_pick_start:.2f} s\")\n",
    "    print(f\"[Timing] Total Time: {time.perf_counter() - t_total:.2f} s\")\n",
    "\n",
    "    near_zero_mask = (np.abs(k_vals) <= (k0_eps_MHz_per_us * 1e12))\n",
    "\n",
    "    return energy_map_db, time_axis_s, k_vals, row_k_peaks, near_zero_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5167f5-3876-478d-94dc-edfc9507301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_scan_energy_window_gpu(chunk_gpu, fs, k_vals,\n",
    "                             nfft=None,\n",
    "                             nfft_mode: str = \"downpow2\",\n",
    "                             nfft_safety_margin: float = 1.05,\n",
    "                             max_df_hz: float | None = None,\n",
    "                             precomp_refs=None,\n",
    "                             scratch=None):\n",
    "    \"\"\"\n",
    "    Dechirp + FFT for all k, fully on GPU.\n",
    "    chunk_gpu: cp.ndarray (signal already on GPU)\n",
    "    \"\"\"\n",
    "    N = chunk_gpu.shape[0]\n",
    "    if nfft is None:\n",
    "        L = choose_nfft(N, fs,\n",
    "                        mode=nfft_mode,\n",
    "                        safety_margin=nfft_safety_margin,\n",
    "                        max_df_hz=max_df_hz)\n",
    "    else:\n",
    "        L = int(nfft)\n",
    "\n",
    "    if precomp_refs is None:\n",
    "        t_rel = cp.arange(N, dtype=cp.float32) / fs\n",
    "        t2_rel = t_rel * t_rel\n",
    "        refs = cp.exp(-1j * np.pi * k_vals[:, None] * t2_rel[None, :])\n",
    "    else:\n",
    "        refs = precomp_refs\n",
    "\n",
    "    if scratch is None:\n",
    "        scratch = cp.empty((len(k_vals), N), dtype=cp.complex64)\n",
    "\n",
    "    cp.multiply(refs, chunk_gpu[None, :], out=scratch)\n",
    "    Y = cp.fft.fft(scratch, n=L, axis=-1)\n",
    "\n",
    "    row_db = 10 * cp.log10(cp.max(cp.abs(Y) ** 2, axis=1) + 1e-30)\n",
    "    return row_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca97ecf-6a2e-4ede-9f3a-93d4cdb8e359",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f05f53bf-35e9-485a-be85-86464ae7b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_pow2(n: int) -> int:\n",
    "    return 1 << (int(n - 1).bit_length())\n",
    "\n",
    "def prev_pow2(n: int) -> int:\n",
    "    \"\"\"Largest power of two <= n (n>=1).\"\"\"\n",
    "    if n <= 1:\n",
    "        return 1\n",
    "    return 1 << ((int(n)).bit_length() - 1)\n",
    "\n",
    "def choose_nfft(N: int, fs: float,\n",
    "                mode: str = \"downpow2\",\n",
    "                safety_margin: float = 1.05,\n",
    "                max_df_hz: float | None = None) -> int:\n",
    "    \"\"\"\n",
    "    Decide an FFT length smaller than next_pow2(N) but not too small.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    N : slice length (samples)\n",
    "    fs : sample rate (Hz)\n",
    "    mode : one of {\"downpow2\", \"nopad\", \"next_pow2\"}\n",
    "      - \"downpow2\": nfft = largest power of two <= safety_margin * N\n",
    "      - \"nopad\":    nfft = N\n",
    "      - \"next_pow2\":nfft = next_pow2(N)  (original behavior)\n",
    "    safety_margin : multiplier >= 1.0 used by \"downpow2\"\n",
    "    max_df_hz : if set, enforce fs/nfft <= max_df_hz by increasing nfft\n",
    "                (only increases within strategy’s family)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nfft : int\n",
    "    \"\"\"\n",
    "    assert N >= 8, \"N too small\"\n",
    "    assert safety_margin >= 1.0, \"safety_margin should be >= 1.0\"\n",
    "\n",
    "    if mode == \"nopad\":\n",
    "        nfft = int(N)\n",
    "\n",
    "    elif mode == \"next_pow2\":\n",
    "        nfft = next_pow2(N)\n",
    "\n",
    "    elif mode == \"downpow2\":\n",
    "        # target is slightly above N, then round DOWN to a friendly pow2\n",
    "        target = max(int(np.ceil(N * safety_margin)), N)\n",
    "        nfft = prev_pow2(target)\n",
    "        # ensure we didn't round below something unreasonably small\n",
    "        # (you can relax/tighten this line if needed)\n",
    "        nfft = max(nfft, prev_pow2(N))  # don't go below the previous pow2 of N\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown nfft mode: {mode}\")\n",
    "\n",
    "    # Enforce bin-spacing guard if requested\n",
    "    if max_df_hz is not None:\n",
    "        # required nfft to meet df <= max_df_hz\n",
    "        needed = int(np.ceil(fs / max_df_hz))\n",
    "        if nfft < needed:\n",
    "            if mode == \"nopad\":\n",
    "                # bump just enough to satisfy df; mixed radix is fine\n",
    "                nfft = needed\n",
    "            elif mode in (\"downpow2\", \"next_pow2\"):\n",
    "                # bump to next power-of-two that meets df\n",
    "                nfft = next_pow2(needed)\n",
    "    return int(nfft)\n",
    "\n",
    "\n",
    "def k_scan_energy_window(chunk, fs, k_vals,\n",
    "                         nfft=None,\n",
    "                         nfft_mode: str = \"downpow2\",\n",
    "                         nfft_safety_margin: float = 1.05,\n",
    "                         max_df_hz: float | None = None,\n",
    "                         precomp_refs=None,\n",
    "                         scratch=None):\n",
    "    \"\"\"\n",
    "    Vectorized version:\n",
    "      - de-chirp by all k in k_vals using precomputed refs\n",
    "      - FFT along axis for all k\n",
    "      - return max-bin power (dB) for each k\n",
    "    \"\"\"\n",
    "    N = len(chunk)\n",
    "    if nfft is None:\n",
    "        L = choose_nfft(N, fs,\n",
    "                        mode=nfft_mode,\n",
    "                        safety_margin=nfft_safety_margin,\n",
    "                        max_df_hz=max_df_hz)\n",
    "    else:\n",
    "        L = int(nfft)\n",
    "\n",
    "    if precomp_refs is None:\n",
    "        # fallback: compute refs on the fly\n",
    "        t_rel = np.arange(N) / fs\n",
    "        t2_rel = t_rel * t_rel\n",
    "        refs = np.exp(-1j * np.pi * k_vals[:, None] * t2_rel[None, :]).astype(np.complex64)\n",
    "    else:\n",
    "        refs = precomp_refs\n",
    "\n",
    "    if scratch is None:\n",
    "        scratch = np.empty((len(k_vals), N), dtype=np.complex64)\n",
    "\n",
    "    # Broadcast multiply: dechirp all k in one go\n",
    "    np.multiply(refs, chunk[None, :], out=scratch)\n",
    "\n",
    "    # Batched FFT along axis=-1\n",
    "    Y = np.fft.fft(scratch, n=L, axis=-1)\n",
    "\n",
    "    # Max bin per row\n",
    "    row_db = 10 * np.log10((np.abs(Y)**2).max(axis=1) + 1e-30)\n",
    "    return row_db\n",
    "\n",
    "\n",
    "def build_energy_map_dechirp(signal_noisy, fs, total_duration_s,\n",
    "                             T_chunk=1e-3, stride_s=None,\n",
    "                             k_range=(-1.0e12, 1.0e12), k_step=0.1e12,\n",
    "                             nfft=None, nfft_mode=\"downpow2\",\n",
    "                             nfft_safety_margin=1.05,\n",
    "                             max_df_hz: float | None = None,\n",
    "                             show_progress=True,\n",
    "                             return_candidates=True,\n",
    "                             row_prom_db=4.5,\n",
    "                             row_skip_margin_db=6.0,\n",
    "                             k0_eps_MHz_per_us=0.05,\n",
    "                             max_peaks_per_row=None,\n",
    "                             k_vals_override=None,\n",
    "                             n_jobs=None):\n",
    "    \"\"\"\n",
    "    Build the dechirp-FFT energy map (CPU or GPU).\n",
    "    If use_gpu=True, runs on GPU without joblib.\n",
    "    \"\"\"\n",
    "    if stride_s is None:\n",
    "        stride_s = T_chunk\n",
    "\n",
    "    chunk_len = int(round(T_chunk * fs))\n",
    "    stride = int(round(stride_s * fs))\n",
    "    n_total = int(round(total_duration_s * fs))\n",
    "    n_win = 1 + max(0, int(np.floor((n_total - chunk_len) / stride)))\n",
    "\n",
    "    # --- build k grid ---\n",
    "    if k_vals_override is not None:\n",
    "        k_vals = np.array(k_vals_override, dtype=float)\n",
    "    else:\n",
    "        k_vals = np.arange(k_range[0], k_range[1] + 0.5*k_step, k_step, dtype=float)\n",
    "\n",
    "    # Precompute refs\n",
    "    N = chunk_len\n",
    "    t_rel = np.arange(N) / fs\n",
    "    t2_rel = t_rel * t_rel\n",
    "    precomp_refs = np.exp(-1j * np.pi * k_vals[:, None] * t2_rel[None, :]).astype(np.complex64)\n",
    "\n",
    "    # --- process windows ---\n",
    "    energy_rows = []\n",
    "    time_axis = []\n",
    "\n",
    "    # parallel CPU path\n",
    "    if n_jobs is None:\n",
    "        n_jobs = multiprocessing.cpu_count()\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_window)(\n",
    "            w, signal_noisy, fs, stride, chunk_len,\n",
    "            k_vals, nfft, nfft_mode,\n",
    "            nfft_safety_margin, max_df_hz,\n",
    "            precomp_refs\n",
    "        )\n",
    "        for w in range(n_win)\n",
    "    )\n",
    "\n",
    "    energy_rows = [row for row, _ in results if row is not None]\n",
    "    time_axis = [t0 for _, t0 in results if t0 is not None]\n",
    "\n",
    "    # --- stack results ---\n",
    "    energy_map_db = np.vstack(energy_rows) if energy_rows else np.empty((0, len(k_vals)))\n",
    "    time_axis_s = np.array(time_axis, dtype=float)\n",
    "\n",
    "    if not return_candidates:\n",
    "        return energy_map_db, time_axis_s, k_vals\n",
    "\n",
    "    # --- candidate gathering ---\n",
    "    row_k_peaks = []\n",
    "    if energy_map_db.size == 0:\n",
    "        near_zero_mask = (np.abs(k_vals) <= (k0_eps_MHz_per_us * 1e12))\n",
    "        return energy_map_db, time_axis_s, k_vals, row_k_peaks, near_zero_mask\n",
    "\n",
    "    near_zero_mask = (np.abs(k_vals) <= (k0_eps_MHz_per_us * 1e12))\n",
    "\n",
    "    for w in range(energy_map_db.shape[0]):\n",
    "        row_db = energy_map_db[w]\n",
    "\n",
    "        if row_skip_margin_db is not None:\n",
    "            if np.max(row_db) < (np.median(row_db) + row_skip_margin_db):\n",
    "                row_k_peaks.append([])\n",
    "                continue\n",
    "\n",
    "        pk, _ = find_peaks(row_db, prominence=row_prom_db)\n",
    "\n",
    "        if (max_peaks_per_row is not None) and (len(pk) > max_peaks_per_row):\n",
    "            top_idx = np.argsort(row_db[pk])[::-1][:max_peaks_per_row]\n",
    "            pk = pk[top_idx]\n",
    "\n",
    "        row_k_peaks.append(pk.tolist())\n",
    "\n",
    "    return energy_map_db, time_axis_s, k_vals, row_k_peaks, near_zero_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc3321-1da5-4b28-a11d-4d07d77f0e11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Identifying Signal Types (CW, LFMCW, or LFM pulses)\n",
    "Single Frequency Pulses will be handled later using the same functions after CW removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0633d7-6302-4521-84d8-5d410e10164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_pulse_chirp_rates_nms(\n",
    "    counts, k_vals, *,\n",
    "    sigma: float = 3.0,        # MAD threshold multiplier\n",
    "    min_count: float = 10.0,   # absolute floor on peak height\n",
    "    nms_bins: int = 3,         # suppress neighbors within ±nms_bins\n",
    "    smooth_bins: int = 0       # 0 = off, else odd window (e.g., 3 or 5)\n",
    "):\n",
    "    \"\"\"\n",
    "    Blind chirp-rate peak detection with robust threshold + 1-D NMS.\n",
    "    - No max-peaks cap.\n",
    "    - NMS is applied uniformly (including k≈0).\n",
    "    - Optional light smoothing for noisier histograms.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    peak_indices : (M,) int\n",
    "    peak_k_vals  : (M,) float  (parabolic sub-bin refinement)\n",
    "    peak_counts  : (M,) float  (from ORIGINAL counts, not smoothed)\n",
    "    \"\"\"\n",
    "    counts = np.asarray(counts, dtype=float)\n",
    "    k_vals = np.asarray(k_vals, dtype=float)\n",
    "    N = counts.size\n",
    "    if N == 0:\n",
    "        return np.array([], int), np.array([]), np.array([])\n",
    "\n",
    "    # ---- optional smoothing for the *detector path* only ----\n",
    "    if smooth_bins and smooth_bins > 1:\n",
    "        if smooth_bins % 2 == 0:\n",
    "            smooth_bins += 1  # ensure odd\n",
    "        kernel = np.ones(smooth_bins, dtype=float) / smooth_bins\n",
    "        counts_det = np.convolve(counts, kernel, mode=\"same\")\n",
    "    else:\n",
    "        counts_det = counts\n",
    "\n",
    "    # ---- robust threshold (MAD) on the detector signal ----\n",
    "    nz = counts_det[counts_det > 0]\n",
    "    if nz.size == 0:\n",
    "        return np.array([], int), np.array([]), np.array([])\n",
    "    med = np.median(nz)\n",
    "    mad = 1.4826 * np.median(np.abs(nz - med))\n",
    "    thr = max(float(min_count), med + sigma * mad)\n",
    "\n",
    "    # ---- initial candidates (no distance; NMS handles clustering) ----\n",
    "    cand, _ = find_peaks(counts_det, height=thr)\n",
    "    if cand.size == 0:\n",
    "        return np.array([], int), np.array([]), np.array([])\n",
    "\n",
    "    # ---- NMS in 1-D over original counts (strong→weak) ----\n",
    "    order = np.argsort(counts[cand])[::-1]\n",
    "    suppressed = np.zeros(N, dtype=bool)\n",
    "    picked = []\n",
    "    for idx in cand[order]:\n",
    "        if suppressed[idx]:\n",
    "            continue\n",
    "        picked.append(int(idx))\n",
    "        lo = max(0, idx - nms_bins)\n",
    "        hi = min(N, idx + nms_bins + 1)\n",
    "        suppressed[lo:hi] = True\n",
    "\n",
    "    picked = np.array(sorted(picked), dtype=int)\n",
    "\n",
    "    # ---- parabolic refinement (use ORIGINAL counts) ----\n",
    "    refined = []\n",
    "    for p in picked:\n",
    "        if p == 0 or p == N - 1:\n",
    "            refined.append(k_vals[p])\n",
    "            continue\n",
    "        c0, c1, c2 = counts[p - 1], counts[p], counts[p + 1]\n",
    "        denom = (c0 - 2.0 * c1 + c2)\n",
    "        delta = 0.0 if denom == 0.0 else 0.5 * (c0 - c2) / denom  # sub-bin offset\n",
    "        # handle irregular grids: average left/right steps\n",
    "        step = 0.5 * ((k_vals[p] - k_vals[p - 1]) + (k_vals[p + 1] - k_vals[p]))\n",
    "        refined.append(k_vals[p] + delta * step)\n",
    "\n",
    "    return picked, np.array(refined, dtype=float), counts[picked]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493c3a1-bcb2-4409-80fa-3627df68085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_candidates(\n",
    "    k_vals,\n",
    "    row_k_peaks,\n",
    "    *,\n",
    "    min_support_frac_lfmcw=0.30,\n",
    "    min_support_frac_cw=0.30,\n",
    "    min_support_windows=None,\n",
    "    zero_band_bins=1,         # how many bins around k=0 to treat as “CW region”\n",
    "    nms_sigma=3.0,\n",
    "    nms_min_count=10,\n",
    "    nms_bins=3,\n",
    "    smooth_bins=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Detect and classify candidate k bins into LFMCW, CW, or pulse types.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1. Compute support_counts (# of windows where each k bin was active).\n",
    "    2. Run blind chirp-rate detection with NMS.\n",
    "    3. Classify refined peaks into:\n",
    "         - CW: near-zero k bins with enough support\n",
    "         - LFMCW: nonzero bins with enough support (fraction or window count)\n",
    "         - Pulse: all other NMS peaks not claimed as CW/LFMCW\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : list[dict]\n",
    "        Each dict has keys: {'mode','k_hat','peak_idx','support_sum'}\n",
    "    support_counts : (K,) int\n",
    "    support_frac   : (K,) float\n",
    "    \"\"\"\n",
    "    K = len(k_vals)\n",
    "    n_win = len(row_k_peaks)\n",
    "\n",
    "    # --- 1. Build support histogram ---\n",
    "    support_counts = np.zeros(K, dtype=int)\n",
    "    for idxs in row_k_peaks:\n",
    "        if idxs is None or len(idxs) == 0:\n",
    "            continue\n",
    "        support_counts[idxs] += 1\n",
    "\n",
    "    support_frac = support_counts / max(1, n_win)\n",
    "\n",
    "    # --- 2. Detect peaks with NMS ---\n",
    "    peak_idx, k_refined, peak_counts = detect_pulse_chirp_rates_nms(\n",
    "        support_counts, k_vals,\n",
    "        sigma=nms_sigma,\n",
    "        min_count=nms_min_count,\n",
    "        nms_bins=nms_bins,\n",
    "        smooth_bins=smooth_bins,\n",
    "    )\n",
    "\n",
    "    # --- 3. Classification ---\n",
    "    results = []\n",
    "    k0 = int(np.argmin(np.abs(k_vals)))  # closest-to-zero index\n",
    "    lo = max(0, k0 - zero_band_bins)\n",
    "    hi = min(K, k0 + zero_band_bins + 1)\n",
    "\n",
    "    for idx, k_val, cnt in zip(peak_idx, k_refined, peak_counts):\n",
    "        mode = \"pulse\"\n",
    "        frac = support_frac[idx]\n",
    "\n",
    "        if lo <= idx < hi and frac >= min_support_frac_cw:\n",
    "            mode = \"cw\"\n",
    "        elif frac >= min_support_frac_lfmcw:\n",
    "            mode = \"lfmcw\"\n",
    "\n",
    "        results.append({\n",
    "            \"mode\": mode,\n",
    "            \"k_hat\": float(k_val),\n",
    "            \"peak_idx\": int(idx),\n",
    "            \"support_sum\": int(cnt),\n",
    "        })\n",
    "\n",
    "    return results, support_counts, support_frac\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf494c5-1cbe-417a-84f2-f114be284db8",
   "metadata": {},
   "source": [
    "## 3. LFMCW/CW Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ff798-6f64-476e-b6fc-c24c5326a9cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Filtering and Coupon Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb608f-30f2-49a0-a077-239a0bf9f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows_supporting_khat(k_hat, k_vals, row_k_peaks, *, max_bin_delta=1): \n",
    "    k_idx_near = int(np.argmin(np.abs(k_vals - k_hat)))\n",
    "    hits = []\n",
    "    for w, idxs in enumerate(row_k_peaks):\n",
    "        if idxs is None or len(idxs) == 0:\n",
    "            continue\n",
    "        idxs = np.asarray(idxs)\n",
    "        if np.any(np.abs(idxs - k_idx_near) <= max_bin_delta):\n",
    "            hits.append(w)\n",
    "    return hits\n",
    "\n",
    "def pick_blocks_from_hits(hits, X): \n",
    "    if not hits:\n",
    "        return []\n",
    "    hits = sorted(hits)\n",
    "    if len(hits) <= X:\n",
    "        return [hits]\n",
    "    mid = len(hits) // 2\n",
    "    blocks = [\n",
    "        hits[:X],\n",
    "        hits[max(0, mid - X//2): max(0, mid - X//2) + X],\n",
    "        hits[-X:]\n",
    "    ]\n",
    "    # dedupe overlapping blocks\n",
    "    uniq, seen = [], set()\n",
    "    for b in blocks:\n",
    "        t = tuple(b)\n",
    "        if t not in seen:\n",
    "            uniq.append(b); seen.add(t)\n",
    "    return uniq\n",
    "\n",
    "def make_nodes_from_windows(block_windows, T_chunk_s): \n",
    "    # minimal nodes for your backend\n",
    "    return [{\"w\": int(w), \"t0\": float(w) * T_chunk_s} for w in sorted(block_windows)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed25c62-580b-4c64-88d9-94f5b6f2d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_segment(sig, fs, T_chunk_s, nodes_block): \n",
    "    t0 = nodes_block[0][\"t0\"]\n",
    "    t1 = nodes_block[-1][\"t0\"] + T_chunk_s\n",
    "    i0, i1 = max(0, int(round(t0*fs))), min(len(sig), int(round(t1*fs)))\n",
    "    if i1 <= i0:\n",
    "        return np.zeros(1, dtype=np.complex64), float(t0), 0.0\n",
    "    x = sig[i0:i1].astype(np.complex64, copy=False)\n",
    "    return x, t0, (i1 - i0) / fs\n",
    "\n",
    "def _alpha_from_k(k_hz_per_s, fs, T_seg): \n",
    "    a = 1.0 + (2.0/np.pi)*np.arctan((k_hz_per_s*T_seg)/fs)\n",
    "    return float(np.clip(a, 1e-3, 2.0-1e-3))\n",
    "def _block_medians(pdws): \n",
    "    if not pdws:\n",
    "        return dict(N=0, PRI_us=np.nan, PW_us=np.nan, Amp_med=np.nan, f_center_Hz=np.nan)\n",
    "    toas = np.sort([p[\"TOA_global_us\"] for p in pdws])\n",
    "    pris = np.diff(toas) if len(toas) >= 2 else np.array([np.nan])\n",
    "    return dict(\n",
    "        N=len(pdws),\n",
    "        PRI_us=float(np.nanmedian(pris)) if pris.size else np.nan,\n",
    "        PW_us =float(np.nanmedian([p[\"PW_us\"] for p in pdws])),\n",
    "        Amp_med=float(np.nanmedian([p[\"Amp_med\"] for p in pdws])),\n",
    "        f_center_Hz=float(np.nanmedian([p.get(\"f_center_Hz\", np.nan) for p in pdws])),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed717f5-91ce-4bcf-860f-426519326982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_all_nodes_df(\n",
    "    clusters,\n",
    "    *,\n",
    "    k_vals, row_k_peaks,\n",
    "    signal, fs, T_chunk_s, X,\n",
    "    peak_frac=0.5,          # CW FFT peak picking threshold\n",
    "    sanity_threshold=0.8,   # fraction of block span required\n",
    "    lfmcw_sanity_rel_margin=0.05,   # 5% slack\n",
    "    lfmcw_sanity_abs_us=1.0,        # +1 µs slack\n",
    "    downsample_factor=1,\n",
    "    AMP_MED_MIN=0.1,\n",
    "    **extract_kwargs\n",
    "):\n",
    "    dfs = []\n",
    "    for cl in clusters:\n",
    "        mode  = cl['mode']\n",
    "        k_hat = float(cl['k_hat'])\n",
    "\n",
    "        # Find supporting windows\n",
    "        hits = windows_supporting_khat(k_hat, k_vals, row_k_peaks, max_bin_delta=1)\n",
    "        if not hits:\n",
    "            dfs.append(pd.DataFrame([{\"error\": f\"{mode} k={k_hat:.3e}: no supporting windows\"}]))\n",
    "            continue\n",
    "\n",
    "        # Pick coupon blocks\n",
    "        X_eff = min(X, len(hits)) if len(hits) < 3*X else X\n",
    "        blocks = pick_blocks_from_hits(hits, X_eff)\n",
    "\n",
    "        summaries, all_pdws, all_f0s = [], [], []   # collect PDWs + tone freqs\n",
    "        for bw in blocks:\n",
    "            nodes_block = make_nodes_from_windows(bw, T_chunk_s)\n",
    "\n",
    "            if mode == \"lfmcw\":\n",
    "                pdws = _extract_block_lfmcw_pdws(\n",
    "                    nodes_block,\n",
    "                    signal=signal, fs=fs, T_chunk_s=T_chunk_s,\n",
    "                    k_hz_per_s=k_hat,\n",
    "                    downsample_factor=downsample_factor,\n",
    "                    **extract_kwargs\n",
    "                )\n",
    "                \n",
    "            elif mode == \"cw\":\n",
    "                # FFT block to detect tones\n",
    "                x_seg, t0_start, T_seg = _build_segment(signal, fs, T_chunk_s, nodes_block)\n",
    "                if len(x_seg) == 0:\n",
    "                    pdws, f0s = [], []\n",
    "                else:\n",
    "                    Nfft = 1 << (len(x_seg)-1).bit_length()\n",
    "                    Xfft = np.fft.fft(x_seg * np.hanning(len(x_seg)), n=Nfft)\n",
    "                    mag  = np.abs(Xfft)\n",
    "                    f_axis = np.fft.fftfreq(Nfft, 1/fs)\n",
    "\n",
    "                    pk, props = find_peaks(mag, height=np.max(mag)*peak_frac)\n",
    "                    f0s = [float(f_axis[p]) for p in pk] if pk.size else [float(f_axis[np.argmax(mag)])]\n",
    "                    \n",
    "                    pdws = []\n",
    "                    for f0_hz in f0s:\n",
    "                        pdws.extend(\n",
    "                            _extract_block_cw_pdws(\n",
    "                                nodes_block,\n",
    "                                signal=signal, fs=fs, T_chunk_s=T_chunk_s,\n",
    "                                f0_hz=f0_hz,\n",
    "                                downsample_factor=downsample_factor,\n",
    "                                **extract_kwargs\n",
    "                            )\n",
    "                        )\n",
    "                all_f0s.extend(f0s)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown cluster mode: {mode}\")\n",
    "\n",
    "            if pdws:\n",
    "                all_pdws.extend(pdws)\n",
    "                summaries.append(_block_medians(pdws))\n",
    "        \n",
    "        if not summaries:\n",
    "            dfs.append(pd.DataFrame([{\"error\": f\"{mode} k={k_hat:.3e}: no PDWs extracted\"}]))\n",
    "            continue\n",
    "        # --- Aggregation ---\n",
    "        if mode == \"lfmcw\":\n",
    "            start_time_us = float(min(p[\"TOA_global_us\"] for p in all_pdws))\n",
    "            PW_us  = float(np.nanmedian([s[\"PW_us\"] for s in summaries]))\n",
    "            PRI_us = float(np.nanmedian([s[\"PRI_us\"] for s in summaries]))\n",
    "            Amp_med = float(np.nanmedian([s[\"Amp_med\"] for s in summaries]))\n",
    "            if not np.isfinite(Amp_med) or Amp_med < AMP_MED_MIN:\n",
    "                continue\n",
    "\n",
    "            vals = [s.get(\"f_center_Hz\", np.nan) for s in summaries]\n",
    "            finite_vals = [v for v in vals if not np.isnan(v)]\n",
    "            f_center_Hz = float(np.median(finite_vals)) if finite_vals else np.nan\n",
    "\n",
    "            k_mhz_per_us = k_hat / 1e12\n",
    "            f_center_MHz = f_center_Hz / 1e6 if not np.isnan(f_center_Hz) else np.nan\n",
    "            BW_MHz = k_mhz_per_us * PW_us\n",
    "            f_start_MHz = f_center_MHz - 0.5 * BW_MHz if not np.isnan(f_center_MHz) else np.nan\n",
    "            f_end_MHz   = f_center_MHz + 0.5 * BW_MHz if not np.isnan(f_center_MHz) else np.nan\n",
    "            ptype = \"LFMCW\"\n",
    "\n",
    "            # ---- LFMCW sanity: PW <= PRI (with slack) ----\n",
    "            if np.isfinite(PW_us) and np.isfinite(PRI_us) and PRI_us > 0:\n",
    "                pri_with_margin = PRI_us * (1.0 + lfmcw_sanity_rel_margin) + lfmcw_sanity_abs_us\n",
    "                lfmcw_sanity_ok = bool(PW_us <= pri_with_margin)\n",
    "            else:\n",
    "                lfmcw_sanity_ok = pd.NA  # unknown PRI/PW → indeterminate\n",
    "\n",
    "            dfs.append(pd.DataFrame([{\n",
    "                \"TOA (us)\": start_time_us,\n",
    "                \"PW (us)\": PW_us,\n",
    "                \"PRI (us)\": PRI_us,\n",
    "                \"Envelope Amplitude\": Amp_med,\n",
    "                \"Center Freq (MHz)\": f_center_MHz,\n",
    "                \"Chirp Rate (MHz/us)\": k_mhz_per_us,\n",
    "                \"Bandwidth (MHz)\": BW_MHz,\n",
    "                \"Start Freq (MHz)\": f_start_MHz,\n",
    "                \"End Freq (MHz)\": f_end_MHz,\n",
    "                \"Pulse Type\": ptype,\n",
    "                \"pulse_sanity_ok\": lfmcw_sanity_ok,   # now boolean/<NA>\n",
    "            }]))\n",
    "\n",
    "        elif mode == \"cw\":\n",
    "\n",
    "            # Use FFT-detected frequencies as the true tone list\n",
    "            finite_f0s = [f for f in all_f0s if not np.isnan(f)]\n",
    "            if not finite_f0s:\n",
    "                continue\n",
    "\n",
    "            # unique baseband tones in MHz (rounded like before)\n",
    "            f_center_list = sorted(list(set([round(f/1e6, 2) for f in finite_f0s])))\n",
    "\n",
    "            # block-level sanity check\n",
    "            block_span_us = X_eff * T_chunk_s * 1e6\n",
    "            pw_blocks = [s[\"PW_us\"] for s in summaries if not np.isnan(s[\"PW_us\"])]\n",
    "            pulse_sanity_ok = all(pw >= sanity_threshold * block_span_us for pw in pw_blocks)\n",
    "\n",
    "            rows = []\n",
    "\n",
    "            # per-tone summarization\n",
    "            for f0_MHz in f_center_list:\n",
    "\n",
    "                # find all PDWs generated for this tone\n",
    "                tone_pdws = [p for p in all_pdws\n",
    "                             if \"tone_freq_hz\" in p\n",
    "                             and round(p[\"tone_freq_hz\"]/1e6, 2) == f0_MHz]\n",
    "\n",
    "                if not tone_pdws:\n",
    "                    continue\n",
    "\n",
    "                # per-tone stats\n",
    "                TOA_us = float(min(p[\"TOA_global_us\"] for p in tone_pdws))\n",
    "                PW_us  = (hits[-1] - hits[0] + 1) * T_chunk_s * 1e6\n",
    "                Amp    = float(np.nanmedian([p[\"Amp_med\"]  for p in tone_pdws]))\n",
    "                if not np.isfinite(Amp) or Amp < AMP_MED_MIN:\n",
    "                    continue\n",
    "\n",
    "                rows.append({\n",
    "                    \"TOA (us)\": TOA_us,\n",
    "                    \"PW (us)\": PW_us,\n",
    "                    \"PRI (us)\": np.nan,\n",
    "                    \"Envelope Amplitude\": Amp,\n",
    "                    \"Center Freq (MHz)\": f0_MHz,\n",
    "                    \"Chirp Rate (MHz/us)\": 0.0,\n",
    "                    \"Bandwidth (MHz)\": 0.0,\n",
    "                    \"Start Freq (MHz)\": f0_MHz,\n",
    "                    \"End Freq (MHz)\": f0_MHz,\n",
    "                    \"Pulse Type\": \"CW\",\n",
    "                    \"pulse_sanity_ok\": pulse_sanity_ok,\n",
    "                })\n",
    "\n",
    "            dfs.append(pd.DataFrame(rows))\n",
    "            continue\n",
    "\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec55a75-74fb-404b-ad94-0ba077ede54d",
   "metadata": {},
   "source": [
    "#### CW Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6a561b7-0ac9-4b82-bd4b-6d822b61da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_block_cw_pdws(\n",
    "    nodes_block, *,\n",
    "    signal, fs, T_chunk_s,\n",
    "    f0_hz,                      # baseband CW frequency estimate\n",
    "    lpf_bw_hz=200e3,            # low-pass cutoff around DC\n",
    "    numtaps=513,\n",
    "    window_us=5.0,\n",
    "    min_sustain_us=5.0,\n",
    "    thr_pct=50,\n",
    "    amp_floor_pct=90,\n",
    "    downsample_factor=1\n",
    "):\n",
    "    # 1) time span\n",
    "    x_seg, t0_start, T_seg = _build_segment(signal, fs, T_chunk_s, nodes_block)\n",
    "    if len(x_seg) == 0:\n",
    "        return []\n",
    "\n",
    "    # 2) heterodyne to DC\n",
    "    t_seg = t0_start + np.arange(len(x_seg))/fs\n",
    "    x_bb  = x_seg * np.exp(-1j*2*np.pi*f0_hz*t_seg)\n",
    "\n",
    "    # 3) LPF\n",
    "    lpf_taps = firwin(numtaps, lpf_bw_hz, fs=fs)\n",
    "    x_filt = lfilter(lpf_taps, 1.0, x_bb)\n",
    "\n",
    "    # 4) Extract PDWs (same baseband extractor as LFMCW)\n",
    "    pulses, bounds, mask, _clean, pdws = baseband_PDW_extractor(\n",
    "        x_filt, fs,\n",
    "        k_hz_per_s=0.0,\n",
    "        t0_global_s=t0_start,\n",
    "        window_us=window_us,\n",
    "        min_sustain_us=min_sustain_us,\n",
    "        threshold_percentile=thr_pct,\n",
    "        amp_floor_pct=amp_floor_pct,\n",
    "        downsample_factor=downsample_factor,\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    for p in pdws:\n",
    "        p[\"tone_freq_hz\"] = f0_hz\n",
    "\n",
    "    return pdws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c8549b-eb00-40f1-9664-0cc7da55d87f",
   "metadata": {},
   "source": [
    "#### LFMCW Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0b95d7b-3b0e-4b38-8f3f-020d20233649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_block_lfmcw_pdws(nodes_block, *, signal, fs, T_chunk_s, k_hz_per_s, half_bin=10,\n",
    "                              prom_db=12.0, window_us=5.0, min_sustain_us=5.0, thr_pct=50, amp_floor_pct=90,\n",
    "                              downsample_factor=1): \n",
    "    # 1) time span for this block\n",
    "    x_seg, t0_start, T_seg = _build_segment(signal, fs, T_chunk_s, nodes_block)\n",
    "\n",
    "    # 2) FrFT at α(k,T_seg); find peaks\n",
    "    alpha = _alpha_from_k(k_hz_per_s, fs, T_seg)\n",
    "    Xa = frft(x_seg, alpha)                   \n",
    "    mag_db = 20*np.log10(np.abs(Xa)+1e-12)\n",
    "    base = np.median(mag_db)\n",
    "    peak_dist = int(max(1, round(fs * T_chunk_s)))\n",
    "    peaks, _ = find_peaks(mag_db, height=base+prom_db, distance=peak_dist)\n",
    "    max_val = np.max(mag_db)\n",
    "    valid = mag_db[peaks] > (max_val - 6)  # within 6 dB of strongest\n",
    "    peaks = peaks[valid]\n",
    "    # 3) For each peak: gate → inverse → extract PDWs (may return multiple pulses)\n",
    "    pdws = []\n",
    "    for p in peaks:\n",
    "        lo, hi = max(0, p-half_bin), min(len(Xa), p+half_bin+1)\n",
    "\n",
    "        G = np.zeros_like(Xa); G[lo:hi] = Xa[lo:hi]\n",
    "        y = frft(G, -alpha)                    \n",
    "        pulses, bounds, mask, _clean, pdw_list = baseband_PDW_extractor(\n",
    "            y, fs,\n",
    "            k_hz_per_s=k_hz_per_s,\n",
    "            t0_global_s=t0_start,\n",
    "            window_us=window_us,\n",
    "            min_sustain_us=min_sustain_us,\n",
    "            threshold_percentile=thr_pct,\n",
    "            amp_floor_pct=amp_floor_pct,\n",
    "            downsample_factor=downsample_factor\n",
    "        )\n",
    "        pdws.extend(pdw_list)\n",
    "    return pdws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5afdd-0863-4bec-95a7-ba02d55b9a47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CW Notching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d67727-4445-4b23-b9ed-4fef9e88aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notch_cws_fft_bins_gpu(signal, fs, cw_freqs_hz, *,\n",
    "                           T_chunk_s, nfft_mode=\"nopad\",\n",
    "                           nfft_safety_margin=1.05,\n",
    "                           max_df_hz=None,\n",
    "                           half_bins=3):\n",
    "    \"\"\"\n",
    "    GPU batched version of notch_cws_fft_bins.\n",
    "    Removes CW tones by zeroing ±half_bins around each CW freq in FFT domain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : ndarray (complex)\n",
    "        Input IQ samples (NumPy or CuPy).\n",
    "    fs : float\n",
    "        Sampling rate [Hz].\n",
    "    cw_freqs_hz : list or array of float\n",
    "        CW freqs to notch (Hz).\n",
    "    T_chunk_s : float\n",
    "        Chunk duration [s].\n",
    "    nfft_mode, nfft_safety_margin, max_df_hz :\n",
    "        Passed to choose_nfft.\n",
    "    half_bins : int\n",
    "        Half-width of notch (bins).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    residual : CuPy ndarray (complex64)\n",
    "        Signal after CW notching, living on GPU.\n",
    "    (half_bins, L_win) : tuple\n",
    "        Diagnostics (same as CPU version).\n",
    "    \"\"\"\n",
    "    # Ensure CuPy array\n",
    "    signal_gpu = cp.asarray(signal, dtype=cp.complex64)\n",
    "\n",
    "    N_total = signal_gpu.size\n",
    "    N_chunk = int(round(T_chunk_s * fs))\n",
    "    n_chunks = N_total // N_chunk\n",
    "    if n_chunks == 0:\n",
    "        return signal_gpu, (half_bins, N_chunk)\n",
    "\n",
    "    # FFT length\n",
    "    L_win = choose_nfft(N_chunk, fs, mode=nfft_mode,\n",
    "                        safety_margin=nfft_safety_margin,\n",
    "                        max_df_hz=max_df_hz)\n",
    "\n",
    "    # Reshape into [n_chunks, N_chunk], zero-pad to [n_chunks, L_win]\n",
    "    chunks = signal_gpu[:n_chunks * N_chunk].reshape(n_chunks, N_chunk)\n",
    "    X_time = cp.zeros((n_chunks, L_win), dtype=cp.complex64)\n",
    "    X_time[:, :N_chunk] = chunks\n",
    "\n",
    "    # FFT along axis=1\n",
    "    X_freq = cp.fft.fft(X_time, n=L_win, axis=1)\n",
    "\n",
    "    # Build a single boolean notch mask over bins\n",
    "    f_axis = cp.fft.fftfreq(L_win, d=1/fs)\n",
    "    notch_mask = cp.zeros(L_win, dtype=cp.bool_)\n",
    "    for f0 in cw_freqs_hz or []:\n",
    "        i0 = int(cp.argmin(cp.abs(f_axis - float(f0))))\n",
    "        lo = max(0, i0 - half_bins)\n",
    "        hi = min(L_win, i0 + half_bins + 1)\n",
    "        notch_mask[lo:hi] = True\n",
    "\n",
    "    # Apply notch mask to all rows (broadcast)\n",
    "    X_freq[:, notch_mask] = 0.0\n",
    "\n",
    "    # IFFT back\n",
    "    X_ifft = cp.fft.ifft(X_freq, n=L_win, axis=1)[:, :N_chunk]\n",
    "\n",
    "    # Flatten back to 1-D\n",
    "    out = X_ifft.reshape(-1).astype(cp.complex64, copy=False)\n",
    "\n",
    "    return out, (half_bins, L_win)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79aa26a-a46a-4c6e-932d-2560a8d17fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notch_cws_fft_bins_fast(signal, fs, cw_freqs_hz, *,\n",
    "                            T_chunk_s, nfft_mode=\"nopad\",\n",
    "                            nfft_safety_margin=1.05,\n",
    "                            max_df_hz=None,\n",
    "                            half_bins=3,\n",
    "                            taper=True,\n",
    "                            batch_chunks=64):\n",
    "    \"\"\"\n",
    "    Drop-in faster version of notch_cws_fft_bins with identical behavior.\n",
    "\n",
    "    Speed tricks (no logic changes):\n",
    "      - Precompute notch bin indices once; use a single boolean mask.\n",
    "      - Batch chunks and use 2D FFT along axis=1.\n",
    "      - Avoids work when no CW freqs.\n",
    "      - Reuses work arrays to reduce allocations.\n",
    "\n",
    "    Parameters (same as original) + batch_chunks:\n",
    "      batch_chunks : int\n",
    "          Number of time-chunks to process per batch (tune to your RAM/CPU).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    residual : complex ndarray (complex64)\n",
    "    (half_bins, L_win) : tuple for diagnostics (unchanged)\n",
    "    \"\"\"\n",
    "    # --- Setup identical to original ---\n",
    "    signal = np.asarray(signal)\n",
    "    N_total = signal.size\n",
    "    N_chunk = int(round(T_chunk_s * fs))\n",
    "\n",
    "    # Truncate tail to full chunks (identical behavior to your loop)\n",
    "    n_chunks = N_total // N_chunk\n",
    "    if n_chunks == 0:\n",
    "        return signal.astype(np.complex64, copy=True), (half_bins, N_chunk)\n",
    "\n",
    "    L_win = choose_nfft(N_chunk, fs, mode=nfft_mode,\n",
    "                        safety_margin=nfft_safety_margin,\n",
    "                        max_df_hz=max_df_hz)\n",
    "\n",
    "    # Early exit: if no tones, skip all FFTs and just return a typed copy\n",
    "    if cw_freqs_hz is None or len(cw_freqs_hz) == 0:\n",
    "        out = signal[:n_chunks * N_chunk].astype(np.complex64, copy=True)\n",
    "        return out, (half_bins, L_win)\n",
    "\n",
    "    # Frequency axis of the L_win FFT (same as original)\n",
    "    f_axis = np.fft.fftfreq(L_win, d=1/fs)\n",
    "\n",
    "    # Precompute Hann (if used), padded to L_win only when applying FFT\n",
    "    win = np.hanning(N_chunk).astype(np.float32) if taper else None\n",
    "\n",
    "    # --- Build one boolean notch mask over FFT bins (vectorized zeroing) ---\n",
    "    notch_mask = np.zeros(L_win, dtype=bool)\n",
    "    # exact same \"nearest-bin\" selection as original\n",
    "    for f0 in cw_freqs_hz:\n",
    "        i0 = int(np.argmin(np.abs(f_axis - float(f0))))\n",
    "        lo = max(0, i0 - half_bins)\n",
    "        hi = min(L_win, i0 + half_bins + 1)\n",
    "        notch_mask[lo:hi] = True\n",
    "\n",
    "    # --- Allocate output and working buffers ---\n",
    "    out = np.empty(n_chunks * N_chunk, dtype=np.complex64)\n",
    "\n",
    "    # A small helper to process a batch of contiguous chunks\n",
    "    def process_batch(bstart, bend):\n",
    "        bsize = bend - bstart\n",
    "        # Build a [bsize, L_win] complex64 buffer\n",
    "        X_time = np.zeros((bsize, L_win), dtype=np.complex64, order='C')\n",
    "\n",
    "        # Copy chunks and apply window (if any)\n",
    "        for bi, w in enumerate(range(bstart, bend)):\n",
    "            s0 = w * N_chunk\n",
    "            s1 = s0 + N_chunk\n",
    "            seg = signal[s0:s1].astype(np.complex64, copy=False)\n",
    "            if taper:\n",
    "                seg = seg * win  # broadcasts to complex64\n",
    "            X_time[bi, :N_chunk] = seg  # zero-padded tail already zeros\n",
    "\n",
    "        # FFT along axis=1 (each row is a chunk)\n",
    "        X_freq = np.fft.fft(X_time, n=L_win, axis=1)\n",
    "\n",
    "        # Notch (vectorized)\n",
    "        X_freq[:, notch_mask] = 0.0\n",
    "\n",
    "        # IFFT back and write first N_chunk samples per row\n",
    "        X_ifft = np.fft.ifft(X_freq, n=L_win, axis=1)[:, :N_chunk].astype(np.complex64, copy=False)\n",
    "        # Store into output\n",
    "        for bi, w in enumerate(range(bstart, bend)):\n",
    "            s0 = w * N_chunk\n",
    "            s1 = s0 + N_chunk\n",
    "            out[s0:s1] = X_ifft[bi]\n",
    "\n",
    "    # --- Batch over all chunks to limit memory while keeping vectorization ---\n",
    "    if batch_chunks is None or batch_chunks < 1:\n",
    "        batch_chunks = 64  # safe default\n",
    "\n",
    "    w = 0\n",
    "    while w < n_chunks:\n",
    "        w_end = min(n_chunks, w + batch_chunks)\n",
    "        process_batch(w, w_end)\n",
    "        w = w_end\n",
    "\n",
    "    # Append untouched tail exactly like original (original dropped it; keep identical)\n",
    "    # Your original function *drops* the tail past last full chunk.\n",
    "    # To be bit-identical, we will drop the tail too.\n",
    "    # If you'd rather keep the tail unchanged, uncomment the lines below:\n",
    "    # tail = signal[n_chunks * N_chunk:]\n",
    "    # if tail.size:\n",
    "    #     out = np.concatenate([out, tail.astype(np.complex64, copy=False)])\n",
    "\n",
    "    return out, (half_bins, L_win)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "936cd74d-0ce1-46c7-95e8-034a24f79f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notch_cws_fft_bins(signal, fs, cw_freqs_hz, *,\n",
    "                       T_chunk_s, nfft_mode=\"nopad\",\n",
    "                       nfft_safety_margin=1.05,\n",
    "                       max_df_hz=None,\n",
    "                       half_bins=3,\n",
    "                       taper=True):\n",
    "    \"\"\"\n",
    "    Remove CW tones in per-window FFTs by zeroing ±half_bins around each tone.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : complex ndarray\n",
    "        Input IQ samples.\n",
    "    fs : float\n",
    "        Sampling rate [Hz].\n",
    "    cw_freqs_hz : list of float\n",
    "        CW center freqs to notch (Hz).\n",
    "    T_chunk_s : float\n",
    "        Chunk (window) duration [s] (same as Stage-4).\n",
    "    nfft_mode, nfft_safety_margin, max_df_hz :\n",
    "        Passed to choose_nfft (Stage-4 FFT length choice).\n",
    "    half_bins : int\n",
    "        Half-width of notch in FFT bins (>=1).\n",
    "    taper : bool\n",
    "        Apply Hann taper before FFT to reduce leakage.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    residual : complex ndarray\n",
    "        Signal after per-window CW notching.\n",
    "    (half_bins, L_win) : tuple returned as well for diagnostics.\n",
    "    \"\"\"\n",
    "    N_chunk = int(round(T_chunk_s * fs))\n",
    "    L_win = choose_nfft(N_chunk, fs, mode=nfft_mode,\n",
    "                        safety_margin=nfft_safety_margin,\n",
    "                        max_df_hz=max_df_hz)\n",
    "    f_axis = np.fft.fftfreq(L_win, d=1/fs)\n",
    "\n",
    "    win = np.hanning(N_chunk).astype(np.float32) if taper else None\n",
    "    n_chunks = len(signal) // N_chunk\n",
    "    out = np.empty_like(signal, dtype=np.complex64)\n",
    "\n",
    "    for w in range(n_chunks):\n",
    "        s0, s1 = w * N_chunk, (w + 1) * N_chunk\n",
    "        seg = signal[s0:s1].astype(np.complex64, copy=False)\n",
    "        if taper:\n",
    "            seg = seg * win\n",
    "\n",
    "        X = np.fft.fft(seg, n=L_win)\n",
    "\n",
    "        for f0 in cw_freqs_hz or []:\n",
    "            i0 = int(np.argmin(np.abs(f_axis - float(f0))))\n",
    "            lo = max(0, i0 - half_bins)\n",
    "            hi = min(L_win, i0 + half_bins + 1)\n",
    "            X[lo:hi] = 0.0\n",
    "\n",
    "        seg_filt = np.fft.ifft(X, n=L_win)[:N_chunk].astype(np.complex64)\n",
    "        out[s0:s1] = seg_filt\n",
    "\n",
    "    return out, (half_bins, L_win)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4458c3fe-4c25-44e8-9496-f714d8a77ceb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Node Building for Pulses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71076cb-1f6d-44e0-bbf7-4dab36110c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_buckets(peaks, k_vals, k_refined, *, width_bins=1):\n",
    "    \"\"\"\n",
    "    Create buckets around each detected slope. \n",
    "    Each detected k_refined has a small set of raw bins associated with it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    peaks : array of ints\n",
    "        Indices of detected peak bins.\n",
    "    k_vals : array of floats\n",
    "        Chirp rate grid (Hz/s).\n",
    "    k_refined : array of floats\n",
    "        Refined chirp rates from parabolic interpolation.\n",
    "    width_bins : int\n",
    "        Half-width (in bins) around each peak to include.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    buckets : list of dict\n",
    "        Each dict has keys: kid, k_refined, bin_indices (np.ndarray).\n",
    "    bin2kid : np.ndarray\n",
    "        For each k-bin index, which kid it belongs to (or -1).\n",
    "    \"\"\"\n",
    "    N = len(k_vals)\n",
    "    K = len(peaks)\n",
    "    buckets, bin2kid = [], -np.ones(N, dtype=int)\n",
    "\n",
    "    for kid, p in enumerate(peaks):\n",
    "        L = max(0, p - width_bins)\n",
    "        R = min(N - 1, p + width_bins)\n",
    "        idxs = np.arange(L, R + 1)\n",
    "\n",
    "        # assign these bins to this kid (nearest wins if overlaps)\n",
    "        for i in idxs:\n",
    "            if bin2kid[i] == -1 or abs(i - p) < abs(i - peaks[bin2kid[i]]):\n",
    "                bin2kid[i] = kid\n",
    "\n",
    "        buckets.append({\n",
    "            \"kid\": kid,\n",
    "            \"k_refined\": k_refined[kid],\n",
    "            \"bin_indices\": idxs\n",
    "        })\n",
    "\n",
    "    return buckets, bin2kid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052793e-4fd8-401c-a40f-bf548f8a5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_buckets_per_window(row_k_peaks_reduced, bin2kid, n_kids):\n",
    "    \"\"\"\n",
    "    For each window, assign presence of each kid (slope bucket).\n",
    "    From row_k_peaks_reduced, mark which k-modes (kids) appear.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    row_k_peaks_reduced : list of arrays\n",
    "        Each entry = active k-bin indices for that window.\n",
    "    bin2kid : np.ndarray\n",
    "        Map from bin index to kid (or -1).\n",
    "    n_kids : int\n",
    "        Number of buckets.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    presence : ndarray of shape (n_windows, n_kids), bool\n",
    "    \"\"\"\n",
    "    n_windows = len(row_k_peaks_reduced)\n",
    "    presence = np.zeros((n_windows, n_kids), dtype=bool)\n",
    "\n",
    "    for w, active_bins in enumerate(row_k_peaks_reduced):\n",
    "        for b in active_bins:\n",
    "            kid = bin2kid[b]\n",
    "            if kid >= 0:\n",
    "                presence[w, kid] = True\n",
    "    return presence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59757cc4-ca05-4b76-89bc-7db58efdc529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_windows_with_gaps(win_list, max_gap=1):\n",
    "    \"\"\"\n",
    "    Turn a list of window indices into contiguous runs, allowing small gaps.\n",
    "    \n",
    "    Returns a list of (w_start, w_end).\n",
    "    \"\"\"\n",
    "    if not win_list:\n",
    "        return []\n",
    "    runs, cur_start, cur_end = [], win_list[0], win_list[0]\n",
    "    for w in win_list[1:]:\n",
    "        if w <= cur_end + max_gap + 1:\n",
    "            cur_end = w\n",
    "        else:\n",
    "            runs.append((cur_start, cur_end))\n",
    "            cur_start, cur_end = w, w\n",
    "    runs.append((cur_start, cur_end))\n",
    "    return runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4806fc-d8d7-48b2-95dd-1267c9328526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    kid: int\n",
    "    k_refined: float\n",
    "    bin_indices: Optional[List[int]]\n",
    "    w_start: int\n",
    "    w_end: int\n",
    "    t_start_s: float\n",
    "    t_end_s: float\n",
    "    n_windows: int\n",
    "    duration_s: float\n",
    "    coverage_frac: float\n",
    "\n",
    "    # new padded gate fields\n",
    "    gate_w_start: int\n",
    "    gate_w_end: int\n",
    "    t_gate_start_s: float\n",
    "    t_gate_end_s: float\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"Node(kid={self.kid}, k_refined={self.k_refined:.2e}, \"\n",
    "                f\"w_range=({self.gate_w_start}-{self.gate_w_end}), \"\n",
    "                f\"t_range=({self.t_gate_start_s:.6f}-{self.t_gate_end_s:.6f})s, \"\n",
    "                f\"n_windows={self.n_windows}, coverage={self.coverage_frac:.2f})\")\n",
    "        \n",
    "def make_nodes_from_groups(\n",
    "    buckets,\n",
    "    presence,\n",
    "    time_axis_s,\n",
    "    T_chunk_s,\n",
    "    stride_s,              # kept for signature compatibility (not needed if time_axis_s is start times)\n",
    "    pad=1,                 # padding in window hops (±pad windows)\n",
    "    min_windows=1,\n",
    "    max_gap=1              # allow this many missing hops inside a run\n",
    "):\n",
    "    \"\"\"\n",
    "    Build Node objects from bucket presence over time, with optional padded time gates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    buckets : list of dict\n",
    "        Output of build_k_buckets. Each item may have keys like {\"k_refined\", \"bin_indices\"}.\n",
    "    presence : ndarray (n_windows, n_kids)\n",
    "        Boolean/int mask: presence[w, kid] == 1 if kid present in window w.\n",
    "    time_axis_s : array-like (n_windows,)\n",
    "        Start time (global) of each analysis window.\n",
    "    T_chunk_s : float\n",
    "        Window length in seconds.\n",
    "    stride_s : float\n",
    "        Stride between consecutive windows (kept for compatibility; not used if time_axis_s holds starts).\n",
    "    pad : int\n",
    "        Number of windows to extend the gate on both sides (in hops).\n",
    "    min_windows : int\n",
    "        Minimum number of present windows to accept a node.\n",
    "    max_gap : int\n",
    "        Max gap (in hops) allowed when grouping runs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nodes : list[Node]\n",
    "    \"\"\"\n",
    "    nodes = []\n",
    "    n_windows, n_kids = presence.shape\n",
    "\n",
    "    # Precompute window indices where each kid is present\n",
    "    present_lists = [np.flatnonzero(presence[:, kid]) for kid in range(n_kids)]\n",
    "\n",
    "    for kid, bucket in enumerate(buckets):\n",
    "        win_idxs = np.asarray(present_lists[kid], dtype=int)\n",
    "        if win_idxs.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Group into runs allowing small gaps\n",
    "        runs = group_windows_with_gaps(win_idxs.tolist(), max_gap=max_gap)\n",
    "\n",
    "        for (w_start, w_end) in runs:\n",
    "            # windows actually present inside the span\n",
    "            in_run = win_idxs[(win_idxs >= w_start) & (win_idxs <= w_end)]\n",
    "            n_present = int(in_run.size)\n",
    "            span_len  = (w_end - w_start + 1)\n",
    "\n",
    "            if n_present < min_windows:\n",
    "                continue\n",
    "\n",
    "            # ----- padded gate in HOPS (indices) -----\n",
    "            gate_w_start = max(0, w_start - pad)\n",
    "            gate_w_end   = min(n_windows - 1, w_end + pad)\n",
    "\n",
    "            # ----- convert to times -----\n",
    "            t_start_s       = float(time_axis_s[w_start])\n",
    "            t_end_s         = float(time_axis_s[w_end]   + T_chunk_s)\n",
    "            t_gate_start_s  = float(time_axis_s[gate_w_start])\n",
    "            t_gate_end_s    = float(time_axis_s[gate_w_end] + T_chunk_s)\n",
    "\n",
    "            duration  = t_end_s - t_start_s\n",
    "            coverage  = n_present / span_len\n",
    "\n",
    "            # Build Node kwargs (include gate ranges so you can use them for PDW extraction)\n",
    "            node_kwargs = dict(\n",
    "                kid=kid,\n",
    "                k_refined=bucket.get(\"k_refined\", None),\n",
    "                w_start=int(w_start),\n",
    "                w_end=int(w_end),\n",
    "                t_start_s=t_start_s,\n",
    "                t_end_s=t_end_s,\n",
    "                n_windows=n_present,      # actual present windows\n",
    "                duration_s=duration,\n",
    "                coverage_frac=coverage,\n",
    "                gate_w_start=int(gate_w_start),\n",
    "                gate_w_end=int(gate_w_end),\n",
    "                t_gate_start_s=t_gate_start_s,\n",
    "                t_gate_end_s=t_gate_end_s,\n",
    "            )\n",
    "            # Optional: pass bin indices if your Node accepts it\n",
    "            if \"bin_indices\" in bucket:\n",
    "                bi = bucket[\"bin_indices\"]\n",
    "                node_kwargs[\"bin_indices\"] = bi.tolist() if hasattr(bi, \"tolist\") else list(bi)\n",
    "\n",
    "            nodes.append(Node(**node_kwargs))\n",
    "\n",
    "    return nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba94021-ae04-4cdb-a2f4-b3411cc50974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nodes_for_candidates(\n",
    "    candidates, k_vals, row_k_peaks, time_axis_s,\n",
    "    T_chunk, stride, *,\n",
    "    width_bins=1, min_windows=2, max_gap=2, margin=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Build nodes (tracks) from candidate peaks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    candidates : list of dict\n",
    "        Each dict should have 'peak_idx' and 'k_hat'.\n",
    "    k_vals : np.ndarray\n",
    "        Chirp-rate grid.\n",
    "    row_k_peaks : list of lists\n",
    "        Peak indices per window.\n",
    "    time_axis_s : np.ndarray\n",
    "        Window times (global).\n",
    "    T_chunk, stride : float\n",
    "        Chunk length and stride (s).\n",
    "    width_bins : int\n",
    "        Half-width around each peak for bucket building.\n",
    "    min_windows, max_gap, margin : int\n",
    "        Node-building parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nodes : list\n",
    "        Node objects built from these candidates.\n",
    "    buckets : list of dict\n",
    "        Bucket definitions for each candidate.\n",
    "    bin2kid : np.ndarray\n",
    "        Mapping from k-bin index → bucket id.\n",
    "    presence : np.ndarray\n",
    "        Presence matrix [n_windows × n_buckets].\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return [], [], None, None\n",
    "\n",
    "    peaks_idx = [c[\"peak_idx\"] for c in candidates]\n",
    "    k_refined = [c[\"k_hat\"] for c in candidates]\n",
    "\n",
    "    buckets, bin2kid = build_k_buckets(\n",
    "        peaks=np.array(peaks_idx, dtype=int),\n",
    "        k_vals=k_vals,\n",
    "        k_refined=np.array(k_refined, dtype=float),\n",
    "        width_bins=width_bins,\n",
    "    )\n",
    "\n",
    "    presence = assign_buckets_per_window(row_k_peaks, bin2kid, len(buckets))\n",
    "\n",
    "    nodes = make_nodes_from_groups(\n",
    "        buckets, presence,\n",
    "        time_axis_s, T_chunk, stride,\n",
    "        min_windows=min_windows, max_gap=max_gap, pad=margin\n",
    "    )\n",
    "\n",
    "    return nodes, buckets, bin2kid, presence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daedfd82-0b6f-4cbf-b919-c23a63012ea0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Pulse PDW Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae6957d-9d3e-49e5-8962-55adce08808e",
   "metadata": {},
   "source": [
    "### Frequency Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0290cb-886c-4327-9eb7-2c16c1718e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dechirped_fft_for_chunk(chunk, fs, k_Hz_per_s, t0_global_s, pad_factor=4):\n",
    "    \"\"\"\n",
    "    Apply de-chirp with k (using global time origin t0), then FFT and return spectrum for residual tone detection.\n",
    "    De-chirp at k, FFT (zero-padded), return centered (F, mag_db).\n",
    "    \"\"\"\n",
    "    N = len(chunk)\n",
    "    t = t0_global_s + np.arange(N)/fs\n",
    "    y = chunk * np.exp(-1j*np.pi*k_Hz_per_s*(t**2))\n",
    "\n",
    "    L = 1 << int((N*pad_factor - 1).bit_length())\n",
    "    Y = np.fft.fftshift(np.fft.fft(y, n=L))\n",
    "    F = np.fft.fftshift(np.fft.fftfreq(L, d=1.0/fs))\n",
    "    mag_db = 20*np.log10(np.abs(Y) + 1e-30)\n",
    "    return F, mag_db\n",
    "\n",
    "\n",
    "def bandpass_filter_with_fstar(chunk, fs, f_star_hz, lpf_bw_hz, mode=\"iir\"):\n",
    "    \"\"\"\n",
    "    Narrowband isolate one tone by heterodyning at f*, LPF, then mixing back.\n",
    "    Operates in raw FFT coordinates (no dechirp).\n",
    "    \"\"\"\n",
    "    x = np.asarray(chunk, dtype=np.complex128)\n",
    "    if x.size == 0:\n",
    "        return x\n",
    "\n",
    "    N = x.size\n",
    "    t = np.arange(N) / fs\n",
    "\n",
    "    # 1) heterodyne to DC\n",
    "    y_bb = x * np.exp(-1j * 2 * np.pi * f_star_hz * t)\n",
    "\n",
    "    # 2) low-pass\n",
    "    if mode == \"iir\":\n",
    "        wn = min(0.45 * lpf_bw_hz / (0.5 * fs), 0.99)\n",
    "        sos = butter(N=4, Wn=wn, btype=\"low\", output=\"sos\")\n",
    "        guard = int(np.ceil(8 * fs / lpf_bw_hz))          # ~8 cycles\n",
    "        guard = min(guard, len(y_bb)-1)\n",
    "        y_ext = np.r_[y_bb[guard:0:-1], y_bb, y_bb[-2:-guard-2:-1]]\n",
    "        y_lp_ext = sosfiltfilt(sos, y_ext, padlen=0)\n",
    "        y_lp = y_lp_ext[guard:guard + len(y_bb)]\n",
    "    else:\n",
    "        y_lp = y_bb\n",
    "\n",
    "    # 3) mix back up\n",
    "    y_up = y_lp * np.exp(+1j * 2 * np.pi * f_star_hz * t)\n",
    "    return y_up.astype(np.complex64, copy=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056e40f-5c47-4dc9-a5ff-2e56947677db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_residual_tones_moving(signal, fs, *, pad_factor=4, floor_span_hz=1e7, offset_db=15.0,\n",
    "                                 min_df_hz=5e6, top=None):\n",
    "    \"\"\"\n",
    "    Use this FFT peak detection function only for single frequency pulses\n",
    "    Moving-threshold residual tone detection.\n",
    "    Returns array of detected tone frequencies [Hz].\n",
    "    \"\"\"\n",
    "    N = signal.size\n",
    "    L = 1 << int((N * pad_factor - 1).bit_length())\n",
    "    Y = np.fft.fftshift(np.fft.fft(signal, n=L))\n",
    "    F = np.fft.fftshift(np.fft.fftfreq(L, d=1.0/fs))\n",
    "    mag_db = 20*np.log10(np.abs(Y) + 1e-30)\n",
    "\n",
    "    # moving average floor\n",
    "    df = abs(F[1] - F[0])\n",
    "    W = max(3, int(np.ceil(floor_span_hz / df)) | 1)  # force odd\n",
    "    floor_db = uniform_filter1d(mag_db, size=W, mode=\"reflect\")\n",
    "    thr_db_vec = floor_db + float(offset_db)\n",
    "\n",
    "    # candidate peaks, thresholded\n",
    "    min_dist = max(1, int(np.ceil(min_df_hz / df)))\n",
    "    idx_all, _ = find_peaks(mag_db, distance=min_dist)\n",
    "    idx = idx_all[mag_db[idx_all] >= thr_db_vec[idx_all]]\n",
    "\n",
    "    # order by peak height\n",
    "    order = np.argsort(mag_db[idx])[::-1]\n",
    "    if top is not None:\n",
    "        order = order[:top]\n",
    "    idx = idx[order]\n",
    "\n",
    "    return F[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f18699-447e-4fd9-a04c-bafa1656ca0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PDW Extraction (Both Tones and LFMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46dc52e0-b1f3-46ec-816c-11897ce43cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_envelope_params_from_node(node, *, Tmin_us: float, k_tone_thresh=1e9):\n",
    "    \"\"\"\n",
    "    Return (window_us, min_sustain_us) based on node t-range as PW proxy.\n",
    "    For tones (|k| < k_tone_thresh), fall back to fixed Tmin_us.\n",
    "    \"\"\"\n",
    "    # Check if tone-like\n",
    "    if abs(node.k_refined) < k_tone_thresh:   # ~0 slope → treat as tone\n",
    "        return Tmin_us, Tmin_us\n",
    "\n",
    "    # Otherwise estimate PW from t-range\n",
    "    PW_est_us = max(1e-3, (node.t_gate_end_s - node.t_gate_start_s)/2 * 1e6)\n",
    "\n",
    "    window_us = 0.5 * PW_est_us\n",
    "    min_sustain_us = 0.25 * PW_est_us\n",
    "\n",
    "    window_us = max(Tmin_us, window_us)\n",
    "    min_sustain_us = max(Tmin_us, min_sustain_us)\n",
    "    \n",
    "    return window_us, min_sustain_us\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb4880b-1285-4bb1-888f-edd58949fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_and_merge_pdws(df_pdws, Tmin_us=5.0, max_gap_us=5.0):\n",
    "    \"\"\"\n",
    "    Post-process PDWs:\n",
    "      1) Drop pulses shorter than Tmin_us\n",
    "      2) Merge adjacent pulses within each node_id if gap < max_gap_us\n",
    "    Works for both tone and LFM PDWs.\n",
    "    \"\"\"\n",
    "    if df_pdws.empty:\n",
    "        return df_pdws\n",
    "\n",
    "    if \"TOA (us)\" not in df_pdws.columns:\n",
    "        # Nothing to clean/merge if schema is unexpected\n",
    "        return df_pdws\n",
    "\n",
    "    out_rows = []\n",
    "\n",
    "    for nid, g in df_pdws.groupby(\"node_id\", dropna=False):\n",
    "        g = g.sort_values(\"TOA (us)\").reset_index(drop=True)\n",
    "\n",
    "        # Drop too-short\n",
    "        g = g[g[\"PW (us)\"] >= Tmin_us].copy()\n",
    "        if g.empty:\n",
    "            continue\n",
    "\n",
    "        # Merge close pulses\n",
    "        s_idx, e_idx = 0, 0\n",
    "        merged = []\n",
    "        for i in range(1, len(g)):\n",
    "            prev_end = g.iloc[e_idx][\"TOA (us)\"] + g.iloc[e_idx][\"PW (us)\"]\n",
    "            curr_start = g.iloc[i][\"TOA (us)\"]\n",
    "            if curr_start - prev_end <= max_gap_us:\n",
    "                e_idx = i\n",
    "            else:\n",
    "                merged.append((s_idx, e_idx))\n",
    "                s_idx, e_idx = i, i\n",
    "        merged.append((s_idx, e_idx))\n",
    "\n",
    "        for s, e in merged:\n",
    "            rows = g.iloc[s:e+1]\n",
    "\n",
    "            toa_start = rows[\"TOA (us)\"].iloc[0]\n",
    "            toa_end   = rows[\"TOA (us)\"].iloc[-1] + rows[\"PW (us)\"].iloc[-1]\n",
    "            pw_us     = toa_end - toa_start\n",
    "\n",
    "            row_out = {\n",
    "                \"node_id\": nid,\n",
    "                \"TOA (us)\": toa_start,\n",
    "                \"PW (us)\": pw_us,\n",
    "                \"Envelope Amplitude\": rows[\"Envelope Amplitude\"].median(),\n",
    "            }\n",
    "\n",
    "            # Frequency-related columns\n",
    "            if \"Center Freq (MHz)\" in rows:\n",
    "                row_out[\"Center Freq (MHz)\"] = rows[\"Center Freq (MHz)\"].median()\n",
    "            if \"Chirp Rate (MHz/us)\" in rows:\n",
    "                row_out[\"Chirp Rate (MHz/us)\"] = rows[\"Chirp Rate (MHz/us)\"].median()\n",
    "            if \"Start Freq (MHz)\" in rows:\n",
    "                row_out[\"Start Freq (MHz)\"] = rows[\"Start Freq (MHz)\"].min()\n",
    "            if \"End Freq (MHz)\" in rows:\n",
    "                row_out[\"End Freq (MHz)\"] = rows[\"End Freq (MHz)\"].max()\n",
    "            if \"Bandwidth (MHz)\" in rows:\n",
    "                row_out[\"Bandwidth (MHz)\"] = rows[\"Bandwidth (MHz)\"].max()\n",
    "\n",
    "            # Pulse type\n",
    "            if \"Pulse Type\" in rows:\n",
    "                row_out[\"Pulse Type\"] = rows[\"Pulse Type\"].mode().iloc[0]\n",
    "\n",
    "            # Sanity flag\n",
    "            if \"pulse_sanity_ok\" in rows:\n",
    "                row_out[\"pulse_sanity_ok\"] = rows[\"pulse_sanity_ok\"].all()\n",
    "\n",
    "            out_rows.append(row_out)\n",
    "\n",
    "    if not out_rows:\n",
    "        return pd.DataFrame(columns=df_pdws.columns)\n",
    "\n",
    "    return pd.DataFrame(out_rows).sort_values(\"TOA (us)\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357df74e-9ca4-441d-8ed9-e9a5438bce1f",
   "metadata": {},
   "source": [
    "### PDW Extraction (Tones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c82566-ab01-4eaa-b401-0fd890617b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_to_nyquist(f_hz, fs_hz):\n",
    "    \"\"\"Map frequency (Hz) to the signed baseband interval (-fs/2, +fs/2].\"\"\"\n",
    "    return ((f_hz + 0.5*fs_hz) % fs_hz) - 0.5*fs_hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad3435-cb29-4eef-a1d6-28bdf2a5d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdws_for_tone_pulses(\n",
    "    node,\n",
    "    signal, fs,\n",
    "    *,\n",
    "    Tmin_us=5.0,\n",
    "    window_us=5.0,\n",
    "    min_sustain_us=5.0,\n",
    "    pad_factor=2,\n",
    "    floor_span_hz=4e6,\n",
    "    min_df_hz=None,\n",
    "    top=1,\n",
    "    k_min_abs_MHz_per_us=0.10,\n",
    "    downsample_factor=1,\n",
    "    debug=False,\n",
    "    t0_override=None,\n",
    "    node_id=None   # 🔹 NEW: index of node from wrapper\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract PDWs over a single node for tone pulses.\n",
    "    Detection + filtering both happen in *raw FFT coordinates*.\n",
    "    \"\"\"\n",
    "    timings = {}\n",
    "    t0_start = time.perf_counter()\n",
    "\n",
    "    # ---- 1) ROI ----\n",
    "    t0 = float(node.t_gate_start_s) if t0_override is None else float(t0_override)\n",
    "    t1 = float(node.t_gate_end_s)\n",
    "    w_lo, w_hi = node.gate_w_start, node.gate_w_end\n",
    "    roi = signal[int(np.floor(t0*fs)) : int(np.ceil(t1*fs))]\n",
    "    k_refined = float(node.k_refined)   # may be ≈0\n",
    "\n",
    "    timings[\"roi_extract\"] = time.perf_counter() - t0_start\n",
    "\n",
    "    # ---- 2) Residual tone detection (raw FFT) ----\n",
    "    t_fft_start = time.perf_counter()\n",
    "    lpf_bw_hz = 3.0 / (Tmin_us * 1e-6)\n",
    "    if min_df_hz is None:\n",
    "        min_df_hz = 1.5 * lpf_bw_hz\n",
    "\n",
    "    f_peaks = detect_residual_tones_moving(roi, fs,)\n",
    "    timings[\"fft\"] = time.perf_counter() - t_fft_start\n",
    "\n",
    "    if len(f_peaks) == 0:\n",
    "        return pd.DataFrame(), pd.DataFrame(), timings\n",
    "\n",
    "    # ---- 3) Extract pulses ----\n",
    "    pdw_rows, meta_rows = [], []\n",
    "    t_pdws_start = time.perf_counter()\n",
    "\n",
    "    for f_star in f_peaks:\n",
    "        f_star = float(wrap_to_nyquist(float(f_star), fs))\n",
    "        if debug:\n",
    "            print(f\"  -> processing f_star={f_star/1e6:.3f} MHz, ROI_len={len(roi)} samples\")\n",
    "\n",
    "        # isolate tone with raw FFT bandpass filter\n",
    "        sig_iso = bandpass_filter_with_fstar(\n",
    "            roi, fs, f_star,\n",
    "            lpf_bw_hz=lpf_bw_hz, mode=\"iir\"\n",
    "        )\n",
    "\n",
    "        pulses, bounds, mask, cleaned, pdws = baseband_PDW_extractor(\n",
    "            sig_iso, fs,\n",
    "            k_hz_per_s=0.0,              # treat as pure tone\n",
    "            t0_global_s=t0,\n",
    "            window_us=Tmin_us,\n",
    "            min_sustain_us=min_sustain_us,\n",
    "            threshold_percentile=50,\n",
    "            amp_floor_pct=90,\n",
    "        )\n",
    "\n",
    "        for d in pdws:\n",
    "            f_start_MHz  = d[\"f_start_Hz\"]  / 1e6\n",
    "            f_end_MHz    = d[\"f_end_Hz\"]    / 1e6\n",
    "            f_center_MHz = d[\"f_center_Hz\"] / 1e6\n",
    "            k_MHz_per_us = k_refined / 1e12   # still recorded for metadata\n",
    "            pulse_type   = \"Single Frequency Pulse\" if abs(k_MHz_per_us) < k_min_abs_MHz_per_us else \"LFM Pulse\"\n",
    "\n",
    "            pdw_rows.append({\n",
    "                \"node_id\": node_id,                     # 🔹 wrapper index\n",
    "                \"node_kid\": getattr(node, \"kid\", None), # 🔹 original node.kid if available\n",
    "                \"TOA (us)\": d[\"TOA_global_us\"],\n",
    "                \"PW (us)\":  d[\"PW_us\"],\n",
    "                \"Envelope Amplitude\": d[\"Amp_med\"],\n",
    "                \"Center Freq (MHz)\": f_center_MHz,\n",
    "                \"Chirp Rate (MHz/us)\": k_MHz_per_us,\n",
    "                \"Bandwidth (MHz)\": abs(f_end_MHz - f_start_MHz),\n",
    "                \"Start Freq (MHz)\": f_start_MHz,\n",
    "                \"End Freq (MHz)\": f_end_MHz,\n",
    "                \"Pulse Type\": pulse_type,\n",
    "                \"pulse_sanity_ok\": (node.coverage_frac == 1.0)\n",
    "            })\n",
    "\n",
    "            meta_rows.append({\n",
    "                \"node_id\": node_id,\n",
    "                \"node_kid\": getattr(node, \"kid\", None),\n",
    "                \"k_refined_Hz_per_s\": k_refined,\n",
    "                \"gate_w_start\": w_lo,\n",
    "                \"gate_w_end\": w_hi,\n",
    "                \"t_gate_start_s\": t0,\n",
    "                \"t_gate_end_s\": t1,\n",
    "                \"f_star_Hz\": f_star,\n",
    "                \"lpf_bw_hz\": lpf_bw_hz,\n",
    "            })\n",
    "\n",
    "    timings[\"pdw_extract\"] = time.perf_counter() - t_pdws_start\n",
    "    timings[\"total\"] = time.perf_counter() - t0_start\n",
    "\n",
    "    if not pdw_rows:\n",
    "        return None, None, timings\n",
    "\n",
    "    df_pdws = pd.DataFrame(pdw_rows).sort_values(\"TOA (us)\", ignore_index=True)\n",
    "    df_meta = pd.DataFrame(meta_rows).reset_index(drop=True)\n",
    "    return df_pdws, df_meta, timings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195b32f-950e-4372-bb54-033137968106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdws_all_tones_parallel(\n",
    "    nodes, signal, fs, *,\n",
    "    Tmin_us=5.0,\n",
    "    pad_factor=2,\n",
    "    floor_span_hz=4e6,\n",
    "    min_df_hz=None,\n",
    "    top=1,\n",
    "    k_min_abs_MHz_per_us=0.10,\n",
    "    downsample_factor=1,\n",
    "    debug=False,\n",
    "    n_jobs=None,\n",
    "    chunk_size=10,   # 🔹 new\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallel wrapper over all Single Frequency Pulse nodes.\n",
    "    Use node chunking, governed by chunk_size to save time\n",
    "    \"\"\"\n",
    "    if n_jobs is None:\n",
    "        n_jobs = multiprocessing.cpu_count()\n",
    "\n",
    "    def process_chunk(chunk, offset):\n",
    "        results = []\n",
    "        for j, node in enumerate(chunk):\n",
    "            i = offset + j\n",
    "            if debug:\n",
    "                print(f\"[{i+1}/{len(nodes)}] kid={node.kid}, \"\n",
    "                      f\"k={node.k_refined:.2e}, win={node.gate_w_start}-{node.gate_w_end}\")\n",
    "\n",
    "            window_us, min_sustain_us = adaptive_envelope_params_from_node(\n",
    "                node, Tmin_us=Tmin_us\n",
    "            )\n",
    "\n",
    "            df_pdws, df_meta, timings = extract_pdws_for_tone_pulses(\n",
    "                node, signal, fs,\n",
    "                Tmin_us=Tmin_us,\n",
    "                window_us=window_us,\n",
    "                min_sustain_us=min_sustain_us,\n",
    "                pad_factor=pad_factor,\n",
    "                floor_span_hz=floor_span_hz,\n",
    "                min_df_hz=min_df_hz,\n",
    "                top=top,\n",
    "                k_min_abs_MHz_per_us=k_min_abs_MHz_per_us,\n",
    "                downsample_factor=downsample_factor,\n",
    "                debug=debug,\n",
    "                node_id=i,\n",
    "            )\n",
    "            if df_pdws is not None and not df_pdws.empty:\n",
    "                results.append((df_pdws, df_meta, timings))\n",
    "        return results\n",
    "\n",
    "    # --- split nodes into chunks ---\n",
    "    chunks = [nodes[i:i+chunk_size] for i in range(0, len(nodes), chunk_size)]\n",
    "\n",
    "    # --- run in parallel ---\n",
    "    all_results = Parallel(n_jobs=n_jobs, prefer=\"processes\")(\n",
    "        delayed(process_chunk)(chunk, offset)\n",
    "        for offset, chunk in zip(range(0, len(nodes), chunk_size), chunks)\n",
    "    )\n",
    "\n",
    "    # --- collect ---\n",
    "    all_pdws, all_meta, all_timings = [], [], []\n",
    "    for results in all_results:\n",
    "        for df_pdws, df_meta, timings in results:\n",
    "            all_pdws.append(df_pdws)\n",
    "            all_meta.append(df_meta)\n",
    "            all_timings.append(timings)\n",
    "\n",
    "    df_pdws_all = pd.concat(all_pdws, ignore_index=True) if all_pdws else pd.DataFrame()\n",
    "    df_meta_all = pd.concat(all_meta, ignore_index=True) if all_meta else pd.DataFrame()\n",
    "    return df_pdws_all, df_meta_all, all_timings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea651a73-179c-4d45-a28d-8225c6134db2",
   "metadata": {},
   "source": [
    "### PDW Extraction (LFM Pulses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151d393-f9a1-46b8-8629-b02ae0f3c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdws_for_lfm_pulses(\n",
    "    node,\n",
    "    signal, fs,\n",
    "    *,\n",
    "    Tmin_us=5.0,\n",
    "    window_us=5.0,\n",
    "    min_sustain_us=5.0,\n",
    "    pad_factor=2,\n",
    "    floor_span_hz=4e6,\n",
    "    min_df_hz=None,\n",
    "    top=1,\n",
    "    downsample_factor=1,\n",
    "    debug=False,\n",
    "    t0_override=None,\n",
    "    node_id=None,\n",
    "    plot=False, plot_raw=False, plot_smoothed=False, plot_mask=False, plot_thres=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract PDWs over a single node for LFM pulses.\n",
    "    Uses dechirp → residual tone isolation → re-chirp → baseband envelope extraction.\n",
    "    Operates on the *raw, unnotched* signal (not signal_after_cw).\n",
    "    \"\"\"\n",
    "\n",
    "    timings = {}\n",
    "    t0_start = time.perf_counter()\n",
    "\n",
    "    # ---- 1) ROI ----\n",
    "    t0 = float(node.t_gate_start_s) if t0_override is None else float(t0_override)\n",
    "    t1 = float(node.t_gate_end_s)\n",
    "    w_lo, w_hi = node.gate_w_start, node.gate_w_end\n",
    "    roi = signal[int(np.floor(t0*fs)) : int(np.ceil(t1*fs))]\n",
    "    k_refined = float(node.k_refined)   # Hz/s (nonzero for LFM)\n",
    "\n",
    "    timings[\"roi_extract\"] = time.perf_counter() - t0_start\n",
    "    if debug:\n",
    "        print(f\"[LFM] node {node.kid}, ROI_len={len(roi)}, k_refined={k_refined:.3e}\")\n",
    "\n",
    "    # ---- 2) Dechirp FFT to find residual tone ----\n",
    "    t_fft_start = time.perf_counter()\n",
    "    F, mag_db = dechirped_fft_for_chunk(roi, fs, k_refined, t0_global_s=t0, pad_factor=pad_factor)\n",
    "\n",
    "    # crude residual peak finder\n",
    "    peak_idx = np.argmax(mag_db)\n",
    "    f_res = float(F[peak_idx])  # Hz residual tone freq\n",
    "    timings[\"fft\"] = time.perf_counter() - t_fft_start\n",
    "\n",
    "    # ---- 3) Dechirp in time domain, isolate residual, re-chirp ----\n",
    "    N = len(roi)\n",
    "    t = t0 + np.arange(N)/fs\n",
    "    roi_dec = roi * np.exp(-1j*np.pi*k_refined*(t**2))\n",
    "\n",
    "    lpf_bw_hz = 3.0 / (Tmin_us * 1e-6)\n",
    "    if min_df_hz is None:\n",
    "        min_df_hz = 1.5 * lpf_bw_hz\n",
    "\n",
    "    y_dc = bandpass_filter_with_fstar(\n",
    "        roi_dec, fs, f_star_hz=f_res, lpf_bw_hz=lpf_bw_hz, mode=\"iir\"\n",
    "    )\n",
    "    sig_iso_lfm = y_dc * np.exp(+1j*np.pi*k_refined*(t**2))\n",
    "\n",
    "    # ---- 4) Envelope + PDWs ----\n",
    "    t_pdws_start = time.perf_counter()\n",
    "    pulses, bounds, mask, cleaned, pdws = baseband_PDW_extractor(\n",
    "        sig_iso_lfm, fs,\n",
    "        k_hz_per_s=k_refined,\n",
    "        t0_global_s=t0,\n",
    "        window_us=window_us,\n",
    "        min_sustain_us=min_sustain_us,\n",
    "        threshold_percentile=50,\n",
    "        amp_floor_pct=90,\n",
    "        downsample_factor=downsample_factor,\n",
    "        plot=plot, plot_raw=plot_raw, plot_smoothed=plot_smoothed,\n",
    "        plot_mask=plot_mask, plot_thres=plot_thres\n",
    "    )\n",
    "    timings[\"pdw_extract\"] = time.perf_counter() - t_pdws_start\n",
    "    timings[\"total\"] = time.perf_counter() - t0_start\n",
    "\n",
    "    # ---- 5) Assemble PDW rows ----\n",
    "    pdw_rows, meta_rows = [], []\n",
    "    for d in pdws:\n",
    "        f_start_MHz  = d[\"f_start_Hz\"]  / 1e6\n",
    "        f_end_MHz    = d[\"f_end_Hz\"]    / 1e6\n",
    "        f_center_MHz = d[\"f_center_Hz\"] / 1e6\n",
    "        k_MHz_per_us = k_refined / 1e12\n",
    "\n",
    "        pdw_rows.append({\n",
    "            \"TOA (us)\": d[\"TOA_global_us\"],\n",
    "            \"PW (us)\":  d[\"PW_us\"],\n",
    "            \"Envelope Amplitude\": d[\"Amp_med\"],\n",
    "            \"Center Freq (MHz)\": f_center_MHz,\n",
    "            \"Chirp Rate (MHz/us)\": k_MHz_per_us,\n",
    "            \"Bandwidth (MHz)\": abs(f_end_MHz - f_start_MHz),\n",
    "            \"Start Freq (MHz)\": f_start_MHz,\n",
    "            \"End Freq (MHz)\": f_end_MHz,\n",
    "            \"Pulse Type\": \"LFM Pulse\",\n",
    "            \"pulse_sanity_ok\": (node.coverage_frac == 1.0)\n",
    "        })\n",
    "\n",
    "        meta_rows.append({\n",
    "            \"node_id\": node.kid,\n",
    "            \"k_refined_Hz_per_s\": k_refined,\n",
    "            \"gate_w_start\": w_lo,\n",
    "            \"gate_w_end\": w_hi,\n",
    "            \"t_gate_start_s\": t0,\n",
    "            \"t_gate_end_s\": t1,\n",
    "            \"f_res_Hz\": f_res,\n",
    "            \"lpf_bw_hz\": lpf_bw_hz,\n",
    "        })\n",
    "\n",
    "    if not pdw_rows:\n",
    "        return None, None, timings\n",
    "    \n",
    "    df_pdws = pd.DataFrame(pdw_rows).sort_values(\"TOA (us)\", ignore_index=True)\n",
    "    df_meta = pd.DataFrame(meta_rows).reset_index(drop=True)\n",
    "    \n",
    "    # 🔹 Use passed-in node_id\n",
    "    df_pdws[\"node_id\"] = node_id\n",
    "    df_meta[\"node_id\"] = node_id\n",
    "    return df_pdws, df_meta, timings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dfd34a-1986-42ba-a73f-528877187ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9e397-cefc-4137-93a8-d2d3effec03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdws_all_lfms_parallel(\n",
    "    nodes_lfm,\n",
    "    signal, fs,\n",
    "    *,\n",
    "    Tmin_us=5.0,\n",
    "    window_us=5.0,\n",
    "    min_sustain_us=5.0,\n",
    "    pad_factor=2,\n",
    "    floor_span_hz=4e6,\n",
    "    min_df_hz=None,\n",
    "    top=None,\n",
    "    downsample_factor=1,\n",
    "    debug=False,\n",
    "    n_jobs=-1,\n",
    "    chunk_size=10,   # 🔹 NEW\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallel wrapper over all LFM nodes, with chunking for efficiency.\n",
    "    Calls extract_pdws_for_lfm_pulses(node, signal, fs, ..., node_id=idx).\n",
    "    \"\"\"\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    def process_chunk(chunk, offset):\n",
    "        results = []\n",
    "        for j, node in enumerate(chunk):\n",
    "            idx = offset + j\n",
    "            df_pdws, df_meta, timings = extract_pdws_for_lfm_pulses(\n",
    "                node,\n",
    "                signal, fs,\n",
    "                Tmin_us=Tmin_us,\n",
    "                window_us=window_us,\n",
    "                min_sustain_us=min_sustain_us,\n",
    "                pad_factor=pad_factor,\n",
    "                floor_span_hz=floor_span_hz,\n",
    "                min_df_hz=min_df_hz,\n",
    "                downsample_factor=downsample_factor,\n",
    "                debug=debug,\n",
    "                node_id=idx,\n",
    "            )\n",
    "            if df_pdws is not None and not df_pdws.empty:\n",
    "                results.append((df_pdws, df_meta, timings))\n",
    "        return results\n",
    "\n",
    "    # --- split into chunks ---\n",
    "    chunks = [nodes_lfm[i:i+chunk_size] for i in range(0, len(nodes_lfm), chunk_size)]\n",
    "\n",
    "    # --- run chunks in parallel ---\n",
    "    all_results = Parallel(n_jobs=n_jobs, prefer=\"processes\")(\n",
    "        delayed(process_chunk)(chunk, offset)\n",
    "        for offset, chunk in zip(range(0, len(nodes_lfm), chunk_size), chunks)\n",
    "    )\n",
    "\n",
    "    # --- collect results ---\n",
    "    pdws_all, meta_all, timings_all = [], [], {}\n",
    "    for results in all_results:\n",
    "        for df_pdws, df_meta, timings in results:\n",
    "            pdws_all.append(df_pdws)\n",
    "            meta_all.append(df_meta)\n",
    "            for k, v in timings.items():\n",
    "                timings_all[k] = timings_all.get(k, 0.0) + v\n",
    "\n",
    "    if not pdws_all:\n",
    "        return pd.DataFrame(), pd.DataFrame(), timings_all\n",
    "\n",
    "    df_pdws = (\n",
    "        pd.concat(pdws_all, ignore_index=True)\n",
    "          .sort_values(\"TOA (us)\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    df_meta = pd.concat(meta_all, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    df_pdws = _clean_and_merge_pdws(df_pdws, Tmin_us=Tmin_us, max_gap_us=Tmin_us)\n",
    "\n",
    "    timings_all[\"total\"] = time.perf_counter() - t0\n",
    "    return df_pdws, df_meta, timings_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6273270f-8d97-4fab-b7b1-a49c09f5afc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a47a34a8-ebb7-483e-ab1c-6a1d422f6166",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Cluster into Emitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd888f2c-a8bd-49f0-ad1e-5aaf89ca9bcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_robust_pri_from_toas_us\u001b[39m(toa_us: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    Robustly estimate PRI from TOAs (us).\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    Returns PRI in microseconds (us), or None if insufficient data.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     toa_us \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(toa_us, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def _pri_consistency_from_toas_us(\n",
    "    toa_us: np.ndarray, tol_frac: float = 0.05, min_pulses: int = 20\n",
    ") -> tuple[bool, Optional[float], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Evaluate PRI consistency from TOAs (µs).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (is_consistent, pri_est, frac_ok)\n",
    "      is_consistent : bool, True if cluster shows stable PRI\n",
    "      pri_est       : median PRI (µs) if available\n",
    "      frac_ok       : fraction of diffs within tol_frac of pri_est\n",
    "    \"\"\"\n",
    "    toa_us = np.asarray(toa_us, dtype=float)\n",
    "    if toa_us.size < min_pulses:\n",
    "        return False, None, None\n",
    "\n",
    "    toa_sorted = np.sort(toa_us)\n",
    "    diffs = np.diff(toa_sorted)\n",
    "    diffs = diffs[diffs > 0]\n",
    "    if diffs.size == 0:\n",
    "        return False, None, None\n",
    "\n",
    "    pri_est = np.median(diffs)\n",
    "    frac_ok = np.mean(np.abs(diffs - pri_est) < tol_frac * pri_est)\n",
    "    return frac_ok > 0.8, float(pri_est), float(frac_ok)\n",
    "\n",
    "\n",
    "def cluster_emitters_dbscan(\n",
    "    df_pdws_all: pd.DataFrame,\n",
    "    *,\n",
    "    min_samples: int = 5,\n",
    "    eps: float = 0.8,\n",
    "    use_types: bool = True,\n",
    "    require_sanity: bool = True,\n",
    "    require_pri_consistency: bool = True,\n",
    "    pri_tol_frac: float = 0.05,\n",
    "    pri_min_pulses: int = 20,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cluster pulses (rows) into emitters via DBSCAN and return a per-emitter summary.\n",
    "\n",
    "    Adds PRI-consistency check to suppress spurious clusters with unstable TOAs,\n",
    "    AND computes per-emitter pulse-train start/end times in seconds for IQ\n",
    "    reconstruction / CAF.\n",
    "\n",
    "    Expected columns in df_pdws_all:\n",
    "      \"TOA (us)\", \"PW (us)\", \"Envelope Amplitude\",\n",
    "      \"Center Freq (MHz)\", \"Chirp Rate (MHz/us)\",\n",
    "      \"Bandwidth (MHz)\", \"Start Freq (MHz)\", \"End Freq (MHz)\",\n",
    "      \"Pulse Type\"\n",
    "    \"\"\"\n",
    "    df = df_pdws_all.copy()\n",
    "\n",
    "    if require_sanity and \"pulse_sanity_ok\" in df.columns:\n",
    "        df = df[df[\"pulse_sanity_ok\"] == True].copy()\n",
    "        if df.empty:\n",
    "            raise ValueError(\"No rows with pulse_sanity_ok == True after filtering.\")\n",
    "\n",
    "    needed = [\n",
    "        \"TOA (us)\",\"PW (us)\",\"Envelope Amplitude\",\"Center Freq (MHz)\",\n",
    "        \"Chirp Rate (MHz/us)\",\"Bandwidth (MHz)\",\"Start Freq (MHz)\",\n",
    "        \"End Freq (MHz)\",\"Pulse Type\"\n",
    "    ]\n",
    "    miss = [c for c in needed if c not in df.columns]\n",
    "    if miss:\n",
    "        raise KeyError(f\"Missing columns: {miss}\")\n",
    "\n",
    "    X_cols = [\"Chirp Rate (MHz/us)\", \"Center Freq (MHz)\", \"PW (us)\"]\n",
    "    X = df[X_cols].to_numpy(dtype=float)\n",
    "\n",
    "    if use_types and \"Pulse Type\" in df.columns:\n",
    "        pt_flag = (df[\"Pulse Type\"].str.contains(\"LFM\", case=False)).astype(float).to_numpy()[:, None]\n",
    "        X = np.hstack([X, pt_flag])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Z = scaler.fit_transform(X)\n",
    "    labels = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1).fit_predict(Z)\n",
    "\n",
    "    df = df.assign(cluster_id=labels)\n",
    "    df = df[df[\"cluster_id\"] >= 0].copy()\n",
    "    if df.empty:\n",
    "        raise ValueError(\"DBSCAN produced no clusters (all noise). Consider relaxing eps or min_samples.\")\n",
    "\n",
    "    rows = []\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    for cid, g in df.groupby(\"cluster_id\"):\n",
    "        # PRI + consistency\n",
    "        is_ok, pri_est, frac_ok = _pri_consistency_from_toas_us(\n",
    "            g[\"TOA (us)\"].to_numpy(),\n",
    "            tol_frac=pri_tol_frac,\n",
    "            min_pulses=pri_min_pulses,\n",
    "        )\n",
    "        if require_pri_consistency and not is_ok:\n",
    "            continue  # drop spurious cluster\n",
    "\n",
    "        medians = g[numeric_cols].median(numeric_only=True).to_dict()\n",
    "        medians.pop(\"TOA (us)\", None)\n",
    "\n",
    "        cat_vals = {}\n",
    "        for c in df.columns.difference(numeric_cols + [\"cluster_id\"]):\n",
    "            mode_val = g[c].mode(dropna=True)\n",
    "            cat_vals[c] = (mode_val.iloc[0] if not mode_val.empty else g[c].iloc[0])\n",
    "\n",
    "        # ------------------------------\n",
    "        # Pulse train time bounds\n",
    "        # ------------------------------\n",
    "        toa_us = g[\"TOA (us)\"].to_numpy()\n",
    "        pw_us  = g[\"PW (us)\"].to_numpy()\n",
    "        t_train_start_s = float(np.min(toa_us)) * 1e-6\n",
    "        t_train_end_s   = float(np.max(toa_us + pw_us)) * 1e-6\n",
    "        n_pulses        = int(len(g))\n",
    "\n",
    "        out = {\n",
    "            \"PRI (us)\": pri_est,\n",
    "            \"PW (us)\": medians.get(\"PW (us)\"),\n",
    "            \"Envelope Amplitude\": medians.get(\"Envelope Amplitude\"),\n",
    "            \"Center Freq (MHz)\": medians.get(\"Center Freq (MHz)\"),\n",
    "            \"Chirp Rate (MHz/us)\": medians.get(\"Chirp Rate (MHz/us)\"),\n",
    "            \"Bandwidth (MHz)\": medians.get(\"Bandwidth (MHz)\"),\n",
    "            \"Start Freq (MHz)\": medians.get(\"Start Freq (MHz)\"),\n",
    "            \"End Freq (MHz)\": medians.get(\"End Freq (MHz)\"),\n",
    "            \"Pulse Type\": cat_vals.get(\"Pulse Type\"),\n",
    "            \"cluster_id\": cid,\n",
    "            \"count\": len(g),\n",
    "            \"PRI_frac_ok\": frac_ok,   # useful diagnostic\n",
    "            # NEW: train timing for CAF / IQ reconstruction\n",
    "            \"t_train_start_s\": t_train_start_s,\n",
    "            \"t_train_end_s\": t_train_end_s,\n",
    "        }\n",
    "        rows.append(out)\n",
    "\n",
    "    cols_out = [\n",
    "        \"PRI (us)\", \"PW (us)\", \"Envelope Amplitude\", \"Center Freq (MHz)\",\n",
    "        \"Chirp Rate (MHz/us)\", \"Bandwidth (MHz)\", \"Start Freq (MHz)\",\n",
    "        \"End Freq (MHz)\", \"Pulse Type\", \"cluster_id\", \"count\",\n",
    "        \"PRI_frac_ok\", \"t_train_start_s\", \"t_train_end_s\",\n",
    "    ]\n",
    "    df_emitters = pd.DataFrame(rows)[cols_out].sort_values(\n",
    "        [\"Pulse Type\",\"Center Freq (MHz)\",\"Chirp Rate (MHz/us)\",\"PRI (us)\"]\n",
    "    )\n",
    "    return df_emitters, df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b1c95-4dae-4160-82f0-c9f611dd310a",
   "metadata": {},
   "source": [
    "## 7. Per Emitter IQ Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68909fd6-6d2f-4d0a-b804-73625ccaafe4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### For Pulse Trains (LFMs and Single Frequency Types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e53d59e1-f49b-47e3-a837-caf4da826db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_emitter_mask_from_pdws(df_pdws_lfm_tagged, cluster_id, fs, N, guard_us=2.0):\n",
    "    \"\"\"\n",
    "    Boolean mask over samples where the chosen LFM emitter is 'on'.\n",
    "    Optional guard expands each pulse a bit.\n",
    "    \"\"\"\n",
    "    mask = np.zeros(N, dtype=bool)\n",
    "    g = df_pdws_lfm_tagged[df_pdws_lfm_tagged[\"cluster_id\"] == cluster_id]\n",
    "\n",
    "    for _, row in g.iterrows():\n",
    "        t0_us = float(row[\"TOA (us)\"])\n",
    "        pw_us = float(row[\"PW (us)\"])\n",
    "\n",
    "        t_start_us = t0_us - guard_us\n",
    "        t_end_us   = t0_us + pw_us + guard_us\n",
    "\n",
    "        s = int(np.floor(t_start_us * 1e-6 * fs))\n",
    "        e = int(np.ceil (t_end_us   * 1e-6 * fs))\n",
    "\n",
    "        s = max(0, s)\n",
    "        e = min(N, e)\n",
    "        if e > s:\n",
    "            mask[s:e] = True\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6035598-6da8-427d-8525-503f9ffc80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_lfm_in_roi(roi, fs, k_refined, t0_global_s, Tmin_us=5.0, pad_factor=2):\n",
    "    \"\"\"\n",
    "    Given a ROI that contains your LFM of interest plus other stuff,\n",
    "    dechirp w/ k_refined, LPF the residual tone, rechirp back.\n",
    "    Returns the isolated complex LFM in the ROI.\n",
    "    \"\"\"\n",
    "    # 1) residual tone via dechirped FFT\n",
    "    F, mag_db = dechirped_fft_for_chunk(\n",
    "        roi, fs, k_refined, t0_global_s=t0_global_s, pad_factor=pad_factor\n",
    "    )\n",
    "    f_res = float(F[np.argmax(mag_db)])\n",
    "\n",
    "    # 2) dechirp in time\n",
    "    N = len(roi)\n",
    "    t = t0_global_s + np.arange(N)/fs\n",
    "    roi_dec = roi * np.exp(-1j*np.pi*k_refined*(t**2))\n",
    "\n",
    "    # 3) LPF around the residual tone\n",
    "    lpf_bw_hz = 3.0 / (Tmin_us * 1e-6)\n",
    "    y_dc = bandpass_filter_with_fstar(\n",
    "        roi_dec, fs, f_star_hz=f_res, lpf_bw_hz=lpf_bw_hz, mode=\"iir\"\n",
    "    )\n",
    "\n",
    "    # 4) rechirp back\n",
    "    sig_iso_lfm = y_dc * np.exp(+1j*np.pi*k_refined*(t**2))\n",
    "    return sig_iso_lfm\n",
    "\n",
    "def extract_emitter_iq_lfm(signal, fs, df_pdws_lfm_tagged, cluster_id, k_refined, Tmin_us=5.0):\n",
    "    \"\"\"\n",
    "    High-level: \n",
    "      - time-gate using PDWs,\n",
    "      - for each contiguous gated block, dechirp -> LPF -> rechirp,\n",
    "      - stitch blocks back into a full-length isolated IQ.\n",
    "    \"\"\"\n",
    "    N = len(signal)\n",
    "    mask = make_emitter_mask_from_pdws(df_pdws_lfm_tagged, cluster_id, fs, N)\n",
    "    y_iso = np.zeros(N, dtype=np.complex64)\n",
    "\n",
    "    # Find contiguous segments where mask is True\n",
    "    on_idx = np.nonzero(mask)[0]\n",
    "    if on_idx.size == 0:\n",
    "        return y_iso\n",
    "\n",
    "    # simple segmentation by breaks > 1 sample\n",
    "    splits = np.where(np.diff(on_idx) > 1)[0] + 1\n",
    "    segments = np.split(on_idx, splits)\n",
    "\n",
    "    for seg in segments:\n",
    "        s = seg[0]\n",
    "        e = seg[-1] + 1\n",
    "        t0 = s / fs\n",
    "        roi = signal[s:e]\n",
    "\n",
    "        sig_roi = isolate_lfm_in_roi(roi, fs, k_refined, t0_global_s=t0, Tmin_us=Tmin_us)\n",
    "        y_iso[s:e] += sig_roi\n",
    "\n",
    "    return y_iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e821d-c457-477d-bac6-d5c938133b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_decimate_bw(\n",
    "    iq_raw,\n",
    "    df_pdws_lfm,\n",
    "    cluster_id,\n",
    "    fs,\n",
    "    f_center,\n",
    "    k_refined,\n",
    "    t_start,\n",
    "    PRI,\n",
    "    PW,\n",
    "    bw_coverage_hz,     # *** NEW: desired retained bandwidth ***\n",
    "    guard_us=200.0,\n",
    "    fir_len=101\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract LFM emitter from FF1, then baseband, lowpass filter, and decimate\n",
    "    using an explicit retained bandwidth (bw_coverage_hz).\n",
    "\n",
    "    This function replaces lp_cutoff_frac entirely.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iq_raw : ndarray\n",
    "        Raw IQ at original sampling fs.\n",
    "    df_pdws_lfm : DataFrame\n",
    "        PDWs for this LFM emitter.\n",
    "    cluster_id : int\n",
    "        Emitter ID.\n",
    "    fs : float\n",
    "        Original sampling rate [Hz].\n",
    "    f_center : float\n",
    "        LO frequency [Hz].\n",
    "    k_refined : float\n",
    "        Chirp slope [Hz/s].\n",
    "    t_start : float\n",
    "        First pulse start time.\n",
    "    PRI : float\n",
    "        PRI for this emitter.\n",
    "    PW : float\n",
    "        Pulse width.\n",
    "    bw_coverage_hz : float\n",
    "        Actual bandwidth (Hz) to retain in baseband after filtering.\n",
    "        Must >= B_eff + 2*fD_max.\n",
    "    guard_us : float\n",
    "        Time guard window around pulses [us].\n",
    "    fir_len : int\n",
    "        FIR filter length.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_ds : ndarray\n",
    "        Decimated LFM baseband (FF1 only).\n",
    "    fs_ds : float\n",
    "        Decimated sample rate.\n",
    "    idx0, idx1 : int\n",
    "        Slice indices for reuse by FF2/FF3.\n",
    "    fir_coeffs : ndarray\n",
    "        FIR kernel.\n",
    "    decim_factor : int\n",
    "        Final decimation factor used.\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.signal import firwin\n",
    "\n",
    "    N_raw = len(iq_raw)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1) Build emitter mask (FF1 reference)\n",
    "    # -----------------------------------------------------------\n",
    "    mask = make_emitter_mask_from_pdws(\n",
    "        df_pdws_lfm,\n",
    "        cluster_id=cluster_id,\n",
    "        fs=fs,\n",
    "        N=N_raw,\n",
    "        guard_us=guard_us\n",
    "    )\n",
    "\n",
    "    y_gate = np.where(mask, iq_raw, 0.0)\n",
    "\n",
    "    # Optional reconstruction\n",
    "    y_iso = extract_emitter_iq_lfm(y_gate, fs, df_pdws_lfm, cluster_id, k_refined)\n",
    "    y_ref = y_iso.astype(np.complex128)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2) Determine clean slice indices\n",
    "    # -----------------------------------------------------------\n",
    "    on_idxs = np.where(mask)[0]\n",
    "    if on_idxs.size == 0:\n",
    "        raise RuntimeError(\"Empty mask for emitter {}\".format(cluster_id))\n",
    "\n",
    "    extra_guard = int(np.round(0.2 * PW * fs))\n",
    "    idx0 = max(0, on_idxs[0] - extra_guard)\n",
    "    idx1 = min(N_raw, on_idxs[-1] + extra_guard)\n",
    "\n",
    "    # Time base (absolute time!)\n",
    "    n_slice = np.arange(idx0, idx1)\n",
    "    t_slice = n_slice / fs\n",
    "    y_slice = y_ref[idx0:idx1]\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 3) Downconvert with common LO (abs time ensures phase alignment)\n",
    "    # -----------------------------------------------------------\n",
    "    mix = np.exp(-1j * 2*np.pi * f_center * t_slice)\n",
    "    y_bb = y_slice * mix\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4) Compute maximum allowed decimation factor\n",
    "    #    Post-decim Nyquist must exceed bw_coverage_hz\n",
    "    # -----------------------------------------------------------\n",
    "    decim_factor = int(fs // (2 * bw_coverage_hz))\n",
    "    decim_factor = max(decim_factor, 1)\n",
    "\n",
    "    fs_ds = fs / decim_factor\n",
    "    nyquist_ds = fs_ds / 2\n",
    "\n",
    "    if bw_coverage_hz >= nyquist_ds:\n",
    "        raise ValueError(\n",
    "            f\"bw_coverage_hz = {bw_coverage_hz} exceeds post-decim Nyquist = {nyquist_ds}. \"\n",
    "            \"Reduce bw_coverage_hz or decim_factor.\"\n",
    "        )\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 5) LPF: cutoff = bw_coverage_hz (direct Hz!)\n",
    "    # -----------------------------------------------------------\n",
    "    fir_coeffs = firwin(\n",
    "        fir_len,\n",
    "        bw_coverage_hz,     # direct cutoff\n",
    "        fs=fs,\n",
    "    )\n",
    "\n",
    "    y_filt = np.convolve(y_bb, fir_coeffs, mode=\"same\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 6) Decimate\n",
    "    # -----------------------------------------------------------\n",
    "    y_ds = y_filt[::decim_factor]\n",
    "\n",
    "    return y_ds, fs_ds, idx0, idx1, fir_coeffs, decim_factor\n",
    "\n",
    "\n",
    "def filter_and_decimate_same_slice(\n",
    "    iq_raw,\n",
    "    fs,\n",
    "    f_center,\n",
    "    idx0,\n",
    "    idx1,\n",
    "    fir_coeffs,\n",
    "    decim_factor,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply the *same* slice, LO mixing, LPF and decimation to another channel\n",
    "    (e.g. FF2 or FF3) as was used for FF1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iq_raw : ndarray (complex)\n",
    "        Raw IQ for this satellite at full-rate fs.\n",
    "    fs : float\n",
    "        Original sampling rate [Hz].\n",
    "    f_center : float\n",
    "        Same LO as used for FF1 [Hz].\n",
    "    idx0, idx1 : int\n",
    "        Slice indices (at full-rate fs) returned by extract_and_decimate_correct.\n",
    "    fir_coeffs : ndarray\n",
    "        FIR coefficients returned by extract_and_decimate_correct.\n",
    "    decim_factor : int\n",
    "        Same decimation factor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_ds : ndarray (complex)\n",
    "        Decimated, basebanded signal for this satellite.\n",
    "    \"\"\"\n",
    "\n",
    "    N_raw = len(iq_raw)\n",
    "    idx0 = max(0, idx0)\n",
    "    idx1 = min(N_raw, idx1)\n",
    "\n",
    "    y_slice = iq_raw[idx0:idx1]\n",
    "\n",
    "    # Absolute time axis for this slice (same as FF1)\n",
    "    n_slice = np.arange(idx0, idx1)\n",
    "    t_slice = n_slice / fs\n",
    "\n",
    "    # Downconvert with the same LO and ABS time\n",
    "    mix = np.exp(-1j * 2.0 * np.pi * f_center * t_slice)\n",
    "    y_bb = y_slice * mix\n",
    "\n",
    "    # Filter and decimate exactly like FF1\n",
    "    y_filt = np.convolve(y_bb, fir_coeffs, mode=\"same\")\n",
    "    y_ds = y_filt[::decim_factor]\n",
    "\n",
    "    return y_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e7d0c-44a8-4f91-a840-6ba24a6284fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### For CWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56d0e8a8-9724-49ae-82f6-e2d9bd4a6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cw_band(iq, fs, f_center_bb, bw_coverage_hz=100e3, fir_len=101):\n",
    "    \"\"\"\n",
    "    Clean CW extraction WITHOUT pulse-train masking.\n",
    "    Mimics extract_and_decimate_bw logic for continuous-wave emitters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iq : ndarray\n",
    "        Composite IQ (complex64) at sampling rate fs.\n",
    "    fs : float\n",
    "        Sampling rate of input IQ.\n",
    "    f_center_tx : float\n",
    "        Transmitter RF center frequency (Hz).\n",
    "    bw_coverage_hz : float\n",
    "        Processing bandwidth to retain around the CW.\n",
    "    fir_len : int\n",
    "        Length of FIR LPF.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds : ndarray\n",
    "        Decimated, filtered CW IQ.\n",
    "    fs_ds : float\n",
    "        New sampling rate after decimation.\n",
    "    h : ndarray\n",
    "        FIR coefficients.\n",
    "    decim_factor : int\n",
    "        Decimation factor applied (must also be applied to FF2/FF3).\n",
    "    \"\"\"\n",
    "    N = len(iq)\n",
    "    t = np.arange(N) / fs\n",
    "\n",
    "    # 1. Mix to baseband (remove the CW tone)\n",
    "    mixed = iq * np.exp(-1j * 2*np.pi * f_center_bb * t)\n",
    "\n",
    "    # 2. Low-pass filter\n",
    "    from scipy.signal import firwin, lfilter\n",
    "    h = firwin(fir_len, bw_coverage_hz/(fs/2))\n",
    "    filt = lfilter(h, 1.0, mixed)\n",
    "\n",
    "    # 3. Auto decimation factor\n",
    "    decim_factor = int(fs // (2 * bw_coverage_hz))\n",
    "    decim_factor = max(decim_factor, 1)\n",
    "    fs_ds = fs / decim_factor\n",
    "\n",
    "    ds = filt[::decim_factor]\n",
    "\n",
    "    return ds, fs_ds, h, decim_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5fbd66-e3c9-456e-8272-7cd674bd8bb3",
   "metadata": {},
   "source": [
    "### For LFMCWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9159a9-dabf-4f20-8c84-ca0681159723",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For simplicity, only extracting first LFMCW sweep for CAF and geolocation later. \n",
    "Can theoretically be extending to multiple sweeps for better performance\n",
    "\n",
    "Idea: Align and extract first sweeps across all receiver channels.\n",
    "\n",
    "Alignment: \n",
    "- FF1 (mainlobe) relies on deinterleaver output.\n",
    "- FF2 and 3 (sidelobes) relies on estimated emitter location, which will affect TDOA and FDOA offsets\n",
    "- Take multiple starting positions evenly divided in AOI. Run CAFs for all starting positions and compare\n",
    "  CAF's Peak to Sidelobe ratio. Proper alignment should yield higher PSR.\n",
    "\n",
    "Extraction: Same concept as LFM case, using time information to time gate, followed by dechirp, rechirp filtering.\n",
    "\n",
    "Estimated emitter location: Will affect alignment significantly. \n",
    "- Idea is to use AOI seed locations as emitter location proxies. Use CAF's PSR as scoring criteria to get appropriate \n",
    "  starting location proxy\n",
    "- Using starting proxy, geolocate LFMCW emitter location, then feed in this location iteratively for a better \n",
    "  CAF/geolocation output\n",
    "'''\n",
    "def _aoi_bounds_from_emitters(EMITTER_LLA, margin_frac=0.20):\n",
    "    lats = np.array([e[\"lat\"] for e in EMITTER_LLA], float)\n",
    "    lons = np.array([e[\"lon\"] for e in EMITTER_LLA], float)\n",
    "    lat_min0, lat_max0 = float(lats.min()), float(lats.max())\n",
    "    lon_min0, lon_max0 = float(lons.min()), float(lons.max())\n",
    "    dlat, dlon = lat_max0 - lat_min0, lon_max0 - lon_min0\n",
    "    if dlat == 0: dlat = 0.1\n",
    "    if dlon == 0: dlon = 0.1\n",
    "    lat_min = lat_min0 - margin_frac * dlat\n",
    "    lat_max = lat_max0 + margin_frac * dlat\n",
    "    lon_min = lon_min0 - margin_frac * dlon\n",
    "    lon_max = lon_max0 + margin_frac * dlon\n",
    "    return lat_min, lat_max, lon_min, lon_max\n",
    "\n",
    "def _caf_psr(CAF, guard=3):\n",
    "    CAF = np.asarray(CAF)\n",
    "    idx = np.argmax(CAF)\n",
    "    i_fd, i_tau = np.unravel_index(idx, CAF.shape)\n",
    "    peak = float(CAF[i_fd, i_tau])\n",
    "\n",
    "    mask = np.ones_like(CAF, dtype=bool)\n",
    "    fd0 = max(0, i_fd-guard); fd1 = min(CAF.shape[0], i_fd+guard+1)\n",
    "    t0  = max(0, i_tau-guard); t1  = min(CAF.shape[1], i_tau+guard+1)\n",
    "    mask[fd0:fd1, t0:t1] = False\n",
    "\n",
    "    sidelobe = float(np.max(CAF[mask])) if np.any(mask) else np.nan\n",
    "    psr = peak / sidelobe if (np.isfinite(sidelobe) and sidelobe > 0) else np.nan\n",
    "    return peak, sidelobe, psr\n",
    "\n",
    "\n",
    "def build_common_lfmcw_roi_geom(\n",
    "    t0_ff1_s, PW_s,\n",
    "    tau21, tau31,\n",
    "    fs, N_raw,\n",
    "    tau_guard_s\n",
    "):\n",
    "    t1_start = t0_ff1_s\n",
    "    t1_end   = t0_ff1_s + PW_s\n",
    "\n",
    "    t2_start = t0_ff1_s + tau21\n",
    "    t2_end   = t2_start + PW_s\n",
    "\n",
    "    t3_start = t0_ff1_s + tau31\n",
    "    t3_end   = t3_start + PW_s\n",
    "\n",
    "    t_roi_start = min(t1_start, t2_start, t3_start) - tau_guard_s\n",
    "    t_roi_end   = max(t1_end, t2_end, t3_end) + tau_guard_s\n",
    "\n",
    "    idx_start = int(np.floor(t_roi_start * fs))\n",
    "    idx_end   = int(np.ceil(t_roi_end * fs))\n",
    "\n",
    "    idx_start = max(0, idx_start)\n",
    "    idx_end   = min(N_raw, idx_end)\n",
    "\n",
    "    return idx_start, idx_end, idx_start / fs\n",
    "\n",
    "\n",
    "def process_lfmcw_channel(\n",
    "    iq, fs,\n",
    "    idx_start, idx_end,\n",
    "    t_roi_start_s,\n",
    "    t0_ff1_s,\n",
    "    PW_s,\n",
    "    tau_est,\n",
    "    k_hz_s,\n",
    "    f_center,\n",
    "    B_proc,\n",
    "    fir_coeffs,\n",
    "    decim_factor\n",
    "):\n",
    "    roi = iq[idx_start:idx_end]\n",
    "    N_roi = len(roi)\n",
    "    t_roi = t_roi_start_s + np.arange(N_roi) / fs\n",
    "\n",
    "    # Time-gate around sweep for this sat\n",
    "    t_sweep_start = t0_ff1_s + tau_est\n",
    "    t_sweep_end   = t_sweep_start + PW_s\n",
    "    gate = (t_roi >= t_sweep_start) & (t_roi < t_sweep_end)\n",
    "    roi = roi * gate.astype(roi.dtype)\n",
    "\n",
    "    # Isolation operator\n",
    "    sweep = isolate_lfm_in_roi(\n",
    "        roi=roi,\n",
    "        fs=fs,\n",
    "        k_refined=k_hz_s,\n",
    "        t0_global_s=t_roi_start_s,   # ✅ correct: absolute time of roi[0]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Downconvert\n",
    "    t_local = np.arange(len(sweep)) / fs\n",
    "    mix = np.exp(-1j * 2*np.pi * f_center * t_local)\n",
    "    sweep_bb = sweep * mix\n",
    "\n",
    "    # LPF\n",
    "    sweep_lp = lfilter(fir_coeffs, [1.0], sweep_bb)\n",
    "\n",
    "    # Decimate\n",
    "    sweep_ds = sweep_lp[::decim_factor]\n",
    "    return sweep_ds\n",
    "\n",
    "\n",
    "def extract_lfmcw_common_window_geom(\n",
    "    iq_FF1, iq_FF2, iq_FF3,\n",
    "    fs,\n",
    "    lfmcw_row,\n",
    "    tau21_est, tau31_est,\n",
    "    tau_guard_s\n",
    "):\n",
    "    t0_ff1_s = lfmcw_row[\"TOA (us)\"] * 1e-6\n",
    "    PW_s     = lfmcw_row[\"PW (us)\"] * 1e-6\n",
    "    f_center = lfmcw_row[\"Center Freq (MHz)\"] * 1e6\n",
    "    k_hz_s   = lfmcw_row[\"Chirp Rate (MHz/us)\"] * 1e12\n",
    "    B_proc   = abs(lfmcw_row[\"Bandwidth (MHz)\"]) * 1e6\n",
    "\n",
    "    N_raw = len(iq_FF1)\n",
    "\n",
    "    # Common ROI\n",
    "    idx0, idx1, t_roi_start_s = build_common_lfmcw_roi_geom(\n",
    "        t0_ff1_s, PW_s,\n",
    "        tau21_est, tau31_est,\n",
    "        fs, N_raw,\n",
    "        tau_guard_s,\n",
    "    )\n",
    "\n",
    "    # FIR + decimation same for all channels\n",
    "    # LPF bandwidth slightly above B_proc\n",
    "    fir_coeffs = firwin(101, 0.55 * B_proc / (fs/2))\n",
    "    target_fs = 2.5 * B_proc\n",
    "    decim_factor = max(1, int(fs // target_fs))\n",
    "\n",
    "    # FF1 ref sweep\n",
    "    x1_ds = process_lfmcw_channel(\n",
    "        iq_FF1, fs,\n",
    "        idx0, idx1,\n",
    "        t_roi_start_s,\n",
    "        t0_ff1_s, PW_s,\n",
    "        tau_est=0.0,\n",
    "        k_hz_s=k_hz_s,\n",
    "        f_center=f_center,\n",
    "        B_proc=B_proc,\n",
    "        fir_coeffs=fir_coeffs,\n",
    "        decim_factor=decim_factor\n",
    "    )\n",
    "\n",
    "    # FF2\n",
    "    x2_ds = process_lfmcw_channel(\n",
    "        iq_FF2, fs,\n",
    "        idx0, idx1,\n",
    "        t_roi_start_s,\n",
    "        t0_ff1_s, PW_s,\n",
    "        tau_est=tau21_est,\n",
    "        k_hz_s=k_hz_s,\n",
    "        f_center=f_center,\n",
    "        B_proc=B_proc,\n",
    "        fir_coeffs=fir_coeffs,\n",
    "        decim_factor=decim_factor\n",
    "    )\n",
    "\n",
    "    # FF3\n",
    "    x3_ds = process_lfmcw_channel(\n",
    "        iq_FF3, fs,\n",
    "        idx0, idx1,\n",
    "        t_roi_start_s,\n",
    "        t0_ff1_s, PW_s,\n",
    "        tau_est=tau31_est,\n",
    "        k_hz_s=k_hz_s,\n",
    "        f_center=f_center,\n",
    "        B_proc=B_proc,\n",
    "        fir_coeffs=fir_coeffs,\n",
    "        decim_factor=decim_factor\n",
    "    )\n",
    "\n",
    "    fs_ds = fs / decim_factor\n",
    "\n",
    "    return x1_ds, x2_ds, x3_ds, fs_ds, {\n",
    "        \"idx_start\": idx0,\n",
    "        \"idx_end\": idx1,\n",
    "        \"fs_ds\": fs_ds,\n",
    "        \"PW_s\": PW_s,\n",
    "        \"f_center\": f_center,\n",
    "        \"k_hz_s\": k_hz_s,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ebf38-ea05-45d8-a53f-c6d05af38390",
   "metadata": {},
   "source": [
    "# CAF Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f202308b-ea19-455c-b16b-b4a18a871341",
   "metadata": {},
   "source": [
    "## 8. CAF Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efe3a5-1107-4a4c-95c8-1fbd2031c45b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Applying Error Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7e16b5-0d55-4f42-a301-2a17cdd4cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Add receiver sync errors (per satellite)\n",
    "# ---------------------------------------------------------\n",
    "def apply_sync_errors(dTau_21_true, dTau_31_true, error_cfg, seed=1234):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sync_err = {\n",
    "        \"FF1\": rng.normal(0, error_cfg[\"sync_err_rms\"]),\n",
    "        \"FF2\": rng.normal(0, error_cfg[\"sync_err_rms\"]),\n",
    "        \"FF3\": rng.normal(0, error_cfg[\"sync_err_rms\"]),\n",
    "    }\n",
    "\n",
    "    dTau_21_err = dTau_21_true + (sync_err[\"FF2\"] - sync_err[\"FF1\"])\n",
    "    dTau_31_err = dTau_31_true + (sync_err[\"FF3\"] - sync_err[\"FF1\"])\n",
    "\n",
    "    return dTau_21_err, dTau_31_err, sync_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839754d3-bf2a-40d6-98df-420af752856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_position_errors(r_sats, error_cfg, seed=1234):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return {\n",
    "        sat: r_sats[sat] + rng.normal(0, error_cfg[\"pos_err_rms\"]/np.sqrt(3), 3)\n",
    "        for sat in r_sats\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdda7ba-e745-4d1f-9812-328225678265",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Compute CAF Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5db722-a0ad-4e3e-b8c4-f11cc05723eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) Compute TDOA & FDOA for ONE emitter\n",
    "# ============================================================\n",
    "def compute_tau_fd_one_emitter(em, r_sats, v_sats, f_c):\n",
    "    \"\"\"\n",
    "    Compute (tau21, tau31, fD21, fD31) for a single emitter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    em : dict\n",
    "        {\"lat\", \"lon\", \"alt\"}\n",
    "    r_sats : dict\n",
    "        {'FF1','FF2','FF3'} → sat ECEF positions\n",
    "    v_sats : dict\n",
    "        {'FF1','FF2','FF3'} → sat ECEF velocities\n",
    "    f_c : float\n",
    "        Carrier frequency Hz\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert emitter to ECEF\n",
    "    e_ecef = lla_to_ecef(em[\"lat\"], em[\"lon\"], em[\"alt\"])\n",
    "\n",
    "    # Reference FF1 always\n",
    "    r1, v1 = r_sats[\"FF1\"], v_sats[\"FF1\"]\n",
    "    r2, v2 = r_sats[\"FF2\"], v_sats[\"FF2\"]\n",
    "    r3, v3 = r_sats[\"FF3\"], v_sats[\"FF3\"]\n",
    "\n",
    "    # Range distances\n",
    "    R1 = np.linalg.norm(e_ecef - r1)\n",
    "    R2 = np.linalg.norm(e_ecef - r2)\n",
    "    R3 = np.linalg.norm(e_ecef - r3)\n",
    "\n",
    "    # TDOA\n",
    "    tau21 = (R2 - R1) / c\n",
    "    tau31 = (R3 - R1) / c\n",
    "\n",
    "    # LOS unit vectors\n",
    "    u1 = (e_ecef - r1) / R1\n",
    "    u2 = (e_ecef - r2) / R2\n",
    "    u3 = (e_ecef - r3) / R3\n",
    "\n",
    "    # Radial velocities\n",
    "    vr1 = np.dot(v1, u1)\n",
    "    vr2 = np.dot(v2, u2)\n",
    "    vr3 = np.dot(v3, u3)\n",
    "\n",
    "    # Dopplers per satellite\n",
    "    fD1 = (f_c / c) * vr1\n",
    "    fD2 = (f_c / c) * vr2\n",
    "    fD3 = (f_c / c) * vr3\n",
    "\n",
    "    # FDOA (relative to FF1)\n",
    "    fD21 = fD2 - fD1\n",
    "    fD31 = fD3 - fD1\n",
    "\n",
    "    return tau21, tau31, fD21, fD31\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Build CAF grids (blind) using the full emitter region\n",
    "# ============================================================\n",
    "def build_caf_grids(r_sats, v_sats, emitter_lla_list, f_c,\n",
    "                    fs_caf, df_step=100.0, margin_frac=0.20):\n",
    "\n",
    "    tau21_list, tau31_list = [], []\n",
    "    fD21_list,  fD31_list  = [], []\n",
    "\n",
    "    # Compute τ and fD for each emitter in coverage region\n",
    "    for em in emitter_lla_list:\n",
    "        tau21, tau31, fD21, fD31 = compute_tau_fd_one_emitter(\n",
    "            em, r_sats, v_sats, f_c\n",
    "        )\n",
    "        tau21_list.append(tau21)\n",
    "        tau31_list.append(tau31)\n",
    "        fD21_list.append(fD21)\n",
    "        fD31_list.append(fD31)\n",
    "\n",
    "    tau21_arr = np.array(tau21_list)\n",
    "    tau31_arr = np.array(tau31_list)\n",
    "    fD21_arr  = np.array(fD21_list)\n",
    "    fD31_arr  = np.array(fD31_list)\n",
    "\n",
    "    # Add margin around the physical range\n",
    "    def bounds(arr):\n",
    "        lo, hi = arr.min(), arr.max()\n",
    "        span = max(abs(lo), abs(hi))\n",
    "        margin = margin_frac * span\n",
    "        return lo - margin, hi + margin\n",
    "\n",
    "    tau21_min, tau21_max = bounds(tau21_arr)\n",
    "    tau31_min, tau31_max = bounds(tau31_arr)\n",
    "    fD21_min,  fD21_max  = bounds(fD21_arr)\n",
    "    fD31_min,  fD31_max  = bounds(fD31_arr)\n",
    "\n",
    "    # Build discrete CAF grids\n",
    "    tau_vals_21 = np.arange(tau21_min, tau21_max + 1/fs_caf, 1/fs_caf)\n",
    "    tau_vals_31 = np.arange(tau31_min, tau31_max + 1/fs_caf, 1/fs_caf)\n",
    "\n",
    "    fd_vals_21  = np.arange(fD21_min, fD21_max + df_step, df_step)\n",
    "    fd_vals_31  = np.arange(fD31_min, fD31_max + df_step, df_step)\n",
    "\n",
    "    return (tau_vals_21, fd_vals_21,\n",
    "            tau_vals_31, fd_vals_31,\n",
    "            (tau21_min, tau21_max,\n",
    "             tau31_min, tau31_max,\n",
    "             fD21_min, fD21_max,\n",
    "             fD31_min, fD31_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a1228-e7bc-4e89-b4c0-edf80532f097",
   "metadata": {},
   "source": [
    "### CAF (Pulse Trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae9b757-f9b1-42d7-9ae3-6f97fe303268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy version: 13.6.0\n",
      "CUDA runtime: 12090\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "print(\"CuPy version:\", cp.__version__)\n",
    "print(\"CUDA runtime:\", cp.cuda.runtime.runtimeGetVersion())\n",
    "print(\"Device count:\", cp.cuda.runtime.getDeviceCount())\n",
    "dev = cp.cuda.Device()\n",
    "print(\"Current device:\", dev.id)\n",
    "print(\"Device name:\", cp.cuda.runtime.getDeviceProperties(dev.id)[\"name\"].decode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5b246-c3fa-4461-9ddc-0a33d90c7aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, cupy as cp\n",
    "\n",
    "x = cp.random.randn(4096, 4096, dtype=cp.float32)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "y = x @ x\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "print(\"matmul seconds:\", time.perf_counter()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c09c56-7a1f-4a71-b681-a6281487e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 1) Build Pulse Windows\n",
    "# ================================================================\n",
    "\n",
    "def make_pulse_windows(x_ref, y_meas, t, centers, half_width_s, fs):\n",
    "    dt = 1.0 / fs\n",
    "    N = len(t)\n",
    "    N_win = int(np.round(2 * half_width_s * fs))\n",
    "\n",
    "    windows = []\n",
    "\n",
    "    for tc in centers:\n",
    "        idx_c = int(np.round(tc * fs))\n",
    "        start = max(0, idx_c - N_win//2)\n",
    "        end   = min(N, start + N_win)\n",
    "\n",
    "        x_seg = x_ref[start:end]\n",
    "        y_seg = y_meas[start:end]\n",
    "        t_seg = t[start:end]\n",
    "\n",
    "        # Pad if needed\n",
    "        if len(x_seg) < N_win:\n",
    "            pad = N_win - len(x_seg)\n",
    "            x_seg = np.pad(x_seg, (0, pad))\n",
    "            y_seg = np.pad(y_seg, (0, pad))\n",
    "            t_last = t_seg[-1]\n",
    "            t_seg  = np.concatenate([t_seg,\n",
    "                                     t_last + dt*np.arange(1, pad+1)])\n",
    "\n",
    "        windows.append((x_seg, y_seg, t_seg))\n",
    "\n",
    "    return windows, N_win\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) Segmented FFT CAF (multi-window)\n",
    "# ================================================================\n",
    "def caf_fft_segmented_multi_gpu(\n",
    "    x_ref, y_meas, t, centers,\n",
    "    tau_vals, fd_vals, fs,\n",
    "    half_width_s,\n",
    "    fd_batch_size=8,     # safe on 8–24GB GPUs\n",
    "    win_batch_size=32    # safe batch for windows (M)\n",
    "):\n",
    "    \"\"\"\n",
    "    GPU version of segmented FFT-domain CAF, with double batching:\n",
    "    - batch over Doppler (fd)\n",
    "    - batch over windows (pulse windows)\n",
    "\n",
    "    This prevents OOM while still gaining GPU parallelism.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_ref, y_meas : 1D np.ndarray (complex)\n",
    "        Reference (FF1) and other channel (FF2/FF3) in the time slice.\n",
    "    t : 1D np.ndarray (float)\n",
    "        Absolute time axis for x_ref/y_meas.\n",
    "    centers : 1D np.ndarray (float)\n",
    "        Pulse centres (seconds) from PDWs.\n",
    "    tau_vals : 1D np.ndarray (float)\n",
    "        TDOA search grid (seconds).\n",
    "    fd_vals : 1D np.ndarray (float)\n",
    "        FDOA search grid (Hz).\n",
    "    fs : float\n",
    "        Sample rate of this CAF slice (Hz).\n",
    "    half_width_s : float\n",
    "        Half-window length for each pulse (seconds).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CAF_tot : 2D np.ndarray (float), shape = (N_fd, N_tau)\n",
    "        Normalised CAF surface accumulated over all pulses.\n",
    "    \"\"\"\n",
    "\n",
    "    # ================================================================\n",
    "    # 2.1) Build ALL windows on CPU\n",
    "    # ================================================================\n",
    "    windows, N_win = make_pulse_windows(\n",
    "        x_ref, y_meas, t, centers, half_width_s, fs\n",
    "    )\n",
    "    M = len(windows)\n",
    "\n",
    "    # Stack windows (CPU)\n",
    "    x_stack = np.stack([w[0] for w in windows], axis=0)  # (M, N_win)\n",
    "    y_stack = np.stack([w[1] for w in windows], axis=0)\n",
    "    t_stack = np.stack([w[2] for w in windows], axis=0)\n",
    "\n",
    "    # ================================================================\n",
    "    # 2.2) Precompute tau indices (CPU)\n",
    "    # ================================================================\n",
    "    tau_axis_win = (np.arange(N_win) - N_win//2) / fs\n",
    "    tau_idx = np.array([\n",
    "        np.argmin(np.abs(tau_axis_win - tau))\n",
    "        for tau in tau_vals\n",
    "    ], dtype=np.int32)\n",
    "    tau_idx_gpu = cp.asarray(tau_idx)\n",
    "\n",
    "    # ================================================================\n",
    "    # 2.3) Prepare output CAF array (GPU)\n",
    "    # ================================================================\n",
    "    N_fd  = len(fd_vals)\n",
    "    N_tau = len(tau_vals)\n",
    "    CAF_tot_gpu = cp.zeros((N_fd, N_tau), dtype=cp.float32)\n",
    "\n",
    "    # Doppler values as GPU array\n",
    "    fd_gpu_all = cp.asarray(fd_vals, dtype=cp.float64)\n",
    "\n",
    "    # ================================================================\n",
    "    # 2.4) Two-level batching\n",
    "    # ================================================================\n",
    "    # batch 1: windows\n",
    "    for w_start in range(0, M, win_batch_size):\n",
    "        w_end = min(M, w_start + win_batch_size)\n",
    "        x_win_gpu = cp.asarray(x_stack[w_start:w_end])   # (Wb, N_win)\n",
    "        y_win_gpu = cp.asarray(y_stack[w_start:w_end])   # (Wb, N_win)\n",
    "        t_win_gpu = cp.asarray(t_stack[w_start:w_end])   # (Wb, N_win)\n",
    "\n",
    "        # FFT reference windows once per window batch\n",
    "        X_gpu = cp.fft.fft(x_win_gpu, axis=1)            # (Wb, N_win)\n",
    "        X_gpu = X_gpu[None, :, :]                        # (1, Wb, N_win)\n",
    "\n",
    "        # batch 2: Doppler bins\n",
    "        for fd_start in range(0, N_fd, fd_batch_size):\n",
    "            fd_end = min(N_fd, fd_start + fd_batch_size)\n",
    "\n",
    "            fd_sub = fd_gpu_all[fd_start:fd_end]         # (Fd_b,)\n",
    "            Fd_gpu = fd_sub[:, None, None]               # (Fd_b, 1, 1)\n",
    "\n",
    "            # Build Doppler mixing matrix\n",
    "            E_gpu = cp.exp(-1j * 2 * cp.pi * Fd_gpu * t_win_gpu[None, :, :])\n",
    "            # (Fd_b, Wb, N_win)\n",
    "\n",
    "            # Mix y\n",
    "            Ymix_gpu = E_gpu * y_win_gpu[None, :, :]     # (Fd_b, Wb, N_win)\n",
    "\n",
    "            # FFT for all fd × windows\n",
    "            Y_gpu = cp.fft.fft(Ymix_gpu, axis=2)         # (Fd_b, Wb, N_win)\n",
    "\n",
    "            # Cross correlation\n",
    "            R_gpu = cp.fft.ifft(Y_gpu * cp.conj(X_gpu), axis=2)\n",
    "            R_gpu = cp.fft.fftshift(R_gpu, axes=2)\n",
    "\n",
    "            R_abs_gpu = cp.abs(R_gpu)                    # (Fd_b, Wb, N_win)\n",
    "\n",
    "            # Gather tau bins and sum windows\n",
    "            CAF_sub = R_abs_gpu[:, :, tau_idx_gpu]       # (Fd_b, Wb, N_tau)\n",
    "            CAF_sub = cp.sum(CAF_sub, axis=1)            # (Fd_b, N_tau)\n",
    "\n",
    "            # Accumulate\n",
    "            CAF_tot_gpu[fd_start:fd_end, :] += CAF_sub.astype(cp.float32)\n",
    "\n",
    "    # Normalize and return to CPU\n",
    "    CAF_tot_gpu /= cp.max(CAF_tot_gpu) + 1e-12\n",
    "    return cp.asnumpy(CAF_tot_gpu)\n",
    "\n",
    "# ================================================================\n",
    "# 3) Utility to measure peak\n",
    "# ================================================================\n",
    "\n",
    "def peak_from_caf(CAF, fd_vals, tau_vals):\n",
    "    i_fd, i_tau = np.unravel_index(np.argmax(CAF), CAF.shape)\n",
    "    return fd_vals[i_fd], tau_vals[i_tau]\n",
    "\n",
    "# ================================================================\n",
    "# 4) Run segmented CAF for a pair\n",
    "# ================================================================\n",
    "def run_segmented_pair_blind_gpu(\n",
    "    label,\n",
    "    y1, yx,\n",
    "    fs, Tp,\n",
    "    centers,\n",
    "    tau_vals,\n",
    "    fd_vals,\n",
    "    fd_batch_size=8,\n",
    "    win_batch_size=32,\n",
    "    seed=1234\n",
    "):\n",
    "   \n",
    "    \"\"\"\n",
    "    Blind segmented CAF for a satellite pair (X→1) using GPU.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y1 : np.ndarray (complex)\n",
    "        Reference channel (FF1) in time slice.\n",
    "    yx : np.ndarray (complex)\n",
    "        Other channel (FF2 or FF3) in time slice.\n",
    "    fs : float\n",
    "        Sample rate used for this CAF (Hz).\n",
    "    Tp : float\n",
    "        Pulse width (s).\n",
    "    centers : np.ndarray\n",
    "        Pulse centres (s).\n",
    "    tau_vals : np.ndarray\n",
    "        TDOA grid (s).\n",
    "    fd_vals : np.ndarray\n",
    "        FDOA grid (Hz).\n",
    "    Returns\n",
    "    -------\n",
    "    fd_hat, tau_hat : float\n",
    "        Estimated FDOA and TDOA from CAF peak.\n",
    "    \"\"\"\n",
    "    N = len(y1)\n",
    "    t = np.arange(N) / fs\n",
    "\n",
    "    tau_span = max(abs(tau_vals.min()), abs(tau_vals.max()))\n",
    "    half_width_s = Tp/2 + tau_span + 3e-6\n",
    "\n",
    "    CAF = caf_fft_segmented_multi_gpu(\n",
    "        y1, yx, t,\n",
    "        centers,\n",
    "        tau_vals, fd_vals,\n",
    "        fs,\n",
    "        half_width_s,\n",
    "        fd_batch_size,\n",
    "        win_batch_size,\n",
    "    )\n",
    "    fd_hat, tau_hat = peak_from_caf(CAF, fd_vals, tau_vals)\n",
    "\n",
    "    # print(f\"\\n========== Segmented CAF (GPU, blind): {label} ==========\")\n",
    "    # print(f\"Est. tau = {tau_hat*1e9:.3f} ns\")\n",
    "    # print(f\"Est. fD  = {fd_hat:.3f} Hz\")\n",
    "    # print(f\"Runtime  = {t1 - t0:.3f} s\")\n",
    "    \n",
    "    return fd_hat, tau_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0df85-71ee-48e1-925f-9a44dcde1abf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CAF (CWs) - 1D Frequency Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b594cff2-94b2-4554-a557-8a9aa35d17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tone_frequency_gpu(x, fs, pad_factor=4):\n",
    "    \"\"\"\n",
    "    Estimate the frequency (Hz) of a single complex tone in x[n],\n",
    "    using GPU (CuPy) if available, with zero-padding and parabolic interpolation.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Move to GPU if available ---\n",
    "    x_gpu = cp.asarray(x)\n",
    "    N = len(x)\n",
    "    N_fft = int(2**np.ceil(np.log2(N * pad_factor)))\n",
    "\n",
    "    # Apply window to reduce leakage\n",
    "    win_gpu = cp.hanning(N)\n",
    "    xw = x_gpu * win_gpu\n",
    "\n",
    "    # FFT\n",
    "    X = cp.fft.fft(xw, n=N_fft)\n",
    "    mag = cp.abs(X)\n",
    "\n",
    "    # Frequency axis\n",
    "    freqs = cp.fft.fftfreq(N_fft, d=1/fs)\n",
    "\n",
    "    # Move back to CPU for interpolation\n",
    "    mag_cpu = cp.asnumpy(mag)\n",
    "    freqs_cpu = cp.asnumpy(freqs)\n",
    "\n",
    "    # Find index of max bin\n",
    "    idx = int(np.argmax(mag_cpu))\n",
    "\n",
    "    # --- Parabolic Interpolation ---\n",
    "    if 0 < idx < len(mag_cpu)-1:\n",
    "        y1, y2, y3 = mag_cpu[idx-1], mag_cpu[idx], mag_cpu[idx+1]\n",
    "        denom = (y1 - 2*y2 + y3)\n",
    "        if denom != 0:\n",
    "            delta = 0.5 * (y1 - y3) / denom\n",
    "        else:\n",
    "            delta = 0.0\n",
    "    else:\n",
    "        delta = 0.0\n",
    "\n",
    "    df = freqs_cpu[1] - freqs_cpu[0]\n",
    "    f_hat = freqs_cpu[idx] + delta * df\n",
    "    return f_hat, N_fft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b06a028f-2ae7-452d-847a-61ad7c9a7c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_fdoa_from_tones(x1, x2, x3, fs, pad_factor=4, error_cfg=None, seed=None):\n",
    "    \"\"\"\n",
    "    FDOA estimation for CW case:\n",
    "      fD21 = f2 - f1\n",
    "      fD31 = f3 - f1\n",
    "\n",
    "    Adds CW-only jitter (frequency-domain):\n",
    "        - estimator jitter (fd_meas_err_rms)\n",
    "        - LO/CFO jitter (lo_err_rms_hz)\n",
    "    Uses a reproducible RNG seeded per call.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int or None\n",
    "        If provided, ensures reproducibility.  Noise samples remain independent.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Tone extraction (exact) ---\n",
    "    f1, N_fft = estimate_tone_frequency_gpu(x1, fs, pad_factor)\n",
    "    f2, _     = estimate_tone_frequency_gpu(x2, fs, pad_factor)\n",
    "    f3, _     = estimate_tone_frequency_gpu(x3, fs, pad_factor)\n",
    "\n",
    "    fD21 = f2 - f1\n",
    "    fD31 = f3 - f1\n",
    "\n",
    "    # --- FFT spacing ---\n",
    "    df_step = fs / N_fft\n",
    "\n",
    "    # --- Jitter injection (CW) ---\n",
    "    if error_cfg is not None:\n",
    "        fd_meas_err = error_cfg.get(\"fd_meas_err_rms\", 0.0)\n",
    "        lo_err      = error_cfg.get(\"lo_err_rms_hz\", 0.0)\n",
    "\n",
    "        # Seeded RNG for reproducibility\n",
    "        # Using key idea: RNG seeded per call, but every draw is independent.\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Estimator jitter (independent for each pair)\n",
    "        if fd_meas_err > 0:\n",
    "            fD21 += rng.normal(0, fd_meas_err)\n",
    "            fD31 += rng.normal(0, fd_meas_err)\n",
    "\n",
    "        # LO jitter (independent for each pair)\n",
    "        # difference between LO2–LO1 gives sqrt(2) * σ_lo\n",
    "        if lo_err > 0:\n",
    "            lo_jitter_21 = rng.normal(0, np.sqrt(2.0) * lo_err)\n",
    "            lo_jitter_31 = rng.normal(0, np.sqrt(2.0) * lo_err)\n",
    "\n",
    "            fD21 += lo_jitter_21\n",
    "            fD31 += lo_jitter_31\n",
    "\n",
    "    return fD21, fD31, df_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0689ffdc-ddbd-4225-ab81-aa47309db500",
   "metadata": {},
   "source": [
    "### Single Sweep CAF (LFMCWs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f6222-0cdf-49da-864e-8ac38b52ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For simplicity, only doing CAF for first LFMCW sweep. \n",
    "Can theoretically be extending to multiple sweeps for better performance\n",
    "\n",
    "Idea: Align and extract first sweeps across all receiver channels then CAF.\n",
    "\n",
    "Alignment: \n",
    "- FF1 (mainlobe) relies on deinterleaver output.\n",
    "- FF2 and 3 (sidelobes) relies on estimated emitter location, which will affect TDOA and FDOA offsets\n",
    "\n",
    "Extraction: Same concept as LFM case, using time information to time gate, followed by dechirp, rechirp filtering.\n",
    "\n",
    "Estimated emitter location: Will affect alignment significantly. \n",
    "- Idea is to use AOI center location as emitter location proxy\n",
    "- Geolocate LFMCW emitter location, then feed in this location iteratively for a better CAF/geolocation output\n",
    "'''\n",
    "\n",
    "def caf_fft_gpu(x, y, tau_vals, fd_vals, fs, batch_size=128):\n",
    "    N = len(x)\n",
    "    x_gpu = cp.asarray(x)\n",
    "    y_gpu = cp.asarray(y)\n",
    "    t_gpu = cp.arange(N, dtype=cp.float64) / fs\n",
    "\n",
    "    X_gpu = cp.fft.fft(x_gpu)\n",
    "    tau_idx_gpu = cp.asarray((np.round(tau_vals * fs).astype(np.int64)) % N)\n",
    "\n",
    "    CAF_gpu = cp.zeros((len(fd_vals), len(tau_vals)), dtype=cp.float32)\n",
    "    fd_vals_gpu = cp.asarray(fd_vals, dtype=cp.float64)\n",
    "\n",
    "    for i0 in range(0, len(fd_vals), batch_size):\n",
    "        i1 = min(i0 + batch_size, len(fd_vals))\n",
    "        fb = fd_vals_gpu[i0:i1]\n",
    "\n",
    "        phase = cp.exp(-1j * 2*np.pi * fb[:,None] * t_gpu[None,:])\n",
    "        y_mix = y_gpu[None,:] * phase\n",
    "        Y_mix = cp.fft.fft(y_mix, axis=1)\n",
    "\n",
    "        R = cp.fft.ifft(Y_mix * cp.conj(X_gpu)[None,:], axis=1)\n",
    "        CAF_gpu[i0:i1,:] = cp.abs(R)[:, tau_idx_gpu]\n",
    "\n",
    "    CAF_gpu /= (cp.max(CAF_gpu) + 1e-12)\n",
    "    return cp.asnumpy(CAF_gpu)\n",
    "\n",
    "def geom_tdoa_fdoa_from_lla(\n",
    "    emitter_lla,\n",
    "    r_sats,\n",
    "    v_sats,\n",
    "    f_center_hz\n",
    "):\n",
    "    \"\"\"\n",
    "    r_sats = {'FF1': np.array([x,y,z]), 'FF2': ..., 'FF3': ...}\n",
    "    v_sats = {'FF1': np.array([vx,vy,vz]), ...}\n",
    "    \"\"\"\n",
    "\n",
    "    r_em = lla_to_ecef(\n",
    "        emitter_lla[\"lat\"],\n",
    "        emitter_lla[\"lon\"],\n",
    "        emitter_lla.get(\"alt\", 0.0),\n",
    "    )\n",
    "\n",
    "    r1 = r_sats['FF1'];  r2 = r_sats['FF2'];  r3 = r_sats['FF3']\n",
    "    v1 = v_sats['FF1'];  v2 = v_sats['FF2'];  v3 = v_sats['FF3']\n",
    "\n",
    "    # Ranges\n",
    "    d1 = np.linalg.norm(r_em - r1)\n",
    "    d2 = np.linalg.norm(r_em - r2)\n",
    "    d3 = np.linalg.norm(r_em - r3)\n",
    "\n",
    "    # TDOA\n",
    "    tau21 = (d2 - d1) / c\n",
    "    tau31 = (d3 - d1) / c\n",
    "\n",
    "    # LOS unit vectors\n",
    "    u1 = (r_em - r1) / d1\n",
    "    u2 = (r_em - r2) / d2\n",
    "    u3 = (r_em - r3) / d3\n",
    "\n",
    "    # Radial velocities\n",
    "    v_r1 = np.dot(v1, u1)\n",
    "    v_r2 = np.dot(v2, u2)\n",
    "    v_r3 = np.dot(v3, u3)\n",
    "\n",
    "    # Doppler per sat: fD = -(v_r/c)*f\n",
    "    fD1 = -(v_r1 / c) * f_center_hz\n",
    "    fD2 = -(v_r2 / c) * f_center_hz\n",
    "    fD3 = -(v_r3 / c) * f_center_hz\n",
    "\n",
    "    # FDOA\n",
    "    fD21 = fD2 - fD1\n",
    "    fD31 = fD3 - fD1\n",
    "\n",
    "    return tau21, tau31, fD21, fD31\n",
    "\n",
    "def test_lfmcw_caf_blind_geom(\n",
    "    # Function used to align extracted LFMCW first sweeps across channels and CAF them\n",
    "    iq_FF1, iq_FF2, iq_FF3,\n",
    "    fs,\n",
    "    lfmcw_row,\n",
    "    emitter_lla,\n",
    "    r_sats,\n",
    "    v_sats,\n",
    "    F_LO,\n",
    "    tau21_true=None, tau31_true=None,\n",
    "    fD21_true=None, fD31_true=None,\n",
    "    tau_guard_extra_us=20.0,\n",
    "    tau_search_us=100.0,\n",
    "    fd_search_hz=10000.0,\n",
    "    caf_batch_size=256,\n",
    "):\n",
    "    # === Geometry-based coarse τ/FDOA ===\n",
    "    f_center = lfmcw_row[\"Center Freq (MHz)\"] * 1e6\n",
    "    f_rf = F_LO + f_center\n",
    "\n",
    "    tau21_est, tau31_est, fD21_est, fD31_est = geom_tdoa_fdoa_from_lla(\n",
    "        emitter_lla, r_sats, v_sats, f_rf\n",
    "    )\n",
    "\n",
    "    tau_guard_s = abs(tau21_est) + abs(tau31_est) + tau_guard_extra_us*1e-6\n",
    "\n",
    "    # === Extract sweeps using common ROI ===\n",
    "    x1_ds, x2_ds, x3_ds, fs_ds, meta = extract_lfmcw_common_window_geom(\n",
    "        iq_FF1, iq_FF2, iq_FF3,\n",
    "        fs, lfmcw_row,\n",
    "        tau21_est, tau31_est,\n",
    "        tau_guard_s,\n",
    "    )\n",
    "\n",
    "    # === CAF grids centered on geometry ===\n",
    "    tau_search = tau_search_us * 1e-6\n",
    "\n",
    "    tau_vals_21 = np.linspace(tau21_est - tau_search,\n",
    "                              tau21_est + tau_search,\n",
    "                              401)\n",
    "    tau_vals_31 = np.linspace(tau31_est - tau_search,\n",
    "                              tau31_est + tau_search,\n",
    "                              401)\n",
    "\n",
    "    fd_vals_21 = np.linspace(fD21_est - fd_search_hz,\n",
    "                             fD21_est + fd_search_hz,\n",
    "                             401)\n",
    "    fd_vals_31 = np.linspace(fD31_est - fd_search_hz,\n",
    "                             fD31_est + fd_search_hz,\n",
    "                             401)\n",
    "\n",
    "    # === CAF FF2→FF1 ===\n",
    "    #print(\"\\nRunning CAF FF2→FF1 ...\")\n",
    "    CAF21 = caf_fft_gpu(x1_ds, x2_ds, tau_vals_21, fd_vals_21, fs_ds,\n",
    "                        batch_size=caf_batch_size)\n",
    "    i_fd21, i_tau21 = np.unravel_index(np.argmax(CAF21), CAF21.shape)\n",
    "    tau21_hat = tau_vals_21[i_tau21]\n",
    "    fD21_hat  = fd_vals_21[i_fd21]\n",
    "\n",
    "    # === CAF FF3→FF1 ===\n",
    "    #print(\"Running CAF FF3→FF1 ...\")\n",
    "    CAF31 = caf_fft_gpu(x1_ds, x3_ds, tau_vals_31, fd_vals_31, fs_ds,\n",
    "                        batch_size=caf_batch_size)\n",
    "    i_fd31, i_tau31 = np.unravel_index(np.argmax(CAF31), CAF31.shape)\n",
    "    tau31_hat = tau_vals_31[i_tau31]\n",
    "    fD31_hat  = fd_vals_31[i_fd31]\n",
    "\n",
    "    return dict(\n",
    "        tau21_hat=tau21_hat,\n",
    "        tau31_hat=tau31_hat,\n",
    "        fD21_hat=fD21_hat,\n",
    "        fD31_hat=fD31_hat,\n",
    "        CAF21=CAF21,\n",
    "        CAF31=CAF31,\n",
    "        tau_vals_21=tau_vals_21,\n",
    "        fd_vals_21=fd_vals_21,\n",
    "        tau_vals_31=tau_vals_31,\n",
    "        fd_vals_31=fd_vals_31,\n",
    "        meta=meta,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9409f7-3be1-45af-bb0d-90ac116eb25e",
   "metadata": {},
   "source": [
    "## 9. Geolocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ecc57b-49e9-4009-8389-c66138442ffc",
   "metadata": {},
   "source": [
    "### 9.1 TDOA (for LFM and Single Frequency Pulses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf029e05-51d5-47be-964b-2858d1c267d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### TDOA geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c02942-ce24-4f3d-a74c-e32181da06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdoa_model_latlon(\n",
    "    lat_rad: float,\n",
    "    lon_rad: float,\n",
    "    r_sats: dict,\n",
    "    alt_m: float = 0.0\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute TDOAs τ21 and τ31 [s] for a given emitter lat/lon, alt=0 by default.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat_rad : float\n",
    "        Latitude in radians.\n",
    "    lon_rad : float\n",
    "        Longitude in radians.\n",
    "    r_sats : dict\n",
    "        Dictionary of satellite ECEF positions, keys: 'FF1','FF2','FF3'.\n",
    "        Each value should be a length-3 ndarray [m].\n",
    "    alt_m : float, optional\n",
    "        Altitude in meters (default 0.0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tau21 : float\n",
    "        TDOA (Sat2 - Sat1) [s].\n",
    "    tau31 : float\n",
    "        TDOA (Sat3 - Sat1) [s].\n",
    "    \"\"\"\n",
    "    lat_deg = np.rad2deg(lat_rad)\n",
    "    lon_deg = np.rad2deg(lon_rad)\n",
    "\n",
    "    e_ecef = lla_to_ecef(lat_deg, lon_deg, alt_m)\n",
    "\n",
    "    r1 = r_sats['FF1']\n",
    "    r2 = r_sats['FF2']\n",
    "    r3 = r_sats['FF3']\n",
    "\n",
    "    R1 = np.linalg.norm(e_ecef - r1)\n",
    "    R2 = np.linalg.norm(e_ecef - r2)\n",
    "    R3 = np.linalg.norm(e_ecef - r3)\n",
    "\n",
    "    tau21 = (R2 - R1) / c\n",
    "    tau31 = (R3 - R1) / c\n",
    "\n",
    "    return tau21, tau31\n",
    "\n",
    "\n",
    "def tdoa_jacobian_latlon(\n",
    "    lat_rad: float,\n",
    "    lon_rad: float,\n",
    "    r_sats: dict,\n",
    "    alt_m: float = 0.0,\n",
    "    eps_rad: float = 1e-6\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Numerically compute Jacobian J of TDOA wrt (lat, lon):\n",
    "\n",
    "        J = [ dτ21/dlat   dτ21/dlon ]\n",
    "            [ dτ31/dlat   dτ31/dlon ]\n",
    "\n",
    "    where lat, lon are in radians.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat_rad : float\n",
    "        Latitude [rad] at which to evaluate the Jacobian.\n",
    "    lon_rad : float\n",
    "        Longitude [rad] at which to evaluate the Jacobian.\n",
    "    r_sats : dict\n",
    "        Satellite positions in ECEF (keys 'FF1','FF2','FF3').\n",
    "    alt_m : float\n",
    "        Altitude in meters (fixed, usually 0).\n",
    "    eps_rad : float\n",
    "        Small perturbation step in radians for numerical differentiation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    J : (2,2) ndarray\n",
    "        Jacobian matrix of TDOA wrt (lat, lon) at the given point.\n",
    "    \"\"\"\n",
    "    # Base TDOAs at (lat, lon)\n",
    "    tau21_0, tau31_0 = tdoa_model_latlon(lat_rad, lon_rad, r_sats, alt_m)\n",
    "\n",
    "    J = np.zeros((2, 2), dtype=float)\n",
    "\n",
    "    # Perturb latitude\n",
    "    lat_p = lat_rad + eps_rad\n",
    "    tau21_p, tau31_p = tdoa_model_latlon(lat_p, lon_rad, r_sats, alt_m)\n",
    "    J[:, 0] = [(tau21_p - tau21_0) / eps_rad,\n",
    "               (tau31_p - tau31_0) / eps_rad]\n",
    "\n",
    "    # Perturb longitude\n",
    "    lon_p = lon_rad + eps_rad\n",
    "    tau21_p, tau31_p = tdoa_model_latlon(lat_rad, lon_p, r_sats, alt_m)\n",
    "    J[:, 1] = [(tau21_p - tau21_0) / eps_rad,\n",
    "               (tau31_p - tau31_0) / eps_rad]\n",
    "\n",
    "    return J\n",
    "\n",
    "def geolocate_from_tdoa(\n",
    "    tau_hat_21: float,\n",
    "    tau_hat_31: float,\n",
    "    r_sats: dict,\n",
    "    lat0_deg: float,\n",
    "    lon0_deg: float,\n",
    "    alt_m: float = 0.0,\n",
    "    max_iter: int = 100,\n",
    "    tol: float = 1e-11\n",
    ") -> tuple[float, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Solve for emitter (lat, lon) given measured TDOAs (τ̂21, τ̂31),\n",
    "    using Gauss–Newton with altitude fixed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tau_hat_21 : float\n",
    "        Measured TDOA for Sat2-Sat1 [s] (from CAF).\n",
    "    tau_hat_31 : float\n",
    "        Measured TDOA for Sat3-Sat1 [s] (from CAF).\n",
    "    r_sats : dict\n",
    "        Satellite ECEF positions {'FF1','FF2','FF3'}.\n",
    "    lat0_deg : float\n",
    "        Initial guess latitude [deg].\n",
    "    lon0_deg : float\n",
    "        Initial guess longitude [deg].\n",
    "    alt_m : float\n",
    "        Fixed altitude [m], usually 0.\n",
    "    max_iter : int\n",
    "        Maximum Gauss–Newton iterations.\n",
    "    tol : float\n",
    "        Convergence threshold on parameter update norm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lat_est_deg : float\n",
    "        Estimated emitter latitude [deg].\n",
    "    lon_est_deg : float\n",
    "        Estimated emitter longitude [deg].\n",
    "    J_est : (2,2) ndarray\n",
    "        Jacobian of TDOA wrt (lat,lon) [rad] at the solution point.\n",
    "        This is used later for covariance / CEP calculation.\n",
    "    \"\"\"\n",
    "    # Work in radians internally\n",
    "    lat = np.deg2rad(lat0_deg)\n",
    "    lon = np.deg2rad(lon0_deg)\n",
    "\n",
    "    z_hat = np.array([tau_hat_21, tau_hat_31], dtype=float)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Predicted TDOAs at current estimate\n",
    "        tau21_pred, tau31_pred = tdoa_model_latlon(lat, lon, r_sats, alt_m)\n",
    "        z_pred = np.array([tau21_pred, tau31_pred], dtype=float)\n",
    "\n",
    "        # Residual in TDOA space\n",
    "        r_vec = z_hat - z_pred   # shape (2,)\n",
    "\n",
    "        # Jacobian at current point\n",
    "        J = tdoa_jacobian_latlon(lat, lon, r_sats, alt_m)\n",
    "\n",
    "        # Gauss–Newton step: dx = (J^T J)^(-1) J^T r\n",
    "        # (we assume measurement noise is isotropic when solving)\n",
    "        JTJ = J.T @ J\n",
    "        try:\n",
    "            dx = np.linalg.solve(JTJ, J.T @ r_vec)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # In degenerate geometry, fallback to pseudo-inverse\n",
    "            dx = np.linalg.pinv(JTJ) @ (J.T @ r_vec)\n",
    "\n",
    "        # Update lat, lon (in radians)\n",
    "        lat += dx[0]\n",
    "        lon += dx[1]\n",
    "\n",
    "        # Check convergence in parameter space\n",
    "        if np.linalg.norm(dx) < tol:\n",
    "            break\n",
    "\n",
    "    lat_est_deg = np.rad2deg(lat)\n",
    "    lon_est_deg = np.rad2deg(lon)\n",
    "\n",
    "    # Final Jacobian at solution (for covariance)\n",
    "    J_est = tdoa_jacobian_latlon(lat, lon, r_sats, alt_m)\n",
    "\n",
    "    return lat_est_deg, lon_est_deg, J_est\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd53ba-45bf-4b36-8f24-be4e72eac3dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### TDOA Ellipse Derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9313863-722c-4172-82a1-6d0afecb8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TDOA_covariance_matrix(\n",
    "    fs_caf, F_LO,\n",
    "    B_sig,\n",
    "    PW,\n",
    "    snr_db_rx1,\n",
    "    snr_db_rx2,\n",
    "    snr_db_rx3,\n",
    "    M,\n",
    "    error_cfg,\n",
    "    mode=\"lfm\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Build correlated TDOA covariance matrix Στ using:\n",
    "      - Ulman TDOA CRLB for σ21 and σ31,\n",
    "      - Synthetic FF1–FF1 Ulman CRLB for σ11,\n",
    "      - sigma1_sq  = 0.5 * sigma_tau_11^2,\n",
    "      - Στ = [[var21, sigma1_sq],\n",
    "             [sigma1_sq, var31]].\n",
    "\n",
    "    External errors are added only to the *pairwise* variances.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) CORE CRLB (Ulman)\n",
    "    # -------------------------\n",
    "\n",
    "    if mode == \"lfm\":\n",
    "        # Synthetic FF1–FF1\n",
    "        sigma_tau_11 = theoretical_delay_sigma(\n",
    "            fs_caf, B_sig, PW,\n",
    "            snr_db_rx1, snr_db_rx1,\n",
    "            M=M\n",
    "        )\n",
    "\n",
    "        # Real pairs\n",
    "        sigma_tau_21 = theoretical_delay_sigma(\n",
    "            fs_caf, B_sig, PW,\n",
    "            snr_db_rx1, snr_db_rx2,\n",
    "            M=M\n",
    "        )\n",
    "        sigma_tau_31 = theoretical_delay_sigma(\n",
    "            fs_caf, B_sig, PW,\n",
    "            snr_db_rx1, snr_db_rx3,\n",
    "            M=M\n",
    "        )\n",
    "\n",
    "    elif mode == \"tone\":\n",
    "        # Synthetic FF1–FF1\n",
    "        sigma_tau_11 = theoretical_delay_sigma(\n",
    "            fs_caf, 2/PW, PW,\n",
    "            snr_db_rx1, snr_db_rx1,\n",
    "            M=M\n",
    "        )\n",
    "\n",
    "        # Real pairs\n",
    "        sigma_tau_21 = theoretical_delay_sigma(\n",
    "            fs_caf, 2/PW, PW,\n",
    "            snr_db_rx1, snr_db_rx2,\n",
    "            M=M\n",
    "        )\n",
    "        sigma_tau_31 = theoretical_delay_sigma(\n",
    "            fs_caf, 2/PW, PW,\n",
    "            snr_db_rx1, snr_db_rx3,\n",
    "            M=M\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Per Receiver TOA variances\n",
    "    # -------------------------\n",
    "    sigma1_sq = 0.5 * sigma_tau_11**2\n",
    "    sigma2_sq = sigma_tau_21**2 - sigma1_sq\n",
    "    sigma3_sq = sigma_tau_31**2 - sigma1_sq\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) External timing errors\n",
    "    # -------------------------\n",
    "    sync_var  = (error_cfg[\"sync_err_rms\"])**2\n",
    "    meas_var  = (error_cfg[\"meas_err_rms\"])**2\n",
    "    pos_var   = (np.sqrt(2.0/3)*error_cfg[\"pos_err_rms\"]/c)**2\n",
    "    quant_var = (1.0 / fs_caf / np.sqrt(12.0))**2\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Per-receiver measurement error\n",
    "    # -------------------------\n",
    "    sigma1_sq += meas_var\n",
    "    sigma2_sq += meas_var\n",
    "    sigma3_sq += meas_var\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Pairwise-only errors\n",
    "    # -------------------------\n",
    "    pair_var_extra = sync_var + pos_var + quant_var\n",
    "    var21   = (sigma1_sq + sigma2_sq) + pair_var_extra\n",
    "    var31   = (sigma1_sq + sigma3_sq) + pair_var_extra\n",
    "    \n",
    "    # -------------------------\n",
    "    # 5) Final correlated Στ\n",
    "    # -------------------------\n",
    "    Sigma_tau = np.array([\n",
    "        [var21,   sigma1_sq],\n",
    "        [sigma1_sq, var31  ]\n",
    "    ])\n",
    "\n",
    "    return Sigma_tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17add8-efc3-411b-a3cf-a95a4951df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_delay_sigma(\n",
    "    fs,\n",
    "    B_sig,\n",
    "    Tp,\n",
    "    snr_db_rx1,\n",
    "    snr_db_rx2,\n",
    "    M,\n",
    "    Bn=None,\n",
    "    use_numeric_Brms=False,\n",
    "    compute_Brms_numerical=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    TDOA CRLB-like σ_τ from Ulman & Geraniotis Eq. (5),(7),(8).\n",
    "\n",
    "    σ_τ ≈ 1 / (β_s * sqrt(Bn * T * γ_eff))\n",
    "\n",
    "    where:\n",
    "      - β_s = 2π * B_rms  (RMS frequency in rad/s)\n",
    "      - T   = M * Tp      (coherent integration time)\n",
    "      - Bn  = noise bandwidth [Hz]\n",
    "      - γ_eff from Eq. (8) with per-receiver SNRs γ1, γ2 in Bn\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fs : float\n",
    "        Sample rate [Hz].\n",
    "    B_sig : float\n",
    "        Signal (LFM) bandwidth B_s [Hz].\n",
    "    Tp : float\n",
    "        Pulse width [s].\n",
    "    snr_db_rx1, snr_db_rx2 : float\n",
    "        Per-receiver SNRs γ1, γ2 in dB, defined in the *receiver noise bandwidth* B_n.\n",
    "    M : int\n",
    "        Number of coherent pulses used in the CAF.\n",
    "    Bn : float or None\n",
    "        Common noise bandwidth [Hz]. If None, defaults to fs/2.\n",
    "    use_numeric_Brms : bool\n",
    "        If True, use user-provided compute_Brms_numerical(fs, Tp, B_sig).\n",
    "        If False, approximate Brms ≈ B_sig / (2*sqrt(3)).\n",
    "    compute_Brms_numerical : callable or None\n",
    "        Brms = compute_Brms_numerical(fs, Tp, B_sig) if use_numeric_Brms is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sigma_tau : float\n",
    "        CRLB-like standard deviation of TDOA [s].\n",
    "    \"\"\"\n",
    "    if Bn is None:\n",
    "        Bn = fs / 2.0\n",
    "\n",
    "    # Per-receiver SNRs in linear scale\n",
    "    gamma1 = 10.0 ** (snr_db_rx1 / 10.0)\n",
    "    gamma2 = 10.0 ** (snr_db_rx2 / 10.0)\n",
    "\n",
    "    eps = 1e-15\n",
    "    gamma1 = max(gamma1, eps)\n",
    "    gamma2 = max(gamma2, eps)\n",
    "\n",
    "    # Eq. (8): effective SNR in CAF\n",
    "    inv_gamma_eff = 0.5 * (1.0/gamma1 + 1.0/gamma2 + 2.0/(gamma1*gamma2))\n",
    "    gamma_eff = 1.0 / inv_gamma_eff\n",
    "\n",
    "    # Azaria-Hertz correction when noise bandwidth > signal bandwidth\n",
    "    if Bn > B_sig:\n",
    "        gamma_eff *= (B_sig / Bn)\n",
    "\n",
    "    # Integration time\n",
    "    T = M * Tp\n",
    "\n",
    "    # β_s = 2π * B_rms\n",
    "    if use_numeric_Brms:\n",
    "        if compute_Brms_numerical is None:\n",
    "            raise ValueError(\"compute_Brms_numerical must be provided.\")\n",
    "        Brms = compute_Brms_numerical(fs, Tp, B_sig)\n",
    "    else:\n",
    "        Brms = B_sig / (2.0 * np.sqrt(3.0))\n",
    "\n",
    "    beta_s = 2.0 * np.pi * Brms\n",
    "\n",
    "    # Eq. (5)\n",
    "    sigma_tau = 1.0 / (beta_s * np.sqrt(Bn * T * gamma_eff))\n",
    "    return sigma_tau\n",
    "\n",
    "\n",
    "# Optional numerical Brms (if you want more accuracy)\n",
    "def compute_Brms_numerical(fs, Tp, B):\n",
    "    \"\"\"Numerically compute B_rms for an LFM pulse.\"\"\"\n",
    "    t = np.arange(0, Tp, 1/fs)\n",
    "    k = B / Tp\n",
    "    s = np.exp(1j * np.pi * k * t**2)\n",
    "    s /= np.sqrt(np.sum(np.abs(s)**2))\n",
    "\n",
    "    dt = 1.0/fs\n",
    "    ds_dt = np.diff(s) / dt\n",
    "\n",
    "    E_sig  = np.sum(np.abs(s)**2) * dt\n",
    "    E_dsig = np.sum(np.abs(ds_dt)**2) * dt\n",
    "\n",
    "    return np.sqrt(E_dsig / (4 * np.pi**2 * E_sig))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7499dcb-eb80-4ca2-82a6-52b8f4bc37e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### TDOA Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad608f-d54a-4bcd-bae8-0070c522a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geolocate_and_cep_from_tdoa(\n",
    "    tau21_hat: float,\n",
    "    tau31_hat: float,\n",
    "    Sigma_tau: np.ndarray,      # 2×2 TDOA covariance\n",
    "    r_sats: dict,\n",
    "    lat0_deg: float,\n",
    "    lon0_deg: float,\n",
    "    alt_m: float = 0.0,\n",
    "    p: float = 0.5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "      - Geolocate emitter from TDOA-only (Gauss–Newton, alt fixed).\n",
    "      - Compute position covariance via Σ_τ.\n",
    "      - Convert to EN covariance at solution point.\n",
    "      - Extract p-level ellipse (e.g. CEP50/95).\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Geolocate from TDOA-only (blind, alt fixed) ---\n",
    "    lat_est_deg, lon_est_deg, J_est = geolocate_from_tdoa(\n",
    "        tau21_hat, tau31_hat,\n",
    "        r_sats,\n",
    "        lat0_deg, lon0_deg,\n",
    "        alt_m=alt_m\n",
    "    )\n",
    "\n",
    "    # --- 2) Position covariance in (lat,lon) [rad^2] ---\n",
    "    Sigma_latlon = position_covariance_latlon(J_est, Sigma_tau)\n",
    "\n",
    "    # --- 3) Convert to local EN covariance [m^2] ---\n",
    "    lat_est_rad = np.deg2rad(lat_est_deg)\n",
    "    Sigma_EN = latlon_cov_to_EN_cov_wgs84(Sigma_latlon, lat_est_rad)\n",
    "\n",
    "    # --- 4) Extract ellipse at probability p (e.g. CEP50) ---\n",
    "    a, b, angle_deg = ellipse_from_cov_EN(Sigma_EN, p=p)\n",
    "\n",
    "    return {\n",
    "        \"lat_est_deg\": lat_est_deg,\n",
    "        \"lon_est_deg\": lon_est_deg,\n",
    "        \"Sigma_latlon\": Sigma_latlon,\n",
    "        \"Sigma_EN\": Sigma_EN,\n",
    "        \"a\": a,\n",
    "        \"b\": b,\n",
    "        \"angle_deg\": angle_deg,\n",
    "        \"p\": p,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a5e88-83ca-41e6-837b-941287b7dc81",
   "metadata": {},
   "source": [
    "### 9.2 FDOA (for CWs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609ba53-457a-440c-a746-ab04a7e1537c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### FDOA Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d744504-da8c-41d7-89dc-eaae54deef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  FDOA GEOLOCATION PIPELINE (FDOA-only, alt fixed)\n",
    "# ============================================================\n",
    "\n",
    "def fdoa_model_latlon(\n",
    "    lat_rad: float,\n",
    "    lon_rad: float,\n",
    "    r_sats: dict,\n",
    "    v_sats: dict,\n",
    "    f_c_hz: float,\n",
    "    alt_m: float = 0.0,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute FDOAs fD21 and fD31 [Hz] for a given emitter lat/lon, alt=0 by default.\n",
    "\n",
    "    We assume a *static* ground emitter and moving satellites, so the Doppler at\n",
    "    each satellite i is\n",
    "\n",
    "        fD_i = -(f_c / c) * (v_i · u_i)\n",
    "\n",
    "    where u_i is the unit LOS from emitter to satellite i.\n",
    "\n",
    "    FDOA is then defined as:\n",
    "\n",
    "        fD21 = fD_2 - fD_1\n",
    "        fD31 = fD_3 - fD_1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat_rad, lon_rad : float\n",
    "        Latitude / longitude in radians.\n",
    "    r_sats : dict\n",
    "        Satellite ECEF positions, keys: 'FF1','FF2','FF3'. Each value shape (3,).\n",
    "    v_sats : dict\n",
    "        Satellite ECEF velocities, same keys as r_sats, each shape (3,).\n",
    "    f_c_hz : float\n",
    "        Carrier frequency [Hz].\n",
    "    alt_m : float, optional\n",
    "        Emitter altitude [m], default 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fD21 : float\n",
    "        FDOA (Sat2 - Sat1) [Hz].\n",
    "    fD31 : float\n",
    "        FDOA (Sat3 - Sat1) [Hz].\n",
    "    \"\"\"\n",
    "    lat_deg = np.rad2deg(lat_rad)\n",
    "    lon_deg = np.rad2deg(lon_rad)\n",
    "\n",
    "    # Emitter ECEF\n",
    "    e_ecef = lla_to_ecef(lat_deg, lon_deg, alt_m)\n",
    "\n",
    "    # Extract sat states\n",
    "    r1, v1 = r_sats['FF1'], v_sats['FF1']\n",
    "    r2, v2 = r_sats['FF2'], v_sats['FF2']\n",
    "    r3, v3 = r_sats['FF3'], v_sats['FF3']\n",
    "\n",
    "    # LOS unit vectors (emitter -> sat)\n",
    "    rho1 = r1 - e_ecef\n",
    "    rho2 = r2 - e_ecef\n",
    "    rho3 = r3 - e_ecef\n",
    "\n",
    "    u1 = rho1 / np.linalg.norm(rho1)\n",
    "    u2 = rho2 / np.linalg.norm(rho2)\n",
    "    u3 = rho3 / np.linalg.norm(rho3)\n",
    "\n",
    "    # Radial velocities (project satellite velocity on LOS)\n",
    "    vr1 = np.dot(v1, u1)\n",
    "    vr2 = np.dot(v2, u2)\n",
    "    vr3 = np.dot(v3, u3)\n",
    "\n",
    "    # Doppler at each satellite (narrowband approximation)\n",
    "    fD1 = -(f_c_hz / c) * vr1\n",
    "    fD2 = -(f_c_hz / c) * vr2\n",
    "    fD3 = -(f_c_hz / c) * vr3\n",
    "\n",
    "    # Differential Dopplers\n",
    "    fD21 = fD2 - fD1\n",
    "    fD31 = fD3 - fD1\n",
    "\n",
    "    return fD21, fD31\n",
    "\n",
    "\n",
    "def fdoa_jacobian_latlon(\n",
    "    lat_rad: float,\n",
    "    lon_rad: float,\n",
    "    r_sats: dict,\n",
    "    v_sats: dict,\n",
    "    f_c_hz: float,\n",
    "    alt_m: float = 0.0,\n",
    "    eps_rad: float = 1e-6,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Numerically compute Jacobian J of FDOA wrt (lat, lon):\n",
    "\n",
    "        J = [ d(fD21)/dlat   d(fD21)/dlon ]\n",
    "            [ d(fD31)/dlat   d(fD31)/dlon ]\n",
    "\n",
    "    where lat, lon are in radians.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat_rad, lon_rad : float\n",
    "        Point (rad) at which to evaluate the Jacobian.\n",
    "    r_sats, v_sats : dict\n",
    "        Satellite ECEF positions / velocities.\n",
    "    f_c_hz : float\n",
    "        Carrier frequency [Hz].\n",
    "    alt_m : float\n",
    "        Emitter altitude [m], fixed.\n",
    "    eps_rad : float\n",
    "        Small perturbation for finite-difference [rad].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    J : (2,2) ndarray\n",
    "        Jacobian matrix of FDOA wrt (lat, lon).\n",
    "    \"\"\"\n",
    "    # Base FDOAs at (lat, lon)\n",
    "    fD21_0, fD31_0 = fdoa_model_latlon(\n",
    "        lat_rad, lon_rad, r_sats, v_sats, f_c_hz, alt_m\n",
    "    )\n",
    "\n",
    "    J = np.zeros((2, 2), dtype=float)\n",
    "\n",
    "    # Perturb latitude\n",
    "    lat_p = lat_rad + eps_rad\n",
    "    fD21_p, fD31_p = fdoa_model_latlon(\n",
    "        lat_p, lon_rad, r_sats, v_sats, f_c_hz, alt_m\n",
    "    )\n",
    "    J[:, 0] = [(fD21_p - fD21_0) / eps_rad,\n",
    "               (fD31_p - fD31_0) / eps_rad]\n",
    "\n",
    "    # Perturb longitude\n",
    "    lon_p = lon_rad + eps_rad\n",
    "    fD21_p, fD31_p = fdoa_model_latlon(\n",
    "        lat_rad, lon_p, r_sats, v_sats, f_c_hz, alt_m\n",
    "    )\n",
    "    J[:, 1] = [(fD21_p - fD21_0) / eps_rad,\n",
    "               (fD31_p - fD31_0) / eps_rad]\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506cba99-2b3c-49ea-bcb1-dbf6258652f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geolocate_from_fdoa(\n",
    "    fD21_hat: float,\n",
    "    fD31_hat: float,\n",
    "    r_sats: dict,\n",
    "    v_sats: dict,\n",
    "    f_c_hz: float,\n",
    "    lat0_deg: float,\n",
    "    lon0_deg: float,\n",
    "    alt_m: float = 0.0,\n",
    "    max_iter: int = 100,\n",
    "    tol: float = 1e-11\n",
    ") -> tuple[float, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Solve for emitter (lat, lon) from FDOA-only measurements\n",
    "    (fD21_hat, fD31_hat) using Gauss–Newton with altitude fixed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fD21_hat : float\n",
    "        Measured FDOA for Sat2-Sat1 [Hz].\n",
    "    fD31_hat : float\n",
    "        Measured FDOA for Sat3-Sat1 [Hz].\n",
    "    r_sats : dict\n",
    "        Satellite ECEF positions {'FF1','FF2','FF3'}.\n",
    "    v_sats : dict\n",
    "        Satellite ECEF velocities {'FF1','FF2','FF3'}.\n",
    "    f_c_hz : float\n",
    "        Carrier frequency in Hz.\n",
    "    lat0_deg, lon0_deg : float\n",
    "        Initial guess.\n",
    "    alt_m : float\n",
    "        Fixed altitude.\n",
    "    max_iter : int\n",
    "        Maximum Gauss–Newton iterations.\n",
    "    tol : float\n",
    "        Convergence threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lat_est_deg, lon_est_deg : float\n",
    "        Estimated emitter LLA.\n",
    "    J_est : (2,2) ndarray\n",
    "        Jacobian of FDOA wrt (lat,lon) [rad] at the solution.\n",
    "    \"\"\"\n",
    "\n",
    "    lat = np.deg2rad(lat0_deg)\n",
    "    lon = np.deg2rad(lon0_deg)\n",
    "\n",
    "    z_hat = np.array([fD21_hat, fD31_hat], dtype=float)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "\n",
    "        # Predicted FDOAs\n",
    "        f21_pred, f31_pred = fdoa_model_latlon(\n",
    "            lat, lon, r_sats, v_sats, f_c_hz, alt_m\n",
    "        )\n",
    "        z_pred = np.array([f21_pred, f31_pred], dtype=float)\n",
    "\n",
    "        # Residual in FDOA space\n",
    "        r_vec = z_hat - z_pred\n",
    "\n",
    "        # Jacobian\n",
    "        J = fdoa_jacobian_latlon(\n",
    "            lat, lon, r_sats, v_sats, f_c_hz, alt_m\n",
    "        )\n",
    "\n",
    "        # Gauss–Newton step\n",
    "        JTJ = J.T @ J\n",
    "        try:\n",
    "            dx = np.linalg.solve(JTJ, J.T @ r_vec)\n",
    "        except np.linalg.LinAlgError:\n",
    "            dx = np.linalg.pinv(JTJ) @ (J.T @ r_vec)\n",
    "\n",
    "        lat += dx[0]\n",
    "        lon += dx[1]\n",
    "\n",
    "        if np.linalg.norm(dx) < tol:\n",
    "            break\n",
    "\n",
    "    lat_est_deg = np.rad2deg(lat)\n",
    "    lon_est_deg = np.rad2deg(lon)\n",
    "\n",
    "    # Final Jacobian for covariance/ellipse\n",
    "    J_est = fdoa_jacobian_latlon(lat, lon, r_sats, v_sats, f_c_hz, alt_m)\n",
    "\n",
    "    return lat_est_deg, lon_est_deg, J_est\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae547c7d-a605-422d-8724-770c4e4ee8b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### FDOA Ellipse Derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5b0b2-8d65-4817-963e-04673ec1f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_doppler_sigma(\n",
    "    fs: float,\n",
    "    B_sig: float,\n",
    "    Tp: float,\n",
    "    snr_db_rx1: float,\n",
    "    snr_db_rx2: float,\n",
    "    M: int,\n",
    "    Bn: float | None = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    FDOA CRLB-like σ_v from Ulman & Geraniotis Eq. (6),(8).\n",
    "\n",
    "    σ_v ≈ 0.55 / (T * sqrt(Bn * T * γ_eff))\n",
    "\n",
    "    where:\n",
    "      - T   = M * Tp      (coherent integration time)\n",
    "      - Bn  = noise bandwidth [Hz]\n",
    "      - γ_eff from Eq. (8) with per-receiver SNRs γ1, γ2 in Bn\n",
    "    \"\"\"\n",
    "    if Bn is None:\n",
    "        Bn = fs / 2.0\n",
    "\n",
    "    # Per-receiver SNRs in linear scale\n",
    "    gamma1 = 10.0 ** (snr_db_rx1 / 10.0)\n",
    "    gamma2 = 10.0 ** (snr_db_rx2 / 10.0)\n",
    "\n",
    "    eps = 1e-15\n",
    "    gamma1 = max(gamma1, eps)\n",
    "    gamma2 = max(gamma2, eps)\n",
    "\n",
    "    # Eq. (8): effective SNR in CAF\n",
    "    inv_gamma_eff = 0.5 * (1.0 / gamma1 + 1.0 / gamma2 + 2.0 / (gamma1 * gamma2))\n",
    "    gamma_eff = 1.0 / inv_gamma_eff\n",
    "\n",
    "    # Azaria–Hertz correction when noise bandwidth > signal bandwidth\n",
    "    if Bn > B_sig:\n",
    "        gamma_eff *= (B_sig / Bn)\n",
    "\n",
    "    # Integration time\n",
    "    T = M * Tp\n",
    "\n",
    "    # Eq. (5) doppler std dev [Hz]\n",
    "    sigma_v = 0.55 / (T * np.sqrt(Bn * T * gamma_eff))\n",
    "    return sigma_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc62c2-2941-4516-861a-77878ffd2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FDOA_covariance_matrix(\n",
    "    fs: float,\n",
    "    F_LO: float,\n",
    "    f_center: float,\n",
    "    B_sig: float,\n",
    "    PW: float,\n",
    "    snr_db_rx1: float,\n",
    "    snr_db_rx2: float,\n",
    "    snr_db_rx3: float,\n",
    "    M: int,\n",
    "    error_cfg: dict,\n",
    "    df_step: float,\n",
    "    mode: str = \"cw\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build correlated FDOA covariance matrix Σ_f.\n",
    "\n",
    "    For CW:\n",
    "      - Use Ulman FDOA CRLB for σ_v,21 and σ_v,31,\n",
    "      - Synthetic FF1–FF1 CRLB for σ_v,11,\n",
    "      - infer per-receiver core variances:\n",
    "            sigma1_sq_core = 0.5 * sigma_v_11^2\n",
    "            sigma2_sq_core = sigma_v_21^2 - sigma1_sq_core\n",
    "            sigma3_sq_core = sigma_v_31^2 - sigma1_sq_core\n",
    "      - Add per-receiver fd_meas_err + LO error into σᵢ²,\n",
    "      - Add vel_err + Doppler grid quantization as pairwise-only errors,\n",
    "      - Form:\n",
    "\n",
    "            var21   = sigma1_sq + sigma2_sq + pair_var_extra\n",
    "            var31   = sigma1_sq + sigma3_sq + pair_var_extra\n",
    "            cov2131 = sigma1_sq\n",
    "\n",
    "        Σ_f = [[var21,   cov2131],\n",
    "               [cov2131, var31  ]]\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) CORE CRLB (Ulman) in Doppler\n",
    "    # -------------------------\n",
    "    if mode == \"cw\":\n",
    "        # Synthetic FF1–FF1\n",
    "        sigma_v_11 = theoretical_doppler_sigma(\n",
    "            fs, B_sig, PW,\n",
    "            snr_db_rx1, snr_db_rx1,\n",
    "            M=M\n",
    "        )\n",
    "\n",
    "        # Real pairs\n",
    "        sigma_v_21 = theoretical_doppler_sigma(\n",
    "            fs, B_sig, PW,\n",
    "            snr_db_rx1, snr_db_rx2,\n",
    "            M=M\n",
    "        )\n",
    "        sigma_v_31 = theoretical_doppler_sigma(\n",
    "            fs, B_sig, PW,\n",
    "            snr_db_rx1, snr_db_rx3,\n",
    "            M=M\n",
    "        )\n",
    "    else:\n",
    "        # Placeholder for LFMCW or other modes later\n",
    "        raise NotImplementedError(\"FDOA_covariance_matrix: mode != 'cw' not implemented yet\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Per-receiver core Doppler variances\n",
    "    # -------------------------\n",
    "    sigma1_sq = 0.5 * sigma_v_11**2\n",
    "    sigma2_sq = sigma_v_21**2 - sigma1_sq\n",
    "    sigma3_sq = sigma_v_31**2 - sigma1_sq\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Per-receiver measurement + LO errors\n",
    "    # -------------------------\n",
    "    meas_var = error_cfg[\"fd_meas_err_rms\"]**2\n",
    "    lo_var   = error_cfg[\"lo_err_rms_hz\"]**2\n",
    "\n",
    "    sigma1_sq +=  (meas_var + lo_var)\n",
    "    sigma2_sq +=  (meas_var + lo_var)\n",
    "    sigma3_sq +=  (meas_var + lo_var)\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Pairwise-only FDOA errors: velocity + Doppler grid quantization\n",
    "    # -------------------------\n",
    "    # Map velocity error [m/s] to Doppler [Hz]:\n",
    "    #   Δf ≈ -(f_c / c) Δv_r\n",
    "    f_c = F_LO + f_center\n",
    "    vel_err_rms = error_cfg[\"vel_err_rms\"]     # [m/s]\n",
    "\n",
    "    vel_pair_std = np.sqrt(2.0/3) * vel_err_rms\n",
    "    vel_var_fd   = (f_c / c)**2 * vel_pair_std**2   # [Hz^2]\n",
    "\n",
    "    # Doppler grid quantization variance (1/√12 rule)\n",
    "    quant_var = (df_step / np.sqrt(12.0))**2\n",
    "\n",
    "    pair_var_extra = vel_var_fd + quant_var\n",
    "\n",
    "    # -------------------------\n",
    "    # 5) Final FDOA variances & covariance\n",
    "    # -------------------------\n",
    "    var21   = sigma1_sq + sigma2_sq + pair_var_extra\n",
    "    var31   = sigma1_sq + sigma3_sq + pair_var_extra\n",
    "\n",
    "    Sigma_v = np.array([\n",
    "        [var21,   sigma1_sq],\n",
    "        [sigma1_sq, var31  ]\n",
    "    ])\n",
    "\n",
    "    return Sigma_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea1366-c4c6-4c3e-a6de-0879b85ae7ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### FDOA Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4cf3c-ca59-41f7-8e21-c461d7307808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geolocate_and_cep_from_fdoa(\n",
    "    fD21_hat: float,\n",
    "    fD31_hat: float,\n",
    "    Sigma_f: np.ndarray,       # 2×2 FDOA covariance\n",
    "    r_sats: dict,\n",
    "    v_sats: dict,\n",
    "    f_c_hz: float,\n",
    "    lat0_deg: float,\n",
    "    lon0_deg: float,\n",
    "    alt_m: float = 0.0,\n",
    "    p: float = 0.5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "      - Geolocate emitter from FDOA-only (Gauss–Newton, alt fixed).\n",
    "      - Compute position covariance via Σ_f.\n",
    "      - Convert to EN covariance at solution point.\n",
    "      - Extract p-level ellipse (e.g. CEP50/95).\n",
    "\n",
    "    Mirrors geolocate_and_cep_from_tdoa.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Geolocate from FDOA-only (blind, alt fixed) ---\n",
    "    lat_est_deg, lon_est_deg, J_est = geolocate_from_fdoa(\n",
    "        fD21_hat, fD31_hat,\n",
    "        r_sats, v_sats, f_c_hz,\n",
    "        lat0_deg, lon0_deg,\n",
    "        alt_m=alt_m\n",
    "    )\n",
    "\n",
    "    # --- 2) Position covariance in (lat,lon) [rad^2] ---\n",
    "    # IDENTICAL STRUCTURE TO TDOA\n",
    "    Sigma_latlon = position_covariance_latlon(J_est, Sigma_f)\n",
    "\n",
    "    # --- 3) Convert to local EN covariance [m^2] ---\n",
    "    lat_est_rad = np.deg2rad(lat_est_deg)\n",
    "    Sigma_EN = latlon_cov_to_EN_cov_wgs84(Sigma_latlon, lat_est_rad)\n",
    "\n",
    "    # --- 4) Extract ellipse at probability p ---\n",
    "    a, b, angle_deg = ellipse_from_cov_EN(Sigma_EN, p=p)\n",
    "\n",
    "    return {\n",
    "        \"lat_est_deg\": lat_est_deg,\n",
    "        \"lon_est_deg\": lon_est_deg,\n",
    "        \"Sigma_latlon\": Sigma_latlon,\n",
    "        \"Sigma_EN\": Sigma_EN,\n",
    "        \"a\": a,\n",
    "        \"b\": b,\n",
    "        \"angle_deg\": angle_deg,\n",
    "        \"p\": p\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53200967-b647-404e-a28f-a851f9cb875d",
   "metadata": {},
   "source": [
    "### 9.3 TDOA (for LFMCWs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef473b0-6389-4989-89e8-112fdeee09b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lfmcw_loopback_refine(\n",
    "    iq_FF1, iq_FF2, iq_FF3,\n",
    "    fs,\n",
    "    lfmcw_row,\n",
    "    r_sats, v_sats,\n",
    "    F_LO,\n",
    "    # initial AOI-centre guess\n",
    "    lat0_deg, lon0_deg,\n",
    "    n_iter=4,\n",
    "    tau_search_us=100.0,\n",
    "    fd_search_hz=10000.0,\n",
    "    tau_guard_extra_us=20.0,\n",
    "    shrink_tau=0.35,\n",
    "    shrink_fd=0.50,\n",
    "    shrink_guard=0.70,\n",
    "    caf_batch_size=256,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loopback refinement:\n",
    "      CAF (blind extraction, geometry-centred)\n",
    "        -> TDOA geolocation\n",
    "        -> feed back estimated LLA\n",
    "        -> shrink CAF windows\n",
    "        -> repeat\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    final : dict\n",
    "        Final CAF outputs + geolocated LLA\n",
    "    debug : list of dict\n",
    "        Per-iteration diagnostic info\n",
    "    \"\"\"\n",
    "\n",
    "    # Initial proxy and GN start\n",
    "    proxy_lla = {\"lat\": float(lat0_deg), \"lon\": float(lon0_deg), \"alt\": 0.0}\n",
    "    lat_gn, lon_gn = float(lat0_deg), float(lon0_deg)\n",
    "\n",
    "    debug = []\n",
    "    final = None\n",
    "\n",
    "    for it in range(int(n_iter)):\n",
    "\n",
    "        # --- 1) CAF ---\n",
    "        res = test_lfmcw_caf_blind_geom(\n",
    "            iq_FF1, iq_FF2, iq_FF3,\n",
    "            fs,\n",
    "            lfmcw_row,\n",
    "            proxy_lla,\n",
    "            r_sats, v_sats,\n",
    "            F_LO,\n",
    "            tau_guard_extra_us=tau_guard_extra_us,\n",
    "            tau_search_us=tau_search_us,\n",
    "            fd_search_hz=fd_search_hz,\n",
    "            caf_batch_size=caf_batch_size,\n",
    "        )\n",
    "\n",
    "        tau21_hat = float(res[\"tau21_hat\"])\n",
    "        tau31_hat = float(res[\"tau31_hat\"])\n",
    "        fD21_hat  = float(res[\"fD21_hat\"])\n",
    "        fD31_hat  = float(res[\"fD31_hat\"])\n",
    "\n",
    "        # --- 2) Geolocate from TDOA ---\n",
    "        lat_est_deg, lon_est_deg, J_est = geolocate_from_tdoa(\n",
    "            tau21_hat, tau31_hat,\n",
    "            r_sats,\n",
    "            lat_gn, lon_gn,\n",
    "            alt_m=0.0,\n",
    "            max_iter=100,\n",
    "            tol=1e-11\n",
    "        )\n",
    "\n",
    "        # Normalize longitude (safety)\n",
    "        if lon_est_deg > 180.0:\n",
    "            lon_est_deg -= 360.0\n",
    "        elif lon_est_deg < -180.0:\n",
    "            lon_est_deg += 360.0\n",
    "\n",
    "        # --- 3) Save debug snapshot ---\n",
    "        debug.append({\n",
    "            \"iter\": it + 1,\n",
    "            \"proxy_lat\": proxy_lla[\"lat\"],\n",
    "            \"proxy_lon\": proxy_lla[\"lon\"],\n",
    "            \"tau_search_us\": tau_search_us,\n",
    "            \"fd_search_hz\": fd_search_hz,\n",
    "            \"tau_guard_extra_us\": tau_guard_extra_us,\n",
    "            \"tau21_hat\": tau21_hat,\n",
    "            \"tau31_hat\": tau31_hat,\n",
    "            \"fD21_hat\": fD21_hat,\n",
    "            \"fD31_hat\": fD31_hat,\n",
    "            \"lat_est_deg\": float(lat_est_deg),\n",
    "            \"lon_est_deg\": float(lon_est_deg),\n",
    "            \"J_est\": J_est,\n",
    "            \"meta\": res.get(\"meta\"),\n",
    "        })\n",
    "\n",
    "        # --- 4) Prepare next iteration ---\n",
    "        proxy_lla = {\"lat\": float(lat_est_deg), \"lon\": float(lon_est_deg), \"alt\": 0.0}\n",
    "        lat_gn, lon_gn = float(lat_est_deg), float(lon_est_deg)\n",
    "\n",
    "        tau_search_us      *= shrink_tau\n",
    "        fd_search_hz       *= shrink_fd\n",
    "        tau_guard_extra_us *= shrink_guard\n",
    "\n",
    "        final = {\n",
    "            \"tau21_hat\": tau21_hat,\n",
    "            \"tau31_hat\": tau31_hat,\n",
    "            \"fD21_hat\": fD21_hat,\n",
    "            \"fD31_hat\": fD31_hat,\n",
    "            \"lat_est_deg\": float(lat_est_deg),\n",
    "            \"lon_est_deg\": float(lon_est_deg),\n",
    "            \"meta\": res.get(\"meta\"),\n",
    "        }\n",
    "\n",
    "    return final, debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9113daa8-41b5-489f-af82-b2c85b655a6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Common Functions (Used by both TDOA and FDOA for CEP Ellipse Derivation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c594e6-69fe-4191-92f5-d8256098cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_covariance_latlon(\n",
    "    J_latlon: np.ndarray,\n",
    "    Sigma_meas: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute covariance of (lat, lon) [rad^2] from measurement covariance Σ_z\n",
    "    and Jacobian J (d[z]/d[lat,lon]), via Fisher Information inversion:\n",
    "\n",
    "        Σ_x = (J^T Σ_z^{-1} J)^(-1)\n",
    "\n",
    "    This works for ANY measurement type:\n",
    "      - TDOA: Σ_z = Σ_tau\n",
    "      - FDOA: Σ_z = Σ_fD\n",
    "      - Joint TDOA+FDOA (if later extended)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    J_latlon : (2,2) ndarray\n",
    "        Jacobian of measurement vector z wrt [lat,lon].\n",
    "    Sigma_meas : (2,2) ndarray\n",
    "        Covariance of measurement vector z.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sigma_latlon : (2,2) ndarray\n",
    "        Covariance of [lat, lon] in radians^2.\n",
    "    \"\"\"\n",
    "    Sigma_z_inv = np.linalg.inv(Sigma_meas)\n",
    "    F = J_latlon.T @ Sigma_z_inv @ J_latlon\n",
    "    Sigma_latlon = np.linalg.inv(F)\n",
    "    return Sigma_latlon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf9ae9-4aac-45f0-9b74-6f5760cf86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latlon_cov_to_EN_cov(\n",
    "    Sigma_latlon: np.ndarray,\n",
    "    lat_rad: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert covariance in (lat,lon) [rad^2] to covariance in local East/North [m^2].\n",
    "\n",
    "    We approximate locally:\n",
    "        dE ≈ R_E * cos(lat) * d(lon_rad)\n",
    "        dN ≈ R_E * d(lat_rad)\n",
    "\n",
    "    So:\n",
    "        [dE, dN]^T = G * [dlat, dlon]^T\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Sigma_latlon : (2,2) ndarray\n",
    "        Covariance of [lat, lon] in radians^2.\n",
    "    lat_rad : float\n",
    "        Latitude (rad) at which to evaluate the local metric scaling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sigma_EN : (2,2) ndarray\n",
    "        Covariance in local East/North plane [m^2].\n",
    "    \"\"\"\n",
    "    Re = WGS84_A  # treat Earth as sphere for local metric here\n",
    "\n",
    "    cos_lat = np.cos(lat_rad)\n",
    "\n",
    "    # Mapping matrix from [dlat, dlon] to [dE, dN]\n",
    "    # state ordering: [lat, lon]\n",
    "    G = np.array([\n",
    "        [0.0,         Re * cos_lat],   # dE/dlat=0, dE/dlon=Re*cos(lat)\n",
    "        [Re,          0.0]             # dN/dlat=Re, dN/dlon=0\n",
    "    ])\n",
    "\n",
    "    Sigma_EN = G @ Sigma_latlon @ G.T\n",
    "    return Sigma_EN\n",
    "\n",
    "def latlon_cov_to_EN_cov_wgs84(\n",
    "    Sigma_latlon: np.ndarray,\n",
    "    lat_rad: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert covariance in (lat,lon) [rad^2] to covariance in local East/North [m^2]\n",
    "    using WGS-84 ellipsoid radii of curvature.\n",
    "\n",
    "    dE ≈ N(phi) * cos(phi) * dlon\n",
    "    dN ≈ M(phi) * dlat\n",
    "    \"\"\"\n",
    "    a  = WGS84_A\n",
    "    e2 = WGS84_E2\n",
    "\n",
    "    sin_lat = np.sin(lat_rad)\n",
    "    cos_lat = np.cos(lat_rad)\n",
    "\n",
    "    denom = np.sqrt(1.0 - e2 * sin_lat**2)\n",
    "\n",
    "    # Prime vertical radius of curvature\n",
    "    N_phi = a / denom\n",
    "\n",
    "    # Meridian radius of curvature\n",
    "    M_phi = a * (1.0 - e2) / (denom**3)\n",
    "\n",
    "    # Mapping from [dlat, dlon] to [dE, dN]\n",
    "    G = np.array([\n",
    "        [0.0,        N_phi * cos_lat],  # dE/dlon\n",
    "        [M_phi,      0.0]               # dN/dlat\n",
    "    ], dtype=float)\n",
    "\n",
    "    return G @ Sigma_latlon @ G.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0804236e-80e3-4a70-b864-9a1816916b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipse_from_cov_EN(\n",
    "    Sigma_EN: np.ndarray,\n",
    "    p: float = 0.5\n",
    ") -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Extract ellipse semi-axes and orientation from EN covariance.\n",
    "\n",
    "    For a given probability p (e.g. 0.5 for CEP50, 0.95 for 95% ellipse),\n",
    "    the ellipse scaling factor is the chi-square quantile with 2 dof:\n",
    "\n",
    "        k_p = χ²_{2, p}\n",
    "\n",
    "    Then:\n",
    "        a = sqrt(k_p * λ1)\n",
    "        b = sqrt(k_p * λ2)\n",
    "\n",
    "    where λ1 >= λ2 are eigenvalues of Σ_EN, and a,b are semi-major/minor axes [m].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Sigma_EN : (2,2) ndarray\n",
    "        Covariance matrix in East/North [m^2].\n",
    "    p : float\n",
    "        Confidence level for the ellipse, e.g. 0.5 for CEP50,\n",
    "        0.95 for 95% probability ellipse.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a : float\n",
    "        Semi-major axis [m].\n",
    "    b : float\n",
    "        Semi-minor axis [m].\n",
    "    angle_deg : float\n",
    "        Orientation of the major axis in degrees, measured CCW from East\n",
    "        (i.e. EN-plane angle of eigenvector).\n",
    "    \"\"\"\n",
    "    # Eigen-decomposition\n",
    "    eigvals, eigvecs = np.linalg.eigh(Sigma_EN)\n",
    "\n",
    "    # Sort eigenvalues (largest first)\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    lam1, lam2 = eigvals[idx]\n",
    "    vec1 = eigvecs[:, idx[0]]\n",
    "\n",
    "    # Chi-square quantile for 2 dof at probability p\n",
    "    # Closed form for 2 dof: F(k) = 1 - exp(-k/2)\n",
    "    # so invert: k_p = -2 ln(1 - p)\n",
    "    k_p = -2.0 * np.log(1.0 - p)\n",
    "\n",
    "    a = np.sqrt(k_p * lam1)\n",
    "    b = np.sqrt(k_p * lam2)\n",
    "\n",
    "    # Orientation: angle of major-axis eigenvector in EN plane\n",
    "    angle_rad = np.arctan2(vec1[1], vec1[0])  # vec = [E_component, N_component]\n",
    "    angle_deg = np.rad2deg(angle_rad)\n",
    "\n",
    "    return a, b, angle_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6345575-c455-4386-80c1-76898e7446e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cep_from_axes(a, b, p=0.5):\n",
    "    \"\"\"\n",
    "    Compute the circular CEP(p) radius from the ellipse semi-axes a, b.\n",
    "\n",
    "    CEP(p) = sqrt( -2 (a^2 + b^2) ln(1 - p) )\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a, b : floats\n",
    "        Semi-major and semi-minor axes (meters).\n",
    "    p : float\n",
    "        Probability level (e.g., 0.5 for CEP50, 0.9 for CEP90, 0.95 for CEP95)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cep_radius : float\n",
    "        Circular-equivalent CEP radius (meters)\n",
    "    \"\"\"\n",
    "    return np.sqrt(-2.0 * (a*a + b*b) * np.log(1.0 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85871409-3bf1-48cc-80f6-7321b5065797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "#  CEP ellipse containment checker: WITH LLA INPUT\n",
    "# ===========================================================\n",
    "from scipy.stats import chi2\n",
    "def gt_in_cep_ellipse_lla_percentile(lat_est, lon_est,\n",
    "                                     lat_gt, lon_gt,\n",
    "                                     a, b, theta_deg,\n",
    "                                     cep_percent=50,\n",
    "                                     alt_est=0.0, alt_gt=0.0):\n",
    "    \"\"\"\n",
    "    Check whether the ground truth LLA lies inside the CEP ellipse corresponding to an\n",
    "    arbitrary CEP percentile (50, 90, 95, 99, ...), centered\n",
    "    at the estimated LLA. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lat_est, lon_est : float\n",
    "        Estimated location (deg). Used as ENU origin.\n",
    "\n",
    "    lat_gt, lon_gt : float\n",
    "        Ground truth location (deg).\n",
    "\n",
    "    a, b : float\n",
    "        Semi-major and semi-minor axes (meters)\n",
    "\n",
    "    theta_deg : float\n",
    "        Ellipse orientation angle (degrees), CCW\n",
    "\n",
    "    alt_est, alt_gt : float\n",
    "        Altitudes (meters). Default=0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    d2 : float\n",
    "        Mahalanobis distance squared.\n",
    "\n",
    "    inside : bool\n",
    "        True if GT lies inside ellipse.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert both points to ECEF\n",
    "    ecef_est = lla_to_ecef(lat_est, lon_est, alt_est)\n",
    "    ecef_gt  = lla_to_ecef(lat_gt,  lon_gt,  alt_gt)\n",
    "\n",
    "    # Convert GT to ENU relative to estimate\n",
    "    gt_enu = ecef_to_enu(ecef_gt, lat_est, lon_est, alt_est)\n",
    "    gx, gy = gt_enu[0], gt_enu[1]\n",
    "\n",
    "    # Rotate ENU axes by -theta (ellipse frame)\n",
    "    theta = np.deg2rad(theta_deg)\n",
    "    xr =  gx * np.cos(theta) + gy * np.sin(theta)\n",
    "    yr = -gx * np.sin(theta) + gy * np.cos(theta)\n",
    "\n",
    "    # Mahalanobis distance squared\n",
    "    d2 = (xr / a)**2 + (yr / b)**2\n",
    "\n",
    "    # Compute χ² threshold for the desired percentile\n",
    "    p = cep_percent / 100.0\n",
    "    threshold = chi2.ppf(p, df=2)\n",
    "\n",
    "    inside = d2 <= threshold\n",
    "    return d2, inside, threshold\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
